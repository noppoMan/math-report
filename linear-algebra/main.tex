\documentclass[dvipdfmx,autodetect-engine]{jsarticle}
\usepackage{tikz}
\usepackage{graphicx,fancybox,ascmac, amsmath, amssymb}
\usepackage[all]{xy}
\usepackage{bm}
\usepackage{cases}

\newtheorem{theo}{定理}[section]
\newtheorem{defi}[theo]{定義}
\newtheorem{rem}[theo]{定理}
\newtheorem{exam}[theo]{例}
\newtheorem{exercise}[theo]{例題}
\newtheorem{prop}[theo]{命題}
\newtheorem{ques}[theo]{問}
\newtheorem{lemm}[theo]{補題}

% functions %
\newcommand{\innerProduct}[2]{\langle \bm{#1}, \bm{#2} \rangle}
\newcommand{\tensorProduct}[2]{\bm{#1} \otimes \bm{#2}}
\newcommand{\transposeMat}[1]{{}^t\!{#1}}
\newcommand{\transposeVec}[1]{{}^t\!{\bm{#1}}}
\newcommand{\vecSpace}[1]{\mathbb{R}^{#1}}
\newcommand{\polynomialVecSet}[1]{\mathbb{R}[x]_{#1}}
\newcommand{\vecSet}[1]{\{\bm{#1}_1, \cdots, \bm{#1}_n\}}
\newcommand{\linearCombination}[2]{\langle #1, \cdots, #2\rangle}
\newcommand{\rank}[1]{{\rm rank}\,#1}
\newcommand{\img}[0]{{\rm Im}}

\setcounter{tocdepth}{3}

\title{線形代数}

\author{武井優己}
\date{\today}
\begin{document}
\maketitle

\tableofcontents

\section{2次行列の演算}

線形代数では、数の拡張としてベクトル、行列という高次元の数を考える。いきなりn次元に目を向けるのは難しいので、ウォーミングアップとして2次元の行列に対して、通常の数と同様の加法と乗法を天下り的に定義する。ここでは、行列は我々の知る実数に似た代数的構造を持つ（実数のような演算ができる）ということが単に分かればよい。なぜそうなるかはこのレポートを読み進めていくうちに自ずと理解できるようになってくる。

$A, A'$をそれぞれ以下の2次（正方）行列とし、行列の演算を進めていくことにする。2次正方行列とは行と列がそれぞれ$2 \times 2$の形をした行列のことである。

$$
A = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix},
A' = \begin{pmatrix}
a' & b' \\
c' & d' \\
\end{pmatrix}
$$

$A$であれば、$(a, b), (c, d)$の横の並びが行、$\begin{pmatrix}
a \\
c \\
\end{pmatrix}, \begin{pmatrix}
b \\
d \\
\end{pmatrix}$の縦の並びが列に該当する。


\subsection{2次行列の加法}

$A, A'$に対し、その和を

$$
A + A' = \begin{pmatrix}
a + a' & b + b' \\
c + c' & d + d'\\
\end{pmatrix}
$$

によって定義する。

$A,A'$の他に、

$$
A'' = \begin{pmatrix}
a'' & b'' \\
c'' & d'' \\
\end{pmatrix}
$$

があるとすれば、これらの和は

$$
(A + A') + A'' = \begin{pmatrix}
(a + a') + a'' & (b + b') + b'' \\
(c + c') + c'' & (d + d') + d'' \\
\end{pmatrix}
$$

$$
A + (A' + A'') = \begin{pmatrix}
a + (a' + a'') & b + (b' + b'') \\
c + (c' + c'') & d + (d' + d'') \\
\end{pmatrix}
$$

と書くことができる。

ここで、和で表される各成分は実数における加法の形になっているため、

$$
(A + A') + A'' = A + (A' + A'')
$$

と結合法則が成立する。同様に、
$$
A + A' = A' + A
$$

と交換法則も成立する。

\subsubsection{零行列}

すべての成分が0である行列を{\bf 零行列}といい

$$
0 = \begin{pmatrix}
0 & 0 \\
0 & 0 \\
\end{pmatrix}
$$

と書く。演算において零行列は単に$0$として表し、$A + 0 = 0 + A = A$が成立する。

\subsubsection{2次行列の減法}

加法の逆演算として減法も可能である。$X + A = A'$となる2次行列

$$
X = \begin{pmatrix}
x & y \\
z & w \\
\end{pmatrix}
$$

が存在する。このような$X$があったとすれば、$A'$のそれぞれの成分は

\begin{eqnarray*}
x + a = a', \quad y + b = b' \\
z + c = c', \quad w + d = d' \\
\end{eqnarray*}

であるから、$X$の成分はそれぞれ

\begin{eqnarray*}
x = a' - a, \quad y = b' - b \\
z = c' - c, \quad w = d' - d \\
\end{eqnarray*}

でなければならない。これより、$X$の成分は$A, A'$の差の唯一の解


$$
X = \begin{pmatrix}
a' - a & b' - b \\
c' - c & d' - d \\
\end{pmatrix}
$$

をもつ。この$X$を$A'-A$とするのが行列の減法である。

\subsection{2次行列の乗法}\label{subsubsection:matrixMultiple}

\subsubsection{2次行列のスカラー倍}

$k \in \mathbb{R}$に対して

$$
kA = \begin{pmatrix}
ka & kb \\
kc & kd
\end{pmatrix}
$$

となるような演算をスカラー倍(乗法)という。また、

$$
A0 = 0A = 0
$$

$-A = -1 \times A$より、

$$
A(-A') = (-A)A' = -AA'
$$

である。

\subsubsection{2次行列の積}

二つの2次行列$A, A'$の積を次のように定義する。

$$
AA' = \begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\begin{pmatrix}
a' & b' \\
c' & d'
\end{pmatrix} = 
\begin{pmatrix}
aa' + bc' & ab' + bd' \\
ca' + dc' & cb' + dd' \\
\end{pmatrix}
$$

$A$の$i$行と$A'$の$j$列の対応する成分の積の和を$AA'$の$(i, j)$成分と定義するのが行列の積である。なぜ加法のような対応ではないのかと疑問に思うが、それは本レポートを読み進めるうちに理解できるようになる。

積についても、結合法則が成立する。

$$
A'' = \begin{pmatrix}
a'' & b'' \\
c'' & d'' \\
\end{pmatrix}
$$

とすると、

$$
(AA')A'' = A(A'A'')
$$

である。しかし、2次行列の積に関しては交換法則

$$
AA' = A'A
$$

は一般には成立しない。実際、$AA'$の$(1, 1)$成分は$aa' + bc'$であるが、$A'A$の$(1, 1)$成分は、$a'a + b'c$となり、$AA' \neq A'A$である。

加法と乗法を組み合わせると、

\begin{eqnarray*}
A(A' + A'') = AA' + AA'' \\
(A' + A'')A = A'A + A''A
\end{eqnarray*}

のような分配法則が成立する。

このように、2次正方行列は一部の例外を除きほぼ実数と同じような振る舞いをすることが分かる。（これは、群・環・体といった代数的構造の賜である）

\section{一般の行列とベクトル}

\subsection{行列の成分表示}

$m \times n$個の数を矩形に並べ括弧で囲んだものを{\bf $m$行$n$列の行列}あるいは、{\bf $m \times n$行列}という。$m \times n$行列の全体の集合を$M_{m,n}(\mathbb{R})$と書き、$m = n$であるとき、{\bf $n$次正方行列}といい、$M_n(\mathbb{R})$と書く。行列を構成するそれぞれの数を{\bf 成分}と呼ぶ。

\defi {一般の行列の成分表示}

上述の例より、一般の $m \times n$行列$A$を成分表示すると次のようになる。 $A$の $(i, j)$成分を $(a_{ij})$とするとき

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix}
$$

である。

多くの場合これを省略して、$A = (a_{ij})$と表記する。ただし、
$(1 \leq i \leq m), (1 \leq j \leq n)$

\exam {$2 \times 2$行列の表示}

$m = 2, n = 2$のとき、2次正方行列Aは以下のように表示される

$$
A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix}
$$

\exam {$4 \times 5$行列の表示}

$m = 4, n = 5$のとき、$4 \times 5$行列Aは以下のように表示される

$$
A = \begin{pmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
\end{pmatrix}
$$

\subsection{一般の行列の加法、スカラー倍}\label{subsection:generalMatrixAdditionAndScalarMultiple}

\defi{一般の行列の加法、スカラー倍}\label{defi:additionAndScalarMultiple}

$A, B \in M_{m,n}(\mathbb{R}), A = (a_{ij}), B = (b_{ij})$とし、加法、スカラー倍を以下のように定義する。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A + B = (a_{ij}) + (b_{ij})$
\item $cA = (ca_{ij}) \quad (c \in \mathbb{R})$
\item $(-1)A = -A, \quad A + (-B) = A - B$
\end{enumerate}

\prop{定義\ref{defi:additionAndScalarMultiple}より、以下の法則を満たす}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $(A + B) + C = A + (B + C)$ \quad (結合法則)
\item $A + B = B + A$ \quad (交換法則)
\item $c(A+B) = cA + cB$ \quad (分配法則)
\end{enumerate}

である。

{\bf 証明}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $a_{ij}とb_{ij}の各成分どうしに結合法則が成り立つことより明らか$
\item $a_{ij}とb_{ij}の各成分どうしに交換法則が成り立つことより明らか$
\item $A + B = a_{ij} + b_ij$より、$c(A + B) = c(a_{ij} + b_ij)$である。
$cA = ca_{ij}, cB = cb_{ij}$であるから、$cA + cB = ca_{ij} + cb_{ij}$となる。
ここで実数における分配法則により、$c(a_{ij} + b_{ij}) = ca_{ij} + cb_{ij}$より、すべての(i, j)について分配法則が適用されるから、$c(A+B) = cA + cB$である。
\end{enumerate}

\defi{零行列との加法、スカラー倍}

$A \in M_{m,n}(\mathbb{R})$とするとき、零行列$0$との加法、スカラー倍で以下が成立する。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A + 0 = 0 + A = A$
\item $A + (-A) = (-A) + A = 0$
\item $1A = A$
\item $0A = 0$
\end{enumerate}


\subsection{一般の行列の積}

\defi{$l, m, n \in \mathbb{Z}$とし、$A \in M_{l,m}(R), B \in M_{m,n}(\mathbb{R})$とするとき、積ABを以下のように定義する。}\label{defi:matrixMultiple}

$$
AB = \sum_{1 \leq k \leq m} a_{ik}b_{kj} \in M_{l, n}(\mathbb{R})
\quad (ただし、1 \leq i \leq l, 1 \leq j \leq n)
$$

\ref{subsubsection:matrixMultiple}で紹介した$2 \times 2$行列の積がまさにこのようになっている。このように行列の積を定義する理由は、線形写像の章で説明する。

\prop{一般の行列の積について以下の法則が成立する}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A(BC) = (AB)C$
\item $A(B+C) = AB + AC$
\item $(B+C)A = BA + CA$
\end{enumerate}

これらも$AB$の各成分どうしが実数の積と和の形をしていることから、実数の演算における法則が適用できる。これにより、実際に演算することでこれらの証明は容易である。
(2)と(3)が同値でない理由として、行列の積は非可換なことが挙げられる。
実際、$AB$と$BA$を行列の積の定義に従い演算すると、それぞれ異なる行列が得られ、$AB \neq BA$となる。そのため、$A(B+C) \neq (B+C)A$となる。

\subsection{単位行列}

\defi

$A \in M_n(\mathbb{R})$に対して$AE = EA = A$となるような行列を{\bf $n$次単位行列}という。単に、単位行列ということもある。

単位行列は、

$$
E = \begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & 1 \\
\end{pmatrix}
$$

のように対角成分が$1$でそれ以外が$0$から構成される行列である。


\subsection{逆行列}

n次行列Aに対し$AX = E, XA = E$を満たすようなn次行列$X$が存在するとき、$A$は{\bf 可逆}である、または{\bf 正則行列}という。

\rem{n次行列Aが可逆であれば、$AX=E$および$YA=E$はそれぞれ唯一の解を持ち、それらの解は一致する。そのような$X, Y$を$A^{-1}$と書き、{\bf 逆行列}という。}

{\bf 証明}

$AX = XA = E, AY = YA = E$となる行列$X, Y$があるとする。このとき、結合法則により

\begin{eqnarray*}
X &= &XE \\
  &= &X(AY) \\
  &= &(XA)Y \\
  &= &EY \\
  &= &Y
\end{eqnarray*}

となり、$X = Y$である。仮に、$AB = BA = E, AC = CA = E$となる行列$B, C$があったとすれば、同様の理由により$X = Y = B = C$とこれらの方程式の解は一意に定まる。

\exam{単位行列の逆行列}

単位行列Eは$EE = E$であるから$E^{-1} = E$であり可逆である。$A, B \in M_n(\mathbb{R})$が可逆ならば、$AB$も可逆で$(AB)^{-1} = B^{-1}A^{-1}$である。
これが言える理由として、$(AB)(B^{-1}A^{-1}) = (B^{-1}A^{-1})(AB) = E$だからである。また、$A$が可逆であれば$A^{-1}$も可逆であり、$(A^{-1})^{-1} = A$。つまり、$(A^{-1})A = A(A^{-1}) = E$である。

これは、$M_n(\mathbb{R})$の可逆元全体が群の性質を持つことを言っており、この行列全体の集合を一般線形群といい、$GL_n(\mathbb{R})$と表される。

\subsection{列ベクトルと行ベクトル}

\subsubsection{(n, 1)型行列}

$$
B = \begin{pmatrix}
b_{11} \\
\vdots \\
b_{n1}
\end{pmatrix}
$$

を{\bf n-dim列ベクトル}という。

本レポートでは、n-dim列ベクトルを

$$
\bm{b} = \begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

のようにボールドのアルファベットで表記する。

\subsubsection{(1, n)型行列}

$(b_{11}, \cdots, b_{n1})$を{\bf n-dim行ベクトル}という。

\subsubsection{ベクトルの演算}

n-dim列ベクトルは(n,1)型の行列であるから、やはり加法と乗法が成立する。以降の節の理解を円滑に進めるに当たり、ここではベクトルの演算に関する細かな定義や性質の説明は飛ばす。(3章のベクトル空間にて、この辺りの細かい議論を行う) よって、予めベクトルの演算はできるものとして気楽に考えてほしい。

2次元ベクトルにおいて、

$$
\bm{a} = \begin{pmatrix}
x \\
y
\end{pmatrix},
\bm{b} = \begin{pmatrix}
x' \\
y'
\end{pmatrix}
$$
を取ってくる。

このとき、$a + b$は

$$
\begin{pmatrix}
x \\
y
\end{pmatrix} + 
\begin{pmatrix}
x' \\
y'
\end{pmatrix}
= \begin{pmatrix}
x + x' \\
y + y'
\end{pmatrix}
$$

である。

また、$c \in \mathbb{R}$に対して、

$$
c\bm{a} =
c\begin{pmatrix}
x \\
y
\end{pmatrix}
= \begin{pmatrix}
cx \\
cy
\end{pmatrix}
$$

である。

\subsection{連立一次方程式と行列}

$$
A = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix} \in M_2(\mathbb{R}),
\bm{x} = c\begin{pmatrix}
x \\
y
\end{pmatrix}
$$
とすれば、

$$
A\bm{x} = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
= \begin{pmatrix}
ax + by \\
cx + dy \\
\end{pmatrix}
$$

のように、2次元ベクトル$\bm{x}$に2次行列$A$を差乗することができる。この行列算の応用として$A\bm{x}$を連立一次方程式として表示することを考える。$ax + by$と$cx + dy$の和をそれぞれ$z, w$とすると

\begin{numcases}
  {}
  ax + by = z & \\
  cx + dy = w &
\end{numcases}

のような連立方程式で表すことができる。これより、もし$A$が逆行列を持つなら、

$$
\begin{pmatrix}
x \\
y
\end{pmatrix}
= A^{-1}
\begin{pmatrix}
z \\
w
\end{pmatrix}
$$
と計算することができる。

\subsection{転置行列}

(m, n)行列

$$
A = \begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots \\
a_{m1} & \cdots & a_{mn} \\
\end{pmatrix}
$$

に対し対角線$a_{aa} a_{22} \cdots$に対して、折返してできる(n, m)行列を${}^t\!A$と書き、$A$の転置行列という。

${}^t\!A$を表示すると以下の通りである。

$$
{}^t\!A = \begin{pmatrix}
a_{11} & \cdots & a_{m1} \\
\vdots & \vdots & \vdots \\
a_{1n} & \cdots & a_{mn} \\
\end{pmatrix}
$$

\prop{$A, B \in M_{m, n}(R)$に対して、明らかに${}^t(A + B) = \transposeMat{A} + \transposeMat{B}, {}^t{}^t\!A = A$が成立する。}

\subsubsection{対称と交代}

$A \in M_n(\mathbb{R})$とする。$A = {}^t\!A$ つまり、$a_{ij} = a_{ji} \quad (1 \leq i, j \leq n)$であるとき、$A$を対称または対称行列という。

$A = -{}^t\!A$ つまり、$a_{ij} = -a_{ji} \quad (1 \leq i, j \leq n)$であるとき、$A$を交代(的)または交代行列という。

\subsubsection{内積とテンソル積}

後の計量ベクトル空間の章で内積について詳しく説明するが、ここでは内積の定義とその意味についてだけ軽く触れておく。

二つのn次元ベクトル

$$
\bm{a} = \begin{pmatrix}
a_1 \\
\vdots \\
a_n
\end{pmatrix}, 
\bm{b} = \begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

に対し、


$$
{}^t\!\bm{a}\bm{b} = \sum_{1 \leq i \leq n} a_ib_i = (a_1, \cdots, a_n)\begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

を{\bf 内積}といい$\langle \bm{a}, \bm{b} \rangle$と書く。

また、
$$
\bm{a}{}^t\!\bm{b} = \begin{pmatrix}
a_1b_1 & \cdots & a_1b_n \\
\vdots & \vdots & \vdots \\
a_bb1 & \cdots & a_nb_n \\
\end{pmatrix}
$$

を$\bm{a}, \bm{b}$の{\bf テンソル積}といい、 $\bm{a} \otimes \bm{b}$と書く。

\exercise{$\bm{a}, \bm{b}, \bm{c}, \bm{d}$をn次元ベクトルとするとき、次の等式を証明せよ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item ${}^t(\tensorProduct{a}{b}) = \tensorProduct{b}{a}$
\item $(\tensorProduct{a}{b})\bm{c} = \innerProduct{b}{c}\bm{a}$
\item $\innerProduct{c}{(\tensorProduct{a}{b})\bm{d}} = \innerProduct{a}{c}\innerProduct{b}{d}$
\end{enumerate}

{\bf 解答}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item ${}^t(\tensorProduct{a}{b}) = {}^t(\bm{a}{}^t\bm{b}) = {}^t{}^t\bm{b}{}^t\bm{a} = \bm{b}{}^t\bm{a} = \tensorProduct{b}{a}$
\item $(\tensorProduct{a}{b})\bm{c} = (\bm{a}{}^t\bm{b})\bm{c} = \innerProduct{b}{c}\bm{a}$
\item $\innerProduct{c}{(\tensorProduct{a}{b})\bm{d}} = {}^t\bm{c}(\bm{a}{}^t\bm{b})\bm{d} = \innerProduct{c}{a}\innerProduct{b}{d} = \innerProduct{a}{c}\innerProduct{b}{d}$
\end{enumerate}


\section{ベクトル空間(線形空間)}

n次元ベクトル全体の集合を$\vecSpace{n}$とかく。2章の時点ですでに2次元のベクトルやベクトルと行列間に演算を導入していたが、一般的な$\vecSpace{n}$に対して、演算を改めて定義する。

\defi{
$$
\bm{x} = \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix},
\bm{y} = \begin{pmatrix}
y_1 \\
\vdots \\
y_n
\end{pmatrix} \in \vecSpace{n}, a \in \mathbb{R}
$$
とするとき、これらの加法(減法)、スカラー倍はそれぞれ次のように定義される。
}

$$
\bm{x} + \bm{y} = \begin{pmatrix}
x_1 + y_1 \\
\vdots \\
x_n + y_n
\end{pmatrix},
\bm{x} - \bm{y} = \bm{x} + -(\bm{y}) \begin{pmatrix}
x_1 + -(y_1) \\
\vdots \\
x_n + -(y_n)
\end{pmatrix},
a\bm{x} = \begin{pmatrix}
ax_1 \\
\vdots \\
ax_n
\end{pmatrix}
$$

$\vecSpace{n}$が次の条件を満たしたとき、単なる集合ではなく、{\bf n次元ベクトル空間}となる。単にベクトル空間ともいう。

\defi{$V = \vecSpace{n}$とするとき、$V$は以下の条件のもと、ベクトル空間となる}\label{defi:VectorSpace}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item 任意の$\bm{x}, \bm{y}, \bm{z} \in V$に対して、$(\bm{x}+\bm{y}) + \bm{z} = \bm{x}+ (\bm{y} + \bm{z}) $ \quad (結合法則)
\item 任意の$\bm{x}, \bm{y}\in V$に対して、$\bm{x} + \bm{y} = \bm{y} + \bm{x}$ \quad (交換法則)
\item 任意の$\bm{x} \in V$に対して、$\bm{0} + \bm{x} = \bm{x} + \bm{0} = \bm{x}$ \quad $(零ベクトルの存在)$
\item 任意の$\bm{x} \in V$に対して、逆ベクトル$-\bm{x}$が存在して、$\bm{x} + (-\bm{x}) = (-\bm{x}) + \bm{x} = \bm{0}$ \quad (逆ベクトルの存在)
\item{
    任意の$\bm{x}, \bm{y} \in V, a, b \in \mathbb{R}$に対して
    \begin{enumerate}
    \item $a(\bm{x}+\bm{y}) = a\bm{x}+a\bm{y}$
    \item $(a+b)\bm{x} = a\bm{x} + b\bm{x}$
    \item $a(b\bm{x}) = ab(\bm{x})$
    \end{enumerate}
}
\item 任意の$\bm{x} \in V$に対して$1 \cdot \bm{x} = \bm{x} \quad (1 \in \mathbb{R})$
\end{enumerate}

\prop{これらをベクトル空間の公理と認めると、以下が成り立つ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{0}\bm{x} = \bm{0}$
\item $(-1)\bm{x} = -\bm{x}$
\end{enumerate}

{\bf 証明}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item (5)より、$\bm{0}\bm{x} = (\bm{0} + \bm{0})\bm{x} = \bm{0}\bm{x} + \bm{0}\bm{x}$となり、両辺から$\bm{0}\bm{x}$を引くと、$\bm{0} = \bm{0}\bm{x}$
\item $\bm{0} = \bm{0}\bm{x} = (1 + -(1))\bm{x} = 1 \cdot \bm{x} + (-1) \cdot \bm{x} = \bm{x} + (-1)\bm{x}$となり、両辺から$-\bm{x}$を引くと、$-\bm{x} = (-1)\bm{x}$
\end{enumerate}

\subsubsection{単位ベクトル}\label{subsection:unitVector}

特殊なベクトルとして$\bm{e}_1, \cdots \bm{e}_n \in V$があり、それぞれが

$$
\bm{e}_1 = \begin{pmatrix}
1 \\
0 \\
\vdots \\
0
\end{pmatrix},
\bm{e}_2 = \begin{pmatrix}
0 \\
1 \\
\vdots \\
0
\end{pmatrix}, 
\cdots, 
\bm{e}_n = \begin{pmatrix}
0 \\
0 \\
\vdots \\
1
\end{pmatrix}
$$

のように大きさが1になるようなベクトルを、{\bf 単位ベクトル}という。この単位ベクトル$\bm{e}_1, \cdots \bm{e}_n$を使えば、任意の$V$に対して

$$
\bm{x} = \sum_{1 \leq i \leq n} x_ie_i
$$

と一意に表現できる。

\subsection{部分空間}

\defi{$V$をn次元ベクトル空間とする。$V \supset W \neq \phi$とする。$W$が以下の条件を満たすとき、$W$を$V$の部分空間という}\label{defi:subspace}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{x}, \bm{y} \in W \Rightarrow \bm{x} + \bm{y} \in W$
\item $\bm{x} \in W, a \in \mathbb{R} \Rightarrow a\bm{x} \in W$
\end{enumerate}

これらの条件は、Wの中でベクトルとしての演算が閉じていることを示している。

\lemm{$W$が$V$の部分空間であれば、$W$もベクトル空間である}

{\bf 証明}

仮定により$\bm{x} \in W$であるから、定義\ref{defi:subspace}(2)を使うと、$0 \in \mathbb{R}, 0 \cdot \bm{x} = \bm{0} \in W$と$-1 \in \mathbb{R}, (-1)\bm{x} = -\bm{x} \in W$である。したがって、任意の$\bm{x}, \bm{y}$に対して、$\bm{x} - \bm{y} = \bm{x} + (-\bm{y}) \in W$である。ベクトルの演算の公理が$V$で成立しているので、$W$においても成立することは明らか。また零ベクトルと任意のベクトル$\bm{x}の$逆ベクトルも$W$に存在することは分かっているので、$W$もまたベクトル空間である。

つまり、部分空間はベクトル空間として扱うことができるということである。

\prop{$\{\bm{0}\}$、$V(=\vecSpace{n})$は部分空間である}

{\bf 証明}

$\bm{0} + \bm{0} = \bm{0}, \quad a\bm{0} = \bm{0} \quad (\forall a \in \mathbb{R}, \bm{0} \in \{\bm{0}\})$より、$\{\bm{0}\}$は部分空間である。また、$V$はベクトル空間なので、部分空間の条件を満たすことは明らか。これらの部分空間を{\bf 自明な部分空間}と呼ぶ。

\exercise{$V = \vecSpace{3}$において、以下は部分空間か判定せよ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $$
W_1 = \left\{ 
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} \mid x_1 + x_2 + x_3 = 0
\right\}
$$
\item $$
W_2 = \left\{ 
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} \mid 2x_1 + x_2 = 5
\right\}
$$
\end{enumerate}

{\bf 解答}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item 部分空間である。実際、
$$
\bm{x} = \begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix},
\bm{y} = \begin{pmatrix}
y_1 \\
y_2 \\
y_3
\end{pmatrix} \in W_1
$$

をとってきて、$\bm{x} + \bm{y}$を計算すると、$(x_1 + x_2 + x_3) = 0, (y_1 + y_2 + y_3) = 0$より、$(\bm{x}_1 + \bm{y}_1) + (\bm{x}_2 + \bm{y}_2) + (\bm{x}_3 + \bm{y}_3) = \bm{0} \in W_1$である。同様に、$\forall a \in \mathbb{R}$を取ってきて、$a\bm{x} = \bm{0} \in W_1$である。

\item 部分空間ではない。凡例をあげる。

$$
\bm{x} = \begin{pmatrix}
1 \\
3 \\
0
\end{pmatrix},
\bm{y} = \begin{pmatrix}
\dfrac{1}{2} \\[1.5ex]
4 \\
0
\end{pmatrix} \in W_2
$$をとってくる。両方とも上の条件を満たすが、

$$
\bm{x} + \bm{y} = \begin{pmatrix}
\dfrac{3}{2} \\[1.5ex]
7 \\
0
\end{pmatrix}
$$

$2 \cdot \dfrac{3}{2} + 7 \neq 5$であるため、$\bm{x} + \bm{y} \notin W_2$となり、部分空間の必要十分条件に反する。
\end{enumerate}

\defi{
$V = \vecSpace{n}$とする。任意の$\bm{x} \in V$に対し、そのスカラー倍全体の集合$\{a\bm{x}\mid a \in \mathbb{R}\}$は明らかに$V$の部分空間になる。さらに一般的に、$\bm{x_1}, \cdots, \bm{x_r} \in V$が与えられたとき、

$$
\bm{x} = \sum_{1 \leq i \leq r} a_{i}\bm{x}_i \quad (a_1, \cdots, a_r \in \mathbb{R})
$$

のような形で$\bm{x}$が表されるとき、これを$\bm{x_1}, \cdots, \bm{x_r}$の{\bf 線形結合}(linear combination)または{\bf 1次結合}という。
}\label{defi:linearCombination}

$\bm{x_1}, \cdots, \bm{x_r}$の線形結合全体の集合を$W = \{a_1\bm{x_1} + \cdots + a_r\bm{x_r} \mid a_1, \cdots, a_2 \in \mathbb{R} \}$とすると、$W$は$V$の一組の部分空間になる。これを$\bm{x_1}, \cdots, \bm{x}_r$によって張られる部分空間といい、$\langle \bm{x_1}, \cdots, \bm{x_r} \rangle$のように書く。(内積と混同しないように注意したい。)
$\newline$

{\bf 部分空間にならないことの証明}

$$
\bm{0} = \sum_{1 \leq i \leq r} 0\bm{x}_i \in W
$$

より$W \neq \phi$である。$\bm{x}, \bm{y} \in W$とすれば、

$$
\bm{x} = \sum_{1 \leq i \leq r} a_i\bm{x}_i, \quad \bm{y} = \sum_{1 \leq i \leq r} b_i\bm{x}_i \quad (a_i, b_i \in \mathbb{R})
$$

と書くことができる。これより、

$$
\bm{x} + \bm{y} = \sum_{1 \leq i \leq r} (a_i + b_i)\bm{x}_i
$$

とそれぞれのベクトルを$\bm{x}_i$の線形結合で表すことができる。

また、$c \in \mathbb{R}$に対し、

$$
c\bm{x} = c \left( \sum_{1 \leq i \leq r} a_i\bm{x}_i \right) = \sum_{1 \leq i \leq r} (ca_i)\bm{x}_i
$$とこれも$\bm{x}_i$の線形結合で表すことができる。

したがって、$\bm{x} + \bm{y}, \quad c\bm{x} \in W$となることから、Wは部分空間である。

\subsubsection{部分空間の共通部分}

\prop{$W_1, W_2$を$V$の二つの部分空間とするとき、その共通部分$W_1 \cap W_2$は部分空間になる。}

{\bf 証明}

$\bm{0} \in W_1 \cap W_2$であるから、$W_1 \cap W_2 \neq \phi$である。$\bm{x}, \bm{y} \in W_1 \cap W_2$とすれば、$\bm{x} + \bm{y} \in W_1$であり、$\bm{x} + \bm{y} \in W_2$である。（$\bm{x}, \bm{y} \in W_1, \bm{x}, \bm{y} \in W_2$だから。）したがって、$\bm{x} + \bm{y} \in W_1 \cap W_2$。同様に、$a \in \mathbb{R}$に対して、$a\bm{x} \in W_1 \cap W_2$である。


\subsection{部分空間の和}

部分空間の共通部分を考えたときに、今度は和集合（ベクトル空間どうしの和）も部分空間になるかを考えたくなるものである。$W_1, W_2$を$V = \vecSpace{3}$の部分空間とし、$W_1$と$W_2$の和集合$W_1 \cup W_2$を考えてみる。

$W_1 = \langle \bm{e}_1 \rangle, \quad W_2 = \langle \bm{e}_2 \rangle$とおくと、
$$
\begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}, 
\begin{pmatrix}
0 \\
1 \\
0
\end{pmatrix}
\in W_1 \cup W_2
$$

であるが、すべての$a, b \in \mathbb{R}$に対して、

$$
a\bm{e}_1 + b\bm{e}_2 = 
\begin{pmatrix}
a \\
b \\
0
\end{pmatrix}
\notin W_1 \cup W_2
$$

となる。（厳密に言うと、和集合の構成次第で含まれているところもあるかもしれないが、含まれていないところもある。）

そこで、$a\bm{e}_1 + b\bm{e}_2$をすべて含むようにすることを考えるために、$\langle \bm{e}_1, \bm{e}_2 \rangle$で張られる空間を考えると良さそうである。

\defi{一般に二つの部分空間$W_1, W_2$の和を以下のように定義する。}\label{defi:VecSpaceUnion}

$$
W_1 + W_2 = \{ \bm{x} + \bm{y} \mid \bm{x} \in W_1, \bm{y} \in W_2 \}
$$

この演算で定義される部分空間は$V$の最小の部分空間となる。それを次の定理で示す。

\rem{$V = \vecSpace{n}$とし、$W_1, W_2$を$V$の部分空間とする。$W_1 + W_2$は$W_1$と$W_2$を含む最小の部分空間となる。}

{\bf 証明}

まず、$W_1 + W_2$が部分空間であることを示す。

$\bm{0} =\bm{0} + \bm{0} \in W_1 + W_2$より、$W_1 + W_2 \neq \phi$。$\bm{x}, \bm{y} \in W_1 + W_2$とすれば、定義\ref{defi:VecSpaceUnion}により、$\bm{x}',\bm{y}' \in W_1, \bm{x}'', \bm{y}'' \in W_2$と書くことができる。（定義\ref{defi:linearCombination}で示した形式を思い出すと良い。）よって、$\bm{x} + \bm{y} = (\bm{x}' + \bm{x}'') + (\bm{y}' + \bm{y}'') = (\bm{x}' + \bm{y}') + (\bm{x}'' + \bm{y}'')$であり、$a \in \mathbb{R}$に対して、$\bm{x}' \in W_1$より$a\bm{x}' \in W_1$また、$\bm{x}'' \in W_2$より$a\bm{x}'' \in W_2$である。

これらより、$\bm{x}' + \bm{y}' \in W_1, \bm{x}'' + \bm{y}'' \in W_2$であるから、$\bm{x} + \bm{y} \in W_1 \cup W_2$であり、$a\bm{x} \in W_1 \cup W_2$である。($a\bm{x} \in W_1$であるから、和集合である$W_1 \cup W_2$に属するのは当然) よって、$W_1 + W_2$は$V$の部分空間である。

次に、$W_1 + W_2$が最小の部分空間であることを示す。

$\bm{x} \in W_1$とすれば、$\bm{x} = \bm{x} + \bm{0} \quad (\bm{x} \in W_1, \bm{0} \in W_2)$より、$\bm{x} \in W_1 + W_2$である。よって、$W_1 \subset W_1 + W_2$。同様に$W_2 \subset W_1 + W_2$が言え、$W_1 + W_2$は$W_1$と$W_2$を含むことが分かる。ここで、$W$を$W1, W_2$を含む部分空間とすれば、任意の$\bm{x} \in W_1, \bm{y} \in W_2$に対し、$\bm{x}, \bm{y} \in W$であるから、$\bm{x} + \bm{y} \in W$。よって、$W_1 + W_2 \subset W$となり、$W_1 + W_2$は$W_1$と$W_2$を含む最小の部分空間である。

\exercise{
一般に二つの部分空間$W_1, W_2$に対し、$W_1 \subset W_2 \Longleftrightarrow W_1 + W_2 = W_2$であることを示せ。
}

$W_1 \subset W_2$とすれば、$W_1 + W_2 \subset W_2 + W_2 = W_2$。$W_1 + W_2 \supset W_2$は自明であるから、$W_1 + W_2 = W_2$。逆に、$W_1 + W_2 = W_2$とすると、$W_1 \subset W_1 + W_2$であるから、$W_1 \subset W_2$である。

\exercise{
$W_1, W_2, W_3$を$V = \vecSpace{n}$の部分空間とするとき、$(W_1 \cap W_3) + (W_2 \cap W_3) \subset (W_1 + W_2) \cap W_3$を示せ。
}

$\bm{x} \in (W_1 \cap W_3) + (W_2 \cap W_3)$とする。このとき、$\bm{x} = \bm{x}' + \bm{x}''$と分解でき、$\bm{x}' \in W_1 \cap W_3 \hspace{3pt} \& \hspace{3pt} \bm{x}'' \in W_2 \cap W_3$. $\bm{x}', \bm{x}''$が両方とも$W_3$の元より、$\bm{x} \in W_3$また、$W_1, W_2$の定義より、$\bm{x} \in W_1 + W_2$。したがって、$\bm{x} \in (W_1 + W_2) \cap W_3$.

\subsection{線形独立と線形従属}

\defi{r個のベクトル$\bm{x}_1, \cdots \bm{x}_r$は、次の条件が満たされるとき{\bf 線形独立}(linearly independent)または{\bf 一次独立}であるという。}\label{defi:linearlyIndependent}

$$
\sum_{1 \leq i \leq r} a_i\bm{x}_i = 0 \quad (a_i \in \mathbb{R}) \Longrightarrow a_1 = \cdots = a_i = 0
$$

一方、線形独立ではない$\bm{x}, \cdots \bm{x}_r$を{\bf 線形従属}(linearly dependent)または{\bf 1次従属}であるという。$\bm{x}, \cdots \bm{x}_r$が線形従属であるとき以下の1次関係式が成立する。

$$
\sum_{1 \leq i \leq r} a_i\bm{x}_i = 0, \quad a_i \in \mathbb{R} \hspace{3pt} \& \hspace{3pt} \exists a_i \neq 0
$$

\subsubsection{線形独立と線形従属の定義の成り立ち}

線形独立と線形従属の定義は初見では理解が難しいので、この定義の成り立ちを考えてみることにする。

$V = \vecSpace{3}$において3つの基本ベクトル

$$
\bm{e}_1 = \begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}, 
\bm{e}_2 = \begin{pmatrix}
0 \\
1 \\
0
\end{pmatrix}, 
\bm{e}_3 = \begin{pmatrix}
0 \\
0 \\
1
\end{pmatrix}
$$

はそれぞれ独立した方向を向いていると考えられる。つまり、$\langle \bm{e}_1, \bm{e}_2 \rangle$を$xy$平面とすると、$\bm{e}_3$はそれに属さない、つまり$z$軸と考えることができる。これは$\bm{e}_3$は$\bm{e_1}, \bm{e_2}$の線形結合で表されないということである。同様に$\bm{e}_1, \bm{e}_2$のいずれかと$\bm{e}_3$が張る部分空間を考えたときに、残りの単位ベクトルはその部分空間に属さない。つまりこれは

\begin{equation}
    a\bm{e}_1 + b\bm{e}_2 + c\bm{e}_3 = 0 \quad (a, b, c, \in \mathbb{R}) 
\end{equation}

ならば$a = b = c = 0$と言い換えることができる。これは定義\ref{defi:linearlyIndependent}そのものである。なぜ$a = b = c = 0$であれば線形独立なのかを考えたいのであれば、逆に$c \neq 0$とすれば式(2)から

$$
\bm{e}_3 = \left(-\frac{a}{c}\right)\bm{e}_1 + \left(-\frac{b}{c}\right)\bm{e}_2
$$

のように$\bm{e}_3$を$\bm{e}_1$と$\bm{e}_2$の線形結合で表すことが出来てしまう。つまり、$\langle \bm{e}_1, \bm{e}_2 \rangle$に$\bm{e}_3$が含まれていることを意味する。$a \neq 0, b \neq 0$のときも同様である。

\prop{$\bm{x}_1, \bm{x}_2, \cdots \bm{x}_r$が線形独立ならばその1部分も線形独立である。}
線形独立の定義よりこれは明らかなので証明は割愛する。

\exercise{
$V = \vecSpace{3}$において

$$
\bm{x} = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}, 
\bm{z} = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}
$$

が線形独立であることを示せ。
}

$a\bm{x} + b\bm{y} + c\bm{z} = 0 \quad (a, b, c \in \mathbb{R})$であれば、$a = b = c = 0$を示す。

\begin{numcases}
  {}
  a + (-b) = 0 & \\
  a + (-c) = 0 & \\
  a + b + c = 0 &
\end{numcases}

という連立1自方程式を解けば、$a = b = c = 0$が唯一の解となるので、$\bm{x}, \bm{y}, \bm{z}$は線形独立である。

\exercise{
$V = \vecSpace{3}$において

$$
\bm{x} = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
0 \\
1 \\
-1
\end{pmatrix}, 
\bm{z} = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}
$$
は線形独立であるか調べよ。
}

$a\bm{x} + b\bm{y} + c\bm{z} = 0 \quad (a, b, c \in \mathbb{R})$とすると、$a, b, c$は

\begin{numcases}
  {}
  a + c = 0 \\
 -a + b = 0 \\
 -b + c = 0 
\end{numcases}

を満たす。このときこの連立一次方程式は$a = 1, b = 1, c = -1$の解を持つため、$\bm{x}, \bm{y}, \bm{z}$は線形従属である。

\lemm{$\bm{x}_1, \bm{x}_2, \cdots \bm{x}_r \in V$に関して次の2つの条件は同値である。}\label{lemm:linearlyIndependent}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{x}_1, \cdots \bm{x}_r$は線形独立である
\item $\bm{x}_1, \cdots \bm{x}_{r-1}$は線形独立で、$\bm{x}_r \notin \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$
\end{enumerate}

{\bf 証明}

(1) $\Rightarrow$ (2)を示す。(1)を仮定すると、$a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} + a_r\bm{x}_r = 0$ならば、$a_i = 0 \quad (1 \leq i \leq r)$である。(2)が成り立たないとすれば、$\bm{x}_r \in \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$であるから$\bm{x}_r = a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} = 0$と表せる。よって、$a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} + (-1)\bm{x}_r = 0$が成立し$a_r \neq 0$であるから(1)に反する。よって、$\bm{x}_r \notin \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$

(2) $\Rightarrow$ (1)を示す。$\bm{x}_1, \cdots \bm{x}_r$が線形独立でないとすると、ある$a_i \neq 0 \quad (1 \leq i \leq r)$が存在し、$a_1\bm{x}_1 + \cdots + a_{r}\bm{x}_{r} = 0$が成立する。もし$a_r = 0$とすると$a_1, \cdots, a_{r-1}$のいずれかが$0$ではなくなる。これは、(2)の$\bm{x}_1, \cdots \bm{x}_{r-1}$が線形独立であるという仮定に反する。よって、$a_r \neq 0$でなければならない。しかしこのとき、

$$
 \left(-\frac{a_1}{a_r}\right)\bm{x}_1 + \cdots +  \left(-\frac{a_{r-1}}{a_r}\right)\bm{x}_{r-1} = \bm{x}_r \in \langle \bm{x}_1 + \cdots + \bm{x}_{r-1} \rangle
$$
となり(2)の仮定に反する。$\bm{x}_1, \cdots, \bm{x}_{r}$は線形独立である。

\rem{
$\bm{x}_1, \cdots \bm{x}_r$が線形独立であるための必要十分条件は$\bm{x}_1 \neq 0$かつ、任意の$i$に対して$\bm{x}_i \notin \langle \bm{x}_1, \cdots \bm{x}_{i-1} \rangle \quad (2 \leq i \leq r)$である。
}\label{rem:independent}

{\bf 証明}

($\Rightarrow$): 補題\ref{lemm:linearlyIndependent}により、$\bm{x}_1, \cdots, \bm{x}_r$が線形独立であれば、$\bm{x}_1, \cdots \bm{x}_i$も線形独立であるから、この主張が成立する。

($\Leftarrow$): この主張が成立すると仮定する。$\bm{x}_1, \cdots, \bm{x}_i$が線形独立であることは、補題\ref{lemm:linearlyIndependent}を再帰的に行うことで判断できる。まず$\bm{x}_1 \neq 0$から$\bm{x}_1$は線形独立である。次に、$\langle \bm{x}_1 \rangle$に$\bm{x}_2$が含まれていないか、さらにその次に$\langle \bm{x}_1, \bm{x}_2 \rangle$に$\bm{x}_3$が含まていないかといった判定を$\bm{x}_{i-1}$まで再帰的に行っていく。このとき、$\bm{x}_1, \cdots \bm{x}_{i-1}$が線形独立であるとすれば、補題\ref{lemm:linearlyIndependent}により、$\bm{x}_1, \cdots \bm{x}_i$も線形独立である。この判定方法は$i = r$まで成立する。(この操作は高々r回で終わる。)

\subsection{部分空間の基底}

基底の定義をする前に、以下の補題にてベクトル空間を張る線形独立なベクトルは別の線形独立なベクトルに置き換えることができることを示す。

\lemm{$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$とする。$\bm{y}_1, \cdots, \bm{y}_s \in W$が線形独立であるとすれば、$s \leq r$である。}\label{lemm:basisReplacing}

{\bf 証明}

$\{i_1, \cdots, i_{s}\} \subset \{1, \cdots, t\}$をとなる$\{i_1, \cdots, i_{s}\}$とってくる。($\{i_1, \cdots, i_s\}$は、この時点では順不同である。) Wを張る$\bm{x}_1, \cdots, \bm{x}_t$のうち、$\bm{x}_{i_1}, \cdots, \bm{x}_{i_s}$を$\bm{y}_1, \cdots, \bm{y}_s$に置き換える、つまり各$\bm{y}_i$は

$$
\bm{y}_i = \sum_{1 \leq j \leq s} a_j\bm{x}_{i_j}
$$

とかけることを帰納的に示す。まず、$s = 0$のときは自明である。$s \geq 1$として、$\bm{y}_1, \cdots, \bm{y}_{s-1}$に対してこの主張が言えたとする。つまり$\{i_1, \cdots, i_{s-1}\} \subset \{1, \cdots, t\}$をとってきて、$\bm{x}_1, \cdots, \bm{x}_t$の中の$\bm{x}_{i_1}, \cdots, \bm{x}_{i_{s-1}}$が$\bm{y}_1, \cdots, \bm{y}_{s-1}$に替えられたとする。これを、

\begin{eqnarray*}
  x_i' = \left\{
    \begin{array}{l}
      \bm{y}_k \quad i = i_k (1 \leq k \leq s -1) \\
      \bm{x}_i \quad otherwise
    \end{array}
  \right.
\end{eqnarray*}

とおく。このとき、$W = \langle \bm{x}_1', \cdots, \bm{x}_t' \rangle, \bm{y}_s \in W$であるから、$\bm{y}_s = a_1\bm{x}_1' + \cdots + a_t\bm{x}_t'$と表される。しかし、補題\ref{lemm:linearlyIndependent}により$\bm{y}_s \notin \langle \bm{y}_1, \cdots, \bm{y}_{s-1} \rangle = \langle \bm{x}_{i_1}', \cdots, \bm{x}_{i_{s-1}}' \rangle$であるから、ある$i \notin \{i_1, \cdots, i_{s-1} \}$に対して$a_i = 0$である。そのような$i$の1つを$i_s$とする。そのとき、$\bm{x}_{i_s}' = \bm{x}_{i_s}$は、$\bm{x}_1', \cdots, \bm{x}_{i_{s-1}}', \bm{y}_s, \bm{x}_{i_{s + 1}}', \cdots, \bm{x}_t'$の線形結合で表される。よって、$x_{i_s}'$を$\bm{y}_s$で置き換えることができる。

この補題により$1$から$r = s$まで順番にこの置き換えを行うことで、次の定理が得られる。

\rem{$W$を$V = \vecSpace{n}$の部分空間とする。$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle = \langle \bm{y}_1, \cdots, \bm{y}_s \rangle$}で、$\bm{x}_1, \cdots, \bm{x}_r$と$\bm{y}_1, \cdots, \bm{y}_s$がともに線形独立であるとすれば、$r = s$である。

\defi{$W$を$V = \vecSpace{n}$の部分空間とする。$W$に対し、次の条件を満たす順序付けられたベクトルの集合をWの{\bf 基底}(basis)という。}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$
\item $\bm{x}_1, \cdots, \bm{x}_r$は線形独立である。
\end{enumerate}

\rem{$V = \vecSpace{n}$の任意の部分空間$W$に対して基底${x_1, \cdots, x_r}$が存在する。基底をなすベクトルの個数は一定で、$r \leq n$となる}

{\bf 証明}

$W \subset V = \langle \bm{e}_1, \cdots, \bm{e}_n \rangle$であるから、補題\ref{lemm:basisReplacing}により$W$に含まれる線形独立なベクトルの個数は高々nである。$\bm{x}_1, \cdots, \bm{x}_r$をWに含まれる最大個数の線形独立なベクトルの集合とする。そのとき、$r \leq n$で任意の$\bm{x} \in W$に対し、$\bm{x} \in \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$が成立する。よって、$W \subset \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$である。$\langle \bm{x}_1, \cdots, \bm{x}_r \rangle \subset W$は明らかであるから、$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$。よって、$\{\bm{x}_1, \cdots, \bm{x}_r \}$は$W$の基底である。補題\ref{lemm:basisReplacing}により、rは一意的に定まる。

この定理により部分空間Wに対して定まる($r \in \mathbb{R}$)をWの{\bf 次元}(dimension)といい、$\dim W$と書く。

\rem{
$\{\bm{x}_1, \cdots, \bm{x}_n\}$を線形独立なベクトルの組とする。このとき、$\{\bm{x}_1, \cdots, \bm{x}_n\}$の線形結合としての表し方は一意的である。すなわち、$\bm{x} = a_1\bm{x}_1 + \cdots + a_n\bm{x}_n \quad (a_i \in \mathbb{R})$において、$a_i$は$\bm{x}$に対して一意的に定まる。
}\label{rem:VectorExpressionUniquness}

{\bf 証明}

$\bm{x}$の線形結合を$a_i$と$a_i'$を使って以下のように表せたとする。
\begin{eqnarray*}
\bm{x} = a_1\bm{x}_1 + \cdots + a_n\bm{x}_n \\
\bm{x} = a_1'\bm{x}_1 + \cdots + a_n'\bm{x}_n
\end{eqnarray*}

これらについて辺々引くと、

$$
0 = (a_1 - a_1')\bm{x}_1 + \cdots + (a_n - a_n')\bm{x}_n
$$

である。$\{\bm{x}_1, \cdots, \bm{x}_n\}$は線形独立なベクトルの組であるから、$a_1 - a_1' = \cdots = a_n - a_n' = 0$より、$a_1 = a_1', \cdots, a_n = a_n'$である。

\exam{
基本ベクトルの集合, $\langle \bm{e}_1, \cdots, \bm{e}_n \rangle$は明らかに$V = \vecSpace{n}$の基底であるから、$\dim V = n$である。これを$\vecSpace{n}$の{\bf 標準基底}という。
}

\exam{
$\{0\}$は線形独立なベクトルを含まないから、$\dim \{0\} = 0$である
}

\exam{  
$V = \vecSpace{n}$とする。$W, W' \subset V$のとき、$W \subset W', \dim W \leq \dim W'$である。
}

\rem{
$V = \vecSpace{n}$とする。$W, W' \subset V$とする。$W \subset W'$のとき、$\dim W = \dim W' \Longrightarrow W = W'$である。

{\bf 証明}

$r \leq n$とする。$W = \linearCombination{\bm{x}_1}{\bm{x}_n}, W' = \linearCombination{\bm{y}_1}{\bm{y}_r}$とする。$W \subset W', \dim W = \dim W'$であれば、$W$の基底$\vecSet{x}$と$W'$の基底$\{\bm{y}_1, \cdots, \bm{y}_r\}$の個数は一致するから、補題\ref{lemm:basisReplacing}により、

$$
\bm{y}_i = \sum_{1 \leq j \leq n} a_j\bm{x}_{j} \quad (a \in \mathbb{R})
$$

と表すことができる。これより、$W = \linearCombination{\bm{x}_1}{\bm{x}_n} = W' = \linearCombination{\bm{y}_1}{\bm{y}_r}$とすることができる。
}

\exercise{
$V = \vecSpace{3}$において、$V$の部分空間
$$
W = \left\{
\begin{pmatrix}
x \\
y \\
z
\end{pmatrix} \mid x + 2y + z = 0
\right\}
$$
の一組の基底を求めよ。
}

$\newline$
$x + 2y + z = 0$を$x$について解くと、$x = -2y - z$であるから、Wの元は

$$
\begin{pmatrix}
-2y -z \\
y \\
z
\end{pmatrix} = 
y \begin{pmatrix}
-2 \\
1 \\
0
\end{pmatrix} + 
z \begin{pmatrix}
-1 \\
0 \\
1
\end{pmatrix} 
$$

と表すことができる。
ここで、

$$
\begin{pmatrix}
-2 \\
1 \\
0
\end{pmatrix}, \begin{pmatrix}
-1 \\
0 \\
1
\end{pmatrix} 
$$

は線形独立であり、$\vecSpace{2}$を張ることから、基底である。

\subsection{基底の変換}

$V = \vecSpace{n}$の基底として、$\{\bm{a}_1, \cdots, \bm{a}_2\}$と$\{\bm{b}_1, \cdots, \bm{b}_n\}$があったとする。これら2組の基底が、

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} =
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}P
$$

というような関係式であれば、行列$P = (p_{ij}) \in M_n(\mathbb{R})$を$\{\bm{a}_1, \cdots, \bm{a}_2\}$から$\{\bm{b}_1, \cdots, \bm{b}_n\}$への{\bf 基底の変換行列}という。基底の変換行列は次のように求められる。

まず、それぞれの基底は$V$に属しているので、$\{\bm{a}_1, \cdots, \bm{a}_2\}$の各ベクトルは、$\{\bm{a}_1, \cdots, \bm{a}_2\}$の線形結合で表すことができる。

$$
\bm{b}_j = \sum_{1 \leq i \leq n} p_{ij}\bm{a}_j \quad (1 \leq j \leq n)
$$

である。これを書き直すと、

\begin{eqnarray*}
&\bm{b}_1 = P_{11}\bm{a}_1 + \cdots + P_{n1}\bm{a}_n = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{11} \\
\vdots \\
P_{n1}
\end{pmatrix} \\
&\vdots \\
&\bm{b}_n = P_{1n}\bm{a}_1 + \cdots + P_{nn}\bm{a}_n = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{1n} \\
\vdots \\
P_{nn}
\end{pmatrix}
\end{eqnarray*}

となり、これらをまとめると

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{11} & \cdots & P_{1n} \\
\vdots & \vdots & \vdots \\
P_{n1} & \cdots & P_{nn} \\
\end{pmatrix}
$$

だから、

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}P
$$

が得られた。この関係式から

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix}p^{-1} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
$$

も容易に導ける。

\exercise{$\vecSpace{3}$の２つの底を

\begin{eqnarray}
\left\{ 
\bm{a}_1 = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix},
\bm{a}_2 = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix},
\bm{a}_3 = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix} 
\right\} \\
\left\{
\bm{b}_1 = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix},
\bm{b}_2 = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix},
\bm{b}_3 = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix} 
\right\}
\end{eqnarray}

とするとき、(10)から(11)への基底の変換行列を求めよ。
\newline
}\label{exercise:basisTransformation1}

基底の変換行列を$P$とすると、$\bm{b}_i$は$\bm{x}_i$によって以下のように表される。

\begin{eqnarray*}
\bm{b}_1 = P_{11}\bm{a}_1 + P_{21}\bm{a}_2 + P_{31}\bm{a}_3 \\
\bm{b}_2 = P_{12}\bm{a}_1 + P_{22}\bm{a}_2 + P_{32}\bm{a}_3 \\
\bm{b}_2 = P_{13}\bm{a}_1 + P_{23}\bm{a}_2 + P_{33}\bm{a}_3
\end{eqnarray*}

これらをそれぞれ連立一次方程式の形式で表し、それらを解くと

$$
P = \begin{pmatrix}
-\frac{2}{3} & \frac{4}{3} & -\frac{2}{3} \\[1.5ex]
-\frac{2}{3} & -\frac{2}{3} & \frac{4}{3} \\[1.5ex]
\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
\end{pmatrix} 
$$

が得られる。

\section{線形写像}

線形写像とはベクトル空間からベクトル空間への（準同型）写像である。

\subsection{線形写像の定義}

\defi{
$V = \vecSpace{n}, V' = \vecSpace{m}$とする。$V$から$V$'への写像を$f$とし、以下の2つの性質を満たす時$f$は{\bf 線形}(一次)であるという。
}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $f(\bm{x} + \bm{y}) = f(\bm{x}) + f(\bm{y}) \quad (\bm{x}, \bm{y} \in V)$
\item $f(c\bm{x}) = cf(\bm{x}) \quad (\bm{x} \in V, c \in \mathbb{R})$
\end{enumerate}

これらの性質は$f$がベクトル空間上の加法とスカラー倍の2つの演算を保存することを意味する。
（つまり線形性とは、$f$が加法とスカラー倍において準同型であるということである。準同型は群の概念であるため、詳しい話は割愛する。詳しく知りたい方は、群のレポート: https://github.com/noppoMan/math-report/blob/master/group/group.pdfを読まれることを推奨する。)

特に、$V$から$V$自身への線形写像を$V$の{\bf 線形変換}(linear transformation)または${\bf 一次変換}$という

\exam{
$V = \vecSpace{3}, V' = \vecSpace{2}, V'' = \mathbb{R}$とするとき写像

$$
\begin{array}{ccc}
V & \stackrel{f_1}{\longrightarrow}  & V' \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} & \longmapsto & \begin{pmatrix}
x_1 - x_2 \\
x_1 - x_3
\end{pmatrix}
\end{array}
$$

$\newline$

$$
\begin{array}{ccc}
V & \stackrel{f_2}{\longrightarrow}  & V'' \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} & \longmapsto & (x_1 + x_2 + x_3)
\end{array}
$$

$\newline$

などは線形である。$f_2$のように$\mathbb{R}$への線形写像を{\bf 線形汎関数}ともいう。
}

\exam{
$\vecSpace{2}$（平面ベクトル）の原点を中心とした時計、反時計周りに$\theta$だけ回転させるような写像は線形変換である。($\vecSpace{2}$から$\vecSpace{2}$への線形写像)

$$
f_3: \begin{pmatrix}
x \\
y
\end{pmatrix} 
\longmapsto
\begin{pmatrix}
cos\theta - sin\theta \\
sin\theta + cos\theta \\
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
= 
\begin{pmatrix}
x cos\theta - y sin\theta \\
x sin\theta + y cos\theta \\
\end{pmatrix}
$$
}

この線形変換は三角関数の加法定理から導かれる。

\begin{numcases}
  {}
  rcos(\alpha + \beta) = rcos\alpha cos\beta - sin\alpha sin\beta \\
  rsin(\alpha + \beta) = rsin\alpha cos\beta + cos\alpha sin\beta
\end{numcases}

変換前の点P$(x, y)$、変換後の点P'$(rcos(\alpha + \beta), rsin(\alpha, \beta))$とする。

Pの座標$(x, y)$を(9), (10)にそれぞれ代入して、

\begin{eqnarray*}
rcos(\alpha + \beta) = xcos\beta - ysin\beta \\
rsin(\alpha + \beta) = ycos\beta + xsin\beta
\end{eqnarray*}

したがって、これを行列表示すると

$$
\begin{pmatrix}
x cos\beta & -y sin\beta \\
x sin\beta & y cos\beta \\
\end{pmatrix}
$$

となる。

\subsection{線形写像と行列の対応}

一般に$A = (a_{ij}) \in M_{m,n}(\mathbb{R})$が与えられた時、$V = \vecSpace{n}, V' = \vecSpace{m}$への写像$f_A$を

$$
f_A = \bm{x} = \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix} 
\longmapsto 
A\bm{x} = 
\begin{pmatrix}
{\displaystyle \sum_{1 \leq j \leq n} a_{1j}x_j} \\
\vdots \\
{\displaystyle \sum_{1 \leq j \leq n} a_{mj}x_j}
\end{pmatrix}
$$

によって定義すれば、$f_A$は線形である。つまり、

\begin{eqnarray*}
f_A(\bm{x} + \bm{y}) = A(\bm{x} + \bm{y}) = A\bm{x} + A\bm{y} \quad (\bm{x}, \bm{y} \in V) \\
f_A(c\bm{x}) = A(c\bm{x}) = cA\bm{x} \quad (c \in \mathbb{R}, \bm{x} \in V)
\end{eqnarray*}

が成立する。

したがって、上で挙げた例に登場した$f_1, f_2, f_3$はそれぞれ行列

$$
A_1 = \begin{pmatrix}
1 & -1 & 0 \\
0 & 1 & -1
\end{pmatrix} \in M_{2,3}\mathbb{R}, \quad
A_2 = (1, 1, 1) \in M_{1,3}\mathbb{R}, \quad
A_3 = \begin{pmatrix}
cos\theta & -sin\theta \\
sin\theta & cos\theta
\end{pmatrix}
$$

に対応する線形写像になっている。

\subsubsection{表現行列}

$V = \vecSpace{n}, V = \vecSpace{m}$とする。$f: V \to V'$を線形写像とする。$\{\bm{e}_1, \cdots, \bm{e}_n\}, \{\bm{e}_1', \cdots, \bm{e}_n'\}$をそれぞれ$V, V'$の標準基底とし、

$$
f(\bm{e}_j) = \sum_{1 \leq i \leq n} a_{ij}\bm{e}_{i}'
$$

とする。$f(\bm{e}_j) = \bm{a}_j$とすれば、この関係式は以下の写像$f$

$$
\begin{array}{ccc}
V & \stackrel{f}{\longrightarrow}  & V \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\bm{e}_1 & \longmapsto & \bm{a}_1 \\
\bm{e}_2 & \longmapsto & \bm{a}_2 \\
\vdots & & \vdots \\
\bm{e}_n & \longmapsto & \bm{a}_n
\end{array}
$$

を定義する。つまり、

$$
f(\bm{e}_1) = \bm{a}_1 = \begin{pmatrix}
a_{11} \\
\vdots \\
a_{m1}
\end{pmatrix}, 
f(\bm{e}_2) = \bm{a}_2 = \begin{pmatrix}
a_{12} \\
\vdots \\
a_{m2}
\end{pmatrix}, 
\cdots,
f(\bm{e}_n) = \bm{a}_n = \begin{pmatrix}
a_{n1} \\
\vdots \\
a_{mn}
\end{pmatrix}
$$

と$n$個のベクトルが得られる。これを並べると、

$$
A = (\bm{a}_1, \cdots, \bm{a}_n) = 
\begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots \\
a_{m1} & \cdots & a_{mn} \\
\end{pmatrix}
$$

のような行列$A$が得られる。つまり、
$$
f(\bm{e}_1) = A\bm{e}_1, f(\bm{e}_2) = A\bm{e}_2, \cdots, f(\bm{e}_n) = A\bm{e}_n
$$
である。

ここで、$V$の任意のベクトル$\bm{x}$は
$$
\bm{x} = \sum_{1 \leq j \leq n} x_{j}\bm{e}_j = x_1\bm{e}_1 + \cdots x_n\bm{e}_n
$$で表すことができることを思い出してほしい。これらを利用すると以下の関係式が得られる。

\begin{eqnarray*}
f(\bm{x}) &= &f(\sum_{1 \leq j \leq n} x_{j}\bm{e}_j)  \\
&= &f(x_1\bm{e}_1 + \cdots x_n\bm{e}_n) \\
&= &x_1f(\bm{e}_1) + \cdots + x_nf(\bm{e}_n) \\
&= &x_1A\bm{e}_1 + \cdots + x_nA\bm{e}_n \\
&= &Ax_1\bm{e}_1 + \cdots + Ax_n\bm{e}_n \\
&= &A(x_1\bm{e}_1 + \cdots + x_n\bm{e}_n) \\
&= &A\bm{x}
\end{eqnarray*}

つまり、$f$は$A = (a_ij)$に対応する線形写像$f_A$と一致するということである。

\defi{
$V$から$V'$への線形写像を$f$とする。$V$の基底$\{\bm{e}_1, \cdots, \bm{e}_n\}$, V'の基底$\{\bm{e}_1', \cdots, \bm{e}_n'\}$に関して、
$$
(f(\bm{e}_1), f(\bm{e}_2), \cdots, f(\bm{e}_n)) = (\bm{e}_1', \bm{e}_2', \cdots, \bm{e}_n')A
$$

の形に表される時、行列$A$を$f$の{\bf 表現行列}と呼ぶ。
}

\prop{表現行列は一意的である}

{\bf 証明}

定理\ref{rem:VectorExpressionUniquness}より明らか。表現行列を表すベクトル$a_1, \cdots, a_n$はすべて一意的に表せるので、表現行列も一意的である。

\exercise{
$\vecSpace{2}$のベクトル$\bm{x}, \bm{y}$と線形変換$f$の標準基底での表現行列$A$を
$$
\bm{x} = \begin{pmatrix}
2 \\
5
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
1 \\
3
\end{pmatrix}, 
A = \begin{pmatrix}
-3 & 1 \\
2 & -2
\end{pmatrix}
$$

とする。この線形変換$f$で$f(a\bm{x} + b\bm{y}) = a'\bm{x} + b'\bm{y}$となるとき、

$$
B\begin{pmatrix}
a \\
b
\end{pmatrix} = 
\begin{pmatrix}
a' \\
b'
\end{pmatrix}
$$
となる行列を求めよ。
}

$\bm{x}, \bm{y}$は$\vecSpace{2}$を張る標準基底${\bm{e}_1, \bm{e}_2}$で

$$
\begin{pmatrix} 
\bm{x} & \bm{y} 
\end{pmatrix} = 
\begin{pmatrix} 
\bm{e}_1 & \bm{e}_2
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix}
$$
と表すことができる。

$$
P = \begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix}
$$とする。


与式$f(a\bm{x} + b\bm{y}) = a'\bm{x} + b'\bm{y}$より、

$$
f(a\bm{x} + b\bm{y}) = A\begin{pmatrix} \bm{x} & \bm{y} \end{pmatrix}
\begin{pmatrix}
a \\
b
\end{pmatrix} = 
AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

また、

$$
a'\bm{x} + b'\bm{y} = 
\begin{pmatrix}
\bm{x} & \bm{y}
\end{pmatrix}
\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P\begin{pmatrix}
a' \\
b'
\end{pmatrix}
$$

の関係式が得られる。これより、

$$
P\begin{pmatrix}
a' \\
b'
\end{pmatrix} = 
AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

である。これに左から$P^{-1}$を掛けて（ここでは、$P$が可逆であることを前提に解答を進める。可逆の判定は行列式の章で解説する。）

$$
P^{-1}P\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P^{-1}AP\begin{pmatrix}
a \\
b
\end{pmatrix}
\Longleftrightarrow
\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P^{-1}AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

つまり、

$$
P^{-1}AP = 
\frac{1}{2 \cdot 3 -5 \cdot 1}\begin{pmatrix}
3 & -1 \\
-5 & 2
\end{pmatrix}
\begin{pmatrix}
-3 & -1 \\
2 & -2
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix} = 
\begin{pmatrix}
3 & 4 \\
-7 & -8
\end{pmatrix}
$$

この例題より以下の定理が得られる。

\rem{
標準基底での$\vecSpace{n}$上の線形変換$f$の表現行列を$A$とする。標準基底$\{\bm{e}_1, \cdots, \bm{e}_n\}$を、基底$\{\bm{x}_1, \cdots, \bm{x}_n\}$に取り替えたとする。
基底の変換行列をPとおくと、基底$\{\bm{x}_1, \cdots,  \bm{x}_n\}$を座標系とした新座標での線形変換$f$の表現行列は$P^{-1}AP$である。
}

\defi{
実数係数を持つ$x$の多項式全体の集合も、通常の加法・スカラー倍に関してベクトル空間である。これを

$$
\polynomialVecSet{n} = \{ a_nx^n + a_{n-1} + \cdots + a_1x + a_0 \mid a_i \in \mathbb{R} (1 \leq i \leq n)\}
$$
と表記する。
}

\exercise{
$\polynomialVecSet{4} = \{a_4x^4 + a_3x^3 + a_2x^2 + a_1x + a_0 \mid a_i \in \mathbb{R}\}$を実数係数の4次以下の多項式全体とする。線形写像$F: \polynomialVecSet{4} \to \polynomialVecSet{4}$を

$$
F(f) = \frac{df(\bm{x})}{dx}
$$
とするとき、基底$\{1, x, x^2, x^3, x^4\}$に関する$F$の表現行列を求めよ。
}

$\newline$

$$
\bm{a} = \begin{pmatrix}
a_1 \\
a_2 \\
a_3 \\
a_4 \\
a_5
\end{pmatrix}
$$を$\vecSpace{5}$の標準基底とする。写像$g: \vecSpace{5} \to \polynomialVecSet{4}$を

$$
g(\bm{a}) = a_1 + a_2 \cdot x + a_3 \cdot x^2 + a_4 \cdot x^3 + a_5 \cdot x^4
$$

と定めると、これは$\vecSpace{5}$の標準基底に$\polynomialVecSet{4}$の基底$\{1, x, x^2, x^3, x^4\}$を対応させる線形写像である。線形写像$g$に対し、$F$を適用すると

\begin{eqnarray*}
F(g(\bm{a})) &= &F(a_1 + a_2 \cdot x + a_3 \cdot x^2 + a_4 \cdot x^3 + a_5 \cdot x^4) \\
&= &a_1F(1) + a_2F(x) + a_3F(x^2) + a_4F(x^3) + a_5F(x^4) \\
&= &a_1 \cdot 0 + a_2 \cdot 1 + a_3 \cdot 2x + a_4 \cdot 3x^2 + a_5 \cdot 4x^3
\end{eqnarray*}

となる。この結果を書き直すと

$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 0 & 4 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
\begin{pmatrix}
1 & x & x^2 & x^3 & x^4
\end{pmatrix}\bm{a}
$$

となっているので、Fの表現行列

$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 0 & 4 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
$$

が得られる。この表現行列より、

$$
(F(1), F(x), F(x^2), F(x^3), F(x^4)) = (1, x, x^2, x^3, x^4)F
$$

の関係式が成立する。


\subsection{線形写像の合成と行列の積}

線形写像$f_A$と$f_B$があるとき、$f_A \circ f_B = f_{AB}$が成立する。実際、任意の$\bm{x} \in \vecSpace{n}$に対して

$$
f_A \circ f_B(\bm{x}) = f_A(f_B(\bm{x})) = A(B\bm{x}) = (AB)\bm{x} = f_{AB}(\bm{x})
$$

であるから、$f_A \circ f_B = f_{AB}$が得られた。

この関係式を具体的な線形写像$f_A, f_B$を使って表すと面白いことが分かる。

$$
A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix}, 
B = \begin{pmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{pmatrix}
$$

を表現行列とし、線形写像$f_A$と$f_B$の合成を考える。

\begin{eqnarray*}
f_A \circ f_B(\bm{x}) = f_A(f_B(\bm{x})) &= &f_A(\begin{pmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{pmatrix} \begin{pmatrix}
x \\
y
\end{pmatrix}) \\
&= &f_A(\begin{pmatrix}
xb_{11} + yb_{12} \\
xb_{21} + yb_{22}
\end{pmatrix}) \\
&= &\begin{pmatrix}
a_{11} + a_{12} \\
a_{21} + a_{22}
\end{pmatrix} 
\begin{pmatrix}
xb_{11} + yb_{12} \\
xb_{21} + yb_{22}
\end{pmatrix} \\
& = &\begin{pmatrix}
a_{11}(xb_{11} + yb_{12}) + a_{12}(xb_{21} + yb_{22}) \\
a_{21}(xb_{11} + yb_{12}) + a_{22}(xb_{21} + yb_{22}) \\
\end{pmatrix} \\
& = &\begin{pmatrix}
(a_{11}b_{11} + a_{12}b_{21})x + (a_{11}b_{12} + a_{12}b_{22})y \\
(a_{21}b_{11} + a_{22}b_{21})x + (a_{21}b_{12} + a_{22}b_{22})y
\end{pmatrix} \\
& = &\begin{pmatrix}
a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix} \\
&= &(AB)\bm{x} \\
\end{eqnarray*}

が得られる。つまり、行列の積は線形写像の合成に等しいということが分かったのである。

したがって、行列の積は線形写像の合成に対応するように定義されていたのだ。

\subsection{線形写像の核と像}

$V = \vecSpace{n}, V' = \vecSpace{m}$とし、線形写像$f: V \to V'$が与えられたとする。
$f$によって、$V'$のベクトル$0_{V'}$に移される$V$のベクトル全体の集合

$$
\ker(f) = \{\bm{x} \in V \mid f(\bm{x}) = \bm{0}\}
$$

を$f$の{\bf 核}(Kernel, カーネル)という。また、$V$の$f$による像全体の集合

$$
\img(f) = \{f(\bm{x}) \mid \bm{x} \in V\}
$$

を$f$の{\bf 像}(Image, イメージ)という。

この定義から、$f$が全射$\Longleftrightarrow \img(f) = V'$である。また、$f$が単射であれば、$\ker(f) = \{0\}$である。これを証明する。

$f$が単射であるとする。$\forall \bm{x} \in \ker(f)$をとると、$f(\bm{x}) = \bm{0}$.ここで、$f$は線形写像より、$f(\bm{0}) = \bm{0}$.よって、$f(\bm{x}) = \bm{0}$ .仮定より、$f$は単射なので$\bm{x} = \bm{0}$.よって、$\ker(f) = \{0\}$となる。

逆に、$f(\bm{x}) = f(\bm{y})$となる$\forall \bm{x}, \bm{y} \in V$に対し、$f(\bm{x}) = f(\bm{y}) = 0$。ここで$f$は線形写像なので、$f(\bm{x} - \bm{y}) = \bm{0}$が言え、$\bm{x} - \bm{y} \in \ker(f) = \{0\}$である。したがって、$\bm{x} - \bm{y} = \bm{0}$より、$\bm{x} = \bm{y}$.

\rem{
$\img(f), \ker(f)$はそれぞれ$V', V$の部分空間である。
}

{\bf 証明}

$\img(f) \subset V'$を示す。$\bm{x}, \bm{y} \in \img(f)$とすれば、ある$\bm{x}, \bm{y} \in V$があって、$\bm{x}' = f(\bm{x}), \bm{y}' = f(\bm{y})$と表せる。よって、

\begin{eqnarray*}
\bm{x}' + \bm{y}' = f(\bm{x}) + f(\bm{y}) = f(\bm{x} + \bm{y}) \in \img(f) \\
c\bm{x}' = cf(\bm{x}) = f(c\bm{x}) \in \img(f)
\end{eqnarray*}

より、$\img(f)$は$V'$の部分空間である。

$\ker(f) \subset V$を示す。$\bm{x}, \bm{y} \in Ker(f)$とすれば、$f(\bm{x}) = f(\bm{y}) = 0$であるから

\begin{eqnarray*}
f(\bm{x} + \bm{y}) = f(\bm{x}) + f(\bm{y}) = \bm{0} \in Ker(f) \\
cf(\bm{x}) = f(c\bm{x}) = \bm{0} \in Ker(f)
\end{eqnarray*}

より、$\ker(f)$は$V$の部分空間である。

\subsection{次元定理}

\rem{
$V = \vecSpace{n}, \dim V = n$のとき、次の等式が成立する。

$$
\dim \img(f) = n - \dim \ker(f)
$$
}

{\bf 証明}

$\dim \ker(f) = s$とし、$\{\bm{x}_1, \cdots, \bm{x}_s\}$を$\ker(f)$の1つの基底とする。$s \leq n$で$V$の元$\bm{x}_{s + 1}, \cdots, \bm{x}_n$を適当に選び、$\vecSet{x}$を$V$の基底とすることができる。このとき、$f(\bm{x}_i) = \bm{0} \quad (1 \leq i \leq s)$であるから、

$$
\img(f) = \linearCombination{f(\bm{x}_1)}{f(\bm{x}_n)} = \linearCombination{f(\bm{x}_{s + 1})}{f(\bm{x}_n)}
$$

である。$f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$が線形独立であれば、このベクトルの組が$\img(f)$の基底となり、$\dim \img(f) = n - \dim \ker(f)$が証明される。$f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$が線形独立であることを示す。

$$
\sum_{s + 1 \leq j \leq n} c_jf(\bm{x}_j) = \bm{0} \quad (c_j \in \mathbb{R})
$$
と仮定すると、$f$は線形より、

$$
f(\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j) = \bm{0}
$$

であるから、

$$
\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j \in \ker(f)
$$

したがって、

$$
\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j = \sum_{1 \leq i \leq n} k_i\bm{x}_i \quad (k_i \in \mathbb{R})
$$

とかける。つまり、$\{\bm{x}_1, \cdots, \bm{x}_s\} \subset \vecSet{x}$であるが、$\{\bm{x}_1, \cdots, \bm{x}_s\}$は$f$により$\bm{0}$に潰れてしまうので、$s \leq n$であっても右辺と左辺の$\bm{0}$に移されないベクトルの個数は
一致するということを言っている。

ここで、$\bm{x}_1, \cdots, \bm{x}_n$は$V$の基底より、線形独立であるから、すべての$j$で$c_j = 0$、すべての$i$で$k_i = 0$でなければならない。よって、$f$によって移される新たな$\img(f)$の基底 $f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$も線形独立である。

また、この定理により次の等式が成立することが直ちに分かる。

$$
\dim V = \dim \ker(f) + \dim \img(f)
$$

$\newline$

\exercise{
$$
A = \begin{pmatrix}
-2 & 1 & 1 \\
1 & -2 & 1 \\
1 & 1 & -2 
\end{pmatrix}
$$
によって定義される線形写像$f: \bm{x} \mapsto A\bm{x} \quad (\bm{x} \in \vecSpace{3})$に対する$\ker(f)$と$Im(f_A)$とぞれぞれの次元を求めよ。
}\label{exercise:FindingTheImageAndKernel1}

$\ker(f)$とその次元を求める。$f$のカーネルは$f$に写すと$\bm{0}$になるベクトルの集合であるから、$A\bm{x} = \bm{0}$.
つまり

$$
\begin{cases}
-2x + y + z = 0 \\
x - 2y + z = 0 \\
x + y - 2z = 0
\end{cases}
$$

を意味する。この連立方程式の解は$x = y = z$であるから

$$
\ker(f) = \left\{ \bm{x} = 
\begin{pmatrix} 
x \\
y \\
z
\end{pmatrix}
\mid x = y = z
\right\}
$$

である。ここから基底を取り出すことを考える。$\bm{x}$の成分はすべて同じ値なので、いずれかの成分を固定する（例えば、$x$で$y,z$を置き換える）ことで

$$
\bm{x} = x\begin{pmatrix} 
1 \\
1 \\
1
\end{pmatrix}
$$

と表すことができる。この1つのベクトル$\bm{x}$は線形独立であり、カーネルを張る基底であるから、$\dim \ker(f) = 1$である。

次に、$\img(f)$とその次元を求める。次元定理を使えば、$\dim \img(f) = 2$となるはずであるから、

$$
\img(f) = \left\{ \bm{x} = 
\begin{pmatrix} 
x \\
y \\
z
\end{pmatrix}
\mid x + y + z = 0
\right\}
$$

と仮定する。ここで、$x$を$x = -y + (-)z$とおけば$y$と$z$で$\bm{x}$を表現できるから、
$$
\bm{x} = y\begin{pmatrix} 
-1 \\
1 \\
0
\end{pmatrix} + z\begin{pmatrix} 
-1 \\
0 \\
1
\end{pmatrix}
$$

より、2つの線形独立なベクトル
$$
\left\{
\begin{pmatrix} 
-1 \\
1 \\
0
\end{pmatrix}, 
\begin{pmatrix} 
-1 \\
0 \\
1
\end{pmatrix}
\right\}
$$

が得られる。これらは$\vecSpace{2}$を張る基底となるから、仮定は正しいことが証明された。


\subsection{行列の階数}

\defi{
線形写像$f: V \to V'$に対し、$\dim \img(f)$を$f$の{\bf 階数}（rank, ランク）といい、$\rank f$とかく。$\rank f = \dim \img(f)$である。

また、$A \in M_{m,n}(\mathbb{R})$に対してその階数を$\rank A = \rank f_A = \dim A \vecSpace{n}$と定義する。したがって、$\rank A$は$A$の列ベクトルの中で線形独立なものの最大個数である。
}\label{defi:rankOfMatrix}

\exam{
例題\ref{exercise:FindingTheImageAndKernel1}の行列$A$のランクは$\rank A = 2$である。
実際、定義\ref{defi:rankOfMatrix}に従えば、$\dim \img(f) = 2$なので、$\rank f = \dim \img(f)$である。
}

\exam{
$A = \bm{x}{}^t\bm{y}, \bm{x} \in \vecSpace{m}, \bm{y} \in \vecSpace{n}, \bm{a}, \bm{b} \neq \bm{0}$とすれば、$\rank A = 1$である。

$\bm{y} = \bm{y_i}$とすれば、$A$の列ベクトルは$y_1\bm{x}, \cdots, y_n\bm{x}$であるから、

$$
\rank A = \dim \langle y_1\bm{x}, \cdots, y_n\bm{x} \rangle = \dim \langle \bm{x} \rangle = 1
$$
である。
}

\rem{
$A \in M_{m,n}(\mathbb{R}), f_A: V \to V'$とすれば、
\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\rank A \leq \min(m, n)$
\item $\rank A = m \Longleftrightarrow f$は全射で、$n \geq m$
\item $\rank A = n \Longleftrightarrow f$は単射で、$n \leq m$
\end{enumerate}
}

{\bf 証明}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\img(f_A) \subset V'$であるから、$\dim \img(f_A) \leq \dim V' = m$.よって、$\rank A \leq m$である。しかし、$\dim \img(f_A) = n - \dim \ker(f)$より、$\dim \img(f_A) \leq n$.よって、$\rank A \leq \min(m, n)$
\item $\rank A = m$ならば、$\img(f) \subset V', \dim \img(f) = \dim V'$

TODO

\end{enumerate}

\begin{thebibliography}{n}
\bibitem[1]{key2} 佐武一郎 [線形代数] 共立出版株式会社
\end{thebibliography}

\end{document}
