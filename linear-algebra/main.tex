\documentclass[dvipdfmx,autodetect-engine]{jsarticle}
\usepackage{tikz}
\usepackage{graphicx,fancybox,ascmac, amsmath, amssymb}
\usepackage[all]{xy}
\usepackage{bm}
\usepackage{cases}
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage{amsthm}
\theoremstyle{definition}

\newtheorem{theo}{定理}[section]
\newtheorem{defi}[theo]{定義}
\newtheorem{rem}[theo]{定理}
\newtheorem{exam}[theo]{例}
\newtheorem{exercise}[theo]{例題}
\newtheorem{prop}[theo]{命題}
\newtheorem{ques}[theo]{問}
\newtheorem{lemm}[theo]{補題}
\newtheorem{cor}[theo]{系} % Corollary
\newtheorem{Proof}{証明}
\newtheorem*{Proof*}{証明}
\newtheorem{Answer}{解答}
\newtheorem*{Answer*}{解答}

% functions %
\newcommand{\innerProduct}[2]{\langle \bm{#1}, \bm{#2} \rangle}
\newcommand{\tensorProduct}[2]{\bm{#1} \otimes \bm{#2}}
\newcommand{\transposeMat}[1]{{}^t\!{#1}}
\newcommand{\transposeVec}[1]{{}^t\!{\bm{#1}}}
\newcommand{\vecSpace}[1]{\mathbb{R}^{#1}}
\newcommand{\polynomialVecSet}[1]{\mathbb{R}[x]_{#1}}
\newcommand{\vecSet}[1]{\{\bm{#1}_1, \cdots, \bm{#1}_n\}}
\newcommand{\linearCombination}[2]{\langle #1, \cdots, #2\rangle}
\newcommand{\rank}[1]{{\rm rank}\,#1}
\newcommand{\img}[0]{{\rm Im}}
\newcommand{\nrm}[1]{\|\bm{#1}\|}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\setcounter{tocdepth}{3}

\title{線形代数}

\author{武井優己}
\date{\today}
\begin{document}
\maketitle

\tableofcontents

\section{2次行列の演算}

線形代数では、数の拡張としてベクトル、行列という高次元の数を考える。いきなりn次元に目を向けるのは難しいので、ウォーミングアップとして2次元の行列に対して、通常の数と同様の加法と乗法を天下り的に定義する。ここでは、行列は我々の知る実数に似た代数的構造を持つ（実数のような演算ができる）ということが単に分かればよい。なぜそうなるかはこのレポートを読み進めていくうちに自ずと理解できるようになってくる。

$A, A'$をそれぞれ以下の2次（正方）行列とし、行列の演算を進めていくことにする。2次正方行列とは行と列がそれぞれ$2 \times 2$の形をした行列のことである。

$$
A = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix},
A' = \begin{pmatrix}
a' & b' \\
c' & d' \\
\end{pmatrix}
$$

$A$であれば、$(a, b), (c, d)$の横の並びが行、$\begin{pmatrix}
a \\
c \\
\end{pmatrix}, \begin{pmatrix}
b \\
d \\
\end{pmatrix}$の縦の並びが列に該当する。


\subsection{2次行列の加法}

$A, A'$に対し、その和を

$$
A + A' = \begin{pmatrix}
a + a' & b + b' \\
c + c' & d + d'\\
\end{pmatrix}
$$

によって定義する。

$A,A'$の他に、

$$
A'' = \begin{pmatrix}
a'' & b'' \\
c'' & d'' \\
\end{pmatrix}
$$

があるとすれば、これらの和は

$$
(A + A') + A'' = \begin{pmatrix}
(a + a') + a'' & (b + b') + b'' \\
(c + c') + c'' & (d + d') + d'' \\
\end{pmatrix}
$$

$$
A + (A' + A'') = \begin{pmatrix}
a + (a' + a'') & b + (b' + b'') \\
c + (c' + c'') & d + (d' + d'') \\
\end{pmatrix}
$$

と書くことができる。

ここで、和で表される各成分は実数における加法の形になっているため、

$$
(A + A') + A'' = A + (A' + A'')
$$

と結合法則が成立する。同様に、
$$
A + A' = A' + A
$$

と交換法則も成立する。

\subsubsection{零行列}

すべての成分が0である行列を{\bf 零行列}といい

$$
0 = \begin{pmatrix}
0 & 0 \\
0 & 0 \\
\end{pmatrix}
$$

と書く。演算において零行列は単に$0$として表し、$A + 0 = 0 + A = A$が成立する。

\subsubsection{2次行列の減法}

加法の逆演算として減法も可能である。$X + A = A'$となる2次行列

$$
X = \begin{pmatrix}
x & y \\
z & w \\
\end{pmatrix}
$$

が存在する。このような$X$があったとすれば、$A'$のそれぞれの成分は

\begin{eqnarray*}
x + a = a', \quad y + b = b' \\
z + c = c', \quad w + d = d' \\
\end{eqnarray*}

であるから、$X$の成分はそれぞれ

\begin{eqnarray*}
x = a' - a, \quad y = b' - b \\
z = c' - c, \quad w = d' - d \\
\end{eqnarray*}

でなければならない。これより、$X$の成分は$A, A'$の差の唯一の解


$$
X = \begin{pmatrix}
a' - a & b' - b \\
c' - c & d' - d \\
\end{pmatrix}
$$

をもつ。この$X$を$A'-A$とするのが行列の減法である。

\subsection{2次行列の乗法}\label{subsubsection:matrixMultiple}

\subsubsection{2次行列のスカラー倍}

$k \in \mathbb{R}$に対して

$$
kA = \begin{pmatrix}
ka & kb \\
kc & kd
\end{pmatrix}
$$

となるような演算をスカラー倍(乗法)という。また、

$$
A0 = 0A = 0
$$

$-A = -1 \times A$より、

$$
A(-A') = (-A)A' = -AA'
$$

である。

\subsubsection{2次行列の積}

二つの2次行列$A, A'$の積を次のように定義する。

$$
AA' = \begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\begin{pmatrix}
a' & b' \\
c' & d'
\end{pmatrix} = 
\begin{pmatrix}
aa' + bc' & ab' + bd' \\
ca' + dc' & cb' + dd' \\
\end{pmatrix}
$$

$A$の$i$行と$A'$の$j$列の対応する成分の積の和を$AA'$の$(i, j)$成分と定義するのが行列の積である。なぜ加法のような対応ではないのかと疑問に思うが、それは本レポートを読み進めるうちに理解できるようになる。

積についても、結合法則が成立する。

$$
A'' = \begin{pmatrix}
a'' & b'' \\
c'' & d'' \\
\end{pmatrix}
$$

とすると、

$$
(AA')A'' = A(A'A'')
$$

である。しかし、2次行列の積に関しては交換法則

$$
AA' = A'A
$$

は一般には成立しない。実際、$AA'$の$(1, 1)$成分は$aa' + bc'$であるが、$A'A$の$(1, 1)$成分は、$a'a + b'c$となり、$AA' \neq A'A$である。

加法と乗法を組み合わせると、

\begin{eqnarray*}
A(A' + A'') = AA' + AA'' \\
(A' + A'')A = A'A + A''A
\end{eqnarray*}

のような分配法則が成立する。

このように、2次正方行列は一部の例外を除きほぼ実数と同じような振る舞いをすることが分かる。（これは、群・環・体といった代数的構造の賜である）

\section{一般の行列とベクトル}

\subsection{行列の成分表示}

$m \times n$個の数を矩形に並べ括弧で囲んだものを{\bf $m$行$n$列の行列}あるいは、{\bf $m \times n$行列}という。$m \times n$行列の全体の集合を$M_{m,n}(\mathbb{R})$と書き、$m = n$であるとき、{\bf $n$次正方行列}といい、$M_n(\mathbb{R})$と書く。行列を構成するそれぞれの数を{\bf 成分}と呼ぶ。

\defi {一般の行列の成分表示}

上述の例より、一般の $m \times n$行列$A$を成分表示すると次のようになる。 $A$の $(i, j)$成分を $(a_{ij})$とするとき

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix}
$$

である。

多くの場合これを省略して、$A = (a_{ij})$と表記する。ただし、
$(1 \leq i \leq m), (1 \leq j \leq n)$

\exam {$2 \times 2$行列の表示}

$m = 2, n = 2$のとき、2次正方行列Aは以下のように表示される

$$
A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix}
$$

\exam {$4 \times 5$行列の表示}

$m = 4, n = 5$のとき、$4 \times 5$行列Aは以下のように表示される

$$
A = \begin{pmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
\end{pmatrix}
$$

\subsection{一般の行列の加法、スカラー倍}\label{subsection:generalMatrixAdditionAndScalarMultiple}

\defi{一般の行列の加法、スカラー倍}\label{defi:additionAndScalarMultiple}

$A, B \in M_{m,n}(\mathbb{R}), A = (a_{ij}), B = (b_{ij})$とし、加法、スカラー倍を以下のように定義する。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A + B = (a_{ij}) + (b_{ij})$
\item $cA = (ca_{ij}) \quad (c \in \mathbb{R})$
\item $(-1)A = -A, \quad A + (-B) = A - B$
\end{enumerate}

\prop{定義\ref{defi:additionAndScalarMultiple}より、以下の法則を満たす}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $(A + B) + C = A + (B + C)$ \quad (結合法則)
\item $A + B = B + A$ \quad (交換法則)
\item $c(A+B) = cA + cB$ \quad (分配法則)
\end{enumerate}

である。

\begin{Proof*}
\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $a_{ij}とb_{ij}の各成分どうしに結合法則が成り立つことより明らか$
\item $a_{ij}とb_{ij}の各成分どうしに交換法則が成り立つことより明らか$
\item $A + B = a_{ij} + b_ij$より、$c(A + B) = c(a_{ij} + b_ij)$である。
$cA = ca_{ij}, cB = cb_{ij}$であるから、$cA + cB = ca_{ij} + cb_{ij}$となる。
ここで実数における分配法則により、$c(a_{ij} + b_{ij}) = ca_{ij} + cb_{ij}$より、すべての(i, j)について分配法則が適用されるから、$c(A+B) = cA + cB$である。
\end{enumerate}
\end{Proof*}

\defi{零行列との加法、スカラー倍}

$A \in M_{m,n}(\mathbb{R})$とするとき、零行列$0$との加法、スカラー倍で以下が成立する。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A + 0 = 0 + A = A$
\item $A + (-A) = (-A) + A = 0$
\item $1A = A$
\item $0A = 0$
\end{enumerate}


\subsection{一般の行列の積}

\defi{$l, m, n \in \mathbb{Z}$とし、$A \in M_{l,m}(R), B \in M_{m,n}(\mathbb{R})$とするとき、積ABを以下のように定義する。}\label{defi:matrixMultiple}

$$
AB = \sum_{1 \leq k \leq m} a_{ik}b_{kj} \in M_{l, n}(\mathbb{R})
\quad (ただし、1 \leq i \leq l, 1 \leq j \leq n)
$$

\ref{subsubsection:matrixMultiple}で紹介した$2 \times 2$行列の積がまさにこのようになっている。このように行列の積を定義する理由は、線形写像の章で説明する。

\prop{一般の行列の積について以下の法則が成立する}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A(BC) = (AB)C$
\item $A(B+C) = AB + AC$
\item $(B+C)A = BA + CA$
\end{enumerate}

これらも$AB$の各成分どうしが実数の積と和の形をしていることから、実数の演算における法則が適用できる。これにより、実際に演算することでこれらの証明は容易である。
(2)と(3)が同値でない理由として、行列の積は非可換なことが挙げられる。
実際、$AB$と$BA$を行列の積の定義に従い演算すると、それぞれ異なる行列が得られ、$AB \neq BA$となる。そのため、$A(B+C) \neq (B+C)A$となる。

\subsection{単位行列}

\defi

$A \in M_n(\mathbb{R})$に対して$AE = EA = A$となるような行列を{\bf $n$次単位行列}という。単に、単位行列ということもある。

単位行列は、

$$
E = \begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & 1 \\
\end{pmatrix}
$$

のように対角成分が$1$でそれ以外が$0$から構成される行列である。


\subsection{逆行列}

n次行列Aに対し$AX = E, XA = E$を満たすようなn次行列$X$が存在するとき、$A$は{\bf 可逆}である、または{\bf 正則行列}という。

\rem{n次行列Aが可逆であれば、$AX=E$および$YA=E$はそれぞれ唯一の解を持ち、それらの解は一致する。そのような$X, Y$を$A^{-1}$と書き、{\bf 逆行列}という。}

\begin{Proof*}
$AX = XA = E, AY = YA = E$となる行列$X, Y$があるとする。このとき、結合法則により

\begin{eqnarray*}
X &= &XE \\
  &= &X(AY) \\
  &= &(XA)Y \\
  &= &EY \\
  &= &Y
\end{eqnarray*}

となり、$X = Y$である。仮に、$AB = BA = E, AC = CA = E$となる行列$B, C$があったとすれば、同様の理由により$X = Y = B = C$とこれらの方程式の解は一意に定まる。
\end{Proof*}

\exam{単位行列の逆行列}

単位行列Eは$EE = E$であるから$E^{-1} = E$であり可逆である。$A, B \in M_n(\mathbb{R})$が可逆ならば、$AB$も可逆で$(AB)^{-1} = B^{-1}A^{-1}$である。
これが言える理由として、$(AB)(B^{-1}A^{-1}) = (B^{-1}A^{-1})(AB) = E$だからである。また、$A$が可逆であれば$A^{-1}$も可逆であり、$(A^{-1})^{-1} = A$。つまり、$(A^{-1})A = A(A^{-1}) = E$である。

これは、$M_n(\mathbb{R})$の可逆元全体が群の性質を持つことを言っており、この行列全体の集合を一般線形群といい、$GL_n(\mathbb{R})$と表される。

\subsection{列ベクトルと行ベクトル}

\subsubsection{(n, 1)型行列}

$$
B = \begin{pmatrix}
b_{11} \\
\vdots \\
b_{n1}
\end{pmatrix}
$$

を{\bf n-dim列ベクトル}という。

本レポートでは、n-dim列ベクトルを

$$
\bm{b} = \begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

のようにボールドのアルファベットで表記する。

\subsubsection{(1, n)型行列}

$(b_{11}, \cdots, b_{n1})$を{\bf n-dim行ベクトル}という。

\subsubsection{ベクトルの演算}

n-dim列ベクトルは(n,1)型の行列であるから、やはり加法と乗法が成立する。以降の節の理解を円滑に進めるに当たり、ここではベクトルの演算に関する細かな定義や性質の説明は飛ばす。(3章のベクトル空間にて、この辺りの細かい議論を行う) よって、予めベクトルの演算はできるものとして気楽に考えてほしい。

2次元ベクトルにおいて、

$$
\bm{a} = \begin{pmatrix}
x \\
y
\end{pmatrix},
\bm{b} = \begin{pmatrix}
x' \\
y'
\end{pmatrix}
$$
を取ってくる。

このとき、$a + b$は

$$
\begin{pmatrix}
x \\
y
\end{pmatrix} + 
\begin{pmatrix}
x' \\
y'
\end{pmatrix}
= \begin{pmatrix}
x + x' \\
y + y'
\end{pmatrix}
$$

である。

また、$c \in \mathbb{R}$に対して、

$$
c\bm{a} =
c\begin{pmatrix}
x \\
y
\end{pmatrix}
= \begin{pmatrix}
cx \\
cy
\end{pmatrix}
$$

である。

\subsection{連立一次方程式と行列}

$$
A = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix} \in M_2(\mathbb{R}),
\bm{x} = c\begin{pmatrix}
x \\
y
\end{pmatrix}
$$
とすれば、

$$
A\bm{x} = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
= \begin{pmatrix}
ax + by \\
cx + dy \\
\end{pmatrix}
$$

のように、2次元ベクトル$\bm{x}$に2次行列$A$を差乗することができる。この行列算の応用として$A\bm{x}$を連立一次方程式として表示することを考える。$ax + by$と$cx + dy$の和をそれぞれ$z, w$とすると

\begin{numcases}
  {}
  ax + by = z & \\
  cx + dy = w &
\end{numcases}

のような連立方程式で表すことができる。これより、もし$A$が逆行列を持つなら、

$$
\begin{pmatrix}
x \\
y
\end{pmatrix}
= A^{-1}
\begin{pmatrix}
z \\
w
\end{pmatrix}
$$
と計算することができる。

\subsection{転置行列}

(m, n)行列

$$
A = \begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots \\
a_{m1} & \cdots & a_{mn} \\
\end{pmatrix}
$$

に対し対角線$a_{aa} a_{22} \cdots$に対して、折返してできる(n, m)行列を${}^t\!A$と書き、$A$の転置行列という。

${}^t\!A$を表示すると以下の通りである。

$$
{}^t\!A = \begin{pmatrix}
a_{11} & \cdots & a_{m1} \\
\vdots & \vdots & \vdots \\
a_{1n} & \cdots & a_{mn} \\
\end{pmatrix}
$$

\prop{$A, B \in M_{m, n}(R)$に対して、明らかに${}^t(A + B) = \transposeMat{A} + \transposeMat{B}, {}^t{}^t\!A = A$が成立する。}

\subsubsection{対称と交代}

$A \in M_n(\mathbb{R})$とする。$A = {}^t\!A$ つまり、$a_{ij} = a_{ji} \quad (1 \leq i, j \leq n)$であるとき、$A$を対称または対称行列という。

$A = -{}^t\!A$ つまり、$a_{ij} = -a_{ji} \quad (1 \leq i, j \leq n)$であるとき、$A$を交代(的)または交代行列という。

\subsubsection{内積とテンソル積}\label{subsubsection:innerProduct}

後の計量ベクトル空間の章で内積について詳しく説明するが、ここでは内積の定義とその意味についてだけ軽く触れておく。

二つのn次元ベクトル

$$
\bm{a} = \begin{pmatrix}
a_1 \\
\vdots \\
a_n
\end{pmatrix}, 
\bm{b} = \begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

に対し、


$$
{}^t\!\bm{a}\bm{b} = \sum_{1 \leq i \leq n} a_ib_i = (a_1, \cdots, a_n)\begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

を{\bf 内積}といい$\langle \bm{a}, \bm{b} \rangle$と書く。

また、
$$
\bm{a}{}^t\!\bm{b} = \begin{pmatrix}
a_1b_1 & \cdots & a_1b_n \\
\vdots & \vdots & \vdots \\
a_bb1 & \cdots & a_nb_n \\
\end{pmatrix}
$$

を$\bm{a}, \bm{b}$の{\bf テンソル積}といい、 $\bm{a} \otimes \bm{b}$と書く。

\exercise{$\bm{a}, \bm{b}, \bm{c}, \bm{d}$をn次元ベクトルとするとき、次の等式を証明せよ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item ${}^t(\tensorProduct{a}{b}) = \tensorProduct{b}{a}$
\item $(\tensorProduct{a}{b})\bm{c} = \innerProduct{b}{c}\bm{a}$
\item $\innerProduct{c}{(\tensorProduct{a}{b})\bm{d}} = \innerProduct{a}{c}\innerProduct{b}{d}$
\end{enumerate}

{\bf 解答}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item ${}^t(\tensorProduct{a}{b}) = {}^t(\bm{a}{}^t\bm{b}) = {}^t{}^t\bm{b}{}^t\bm{a} = \bm{b}{}^t\bm{a} = \tensorProduct{b}{a}$
\item $(\tensorProduct{a}{b})\bm{c} = (\bm{a}{}^t\bm{b})\bm{c} = \innerProduct{b}{c}\bm{a}$
\item $\innerProduct{c}{(\tensorProduct{a}{b})\bm{d}} = {}^t\bm{c}(\bm{a}{}^t\bm{b})\bm{d} = \innerProduct{c}{a}\innerProduct{b}{d} = \innerProduct{a}{c}\innerProduct{b}{d}$
\end{enumerate}


\section{ベクトル空間(線形空間)}

n次元ベクトル全体の集合を$\vecSpace{n}$とかく。2章の時点ですでに2次元のベクトルやベクトルと行列間に演算を導入していたが、一般的な$\vecSpace{n}$に対して、演算を改めて定義する。

\defi{
$$
\bm{x} = \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix},
\bm{y} = \begin{pmatrix}
y_1 \\
\vdots \\
y_n
\end{pmatrix} \in \vecSpace{n}, a \in \mathbb{R}
$$
とするとき、これらの加法(減法)、スカラー倍はそれぞれ次のように定義される。
}

$$
\bm{x} + \bm{y} = \begin{pmatrix}
x_1 + y_1 \\
\vdots \\
x_n + y_n
\end{pmatrix},
\bm{x} - \bm{y} = \bm{x} + -(\bm{y}) \begin{pmatrix}
x_1 + -(y_1) \\
\vdots \\
x_n + -(y_n)
\end{pmatrix},
a\bm{x} = \begin{pmatrix}
ax_1 \\
\vdots \\
ax_n
\end{pmatrix}
$$

$\vecSpace{n}$が次の条件を満たしたとき、単なる集合ではなく、{\bf n次元ベクトル空間}となる。単にベクトル空間ともいう。

\defi{$V = \vecSpace{n}$とするとき、$V$は以下の条件のもと、ベクトル空間となる}\label{defi:VectorSpace}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item 任意の$\bm{x}, \bm{y}, \bm{z} \in V$に対して、$(\bm{x}+\bm{y}) + \bm{z} = \bm{x}+ (\bm{y} + \bm{z}) $ \quad (結合法則)
\item 任意の$\bm{x}, \bm{y}\in V$に対して、$\bm{x} + \bm{y} = \bm{y} + \bm{x}$ \quad (交換法則)
\item 任意の$\bm{x} \in V$に対して、$\bm{0} + \bm{x} = \bm{x} + \bm{0} = \bm{x}$ \quad $(零ベクトルの存在)$
\item 任意の$\bm{x} \in V$に対して、逆ベクトル$-\bm{x}$が存在して、$\bm{x} + (-\bm{x}) = (-\bm{x}) + \bm{x} = \bm{0}$ \quad (逆ベクトルの存在)
\item{
    任意の$\bm{x}, \bm{y} \in V, a, b \in \mathbb{R}$に対して
    \begin{enumerate}
    \item $a(\bm{x}+\bm{y}) = a\bm{x}+a\bm{y}$
    \item $(a+b)\bm{x} = a\bm{x} + b\bm{x}$
    \item $a(b\bm{x}) = ab(\bm{x})$
    \end{enumerate}
}
\item 任意の$\bm{x} \in V$に対して$1 \cdot \bm{x} = \bm{x} \quad (1 \in \mathbb{R})$
\end{enumerate}

\prop{これらをベクトル空間の公理と認めると、以下が成り立つ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{0}\bm{x} = \bm{0}$
\item $(-1)\bm{x} = -\bm{x}$
\end{enumerate}

\begin{Proof*}
\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item (5)より、$\bm{0}\bm{x} = (\bm{0} + \bm{0})\bm{x} = \bm{0}\bm{x} + \bm{0}\bm{x}$となり、両辺から$\bm{0}\bm{x}$を引くと、$\bm{0} = \bm{0}\bm{x}$
\item $\bm{0} = \bm{0}\bm{x} = (1 + -(1))\bm{x} = 1 \cdot \bm{x} + (-1) \cdot \bm{x} = \bm{x} + (-1)\bm{x}$となり、両辺から$-\bm{x}$を引くと、$-\bm{x} = (-1)\bm{x}$
\end{enumerate}

\subsubsection{単位ベクトル}\label{subsection:unitVector}

特殊なベクトルとして$\bm{e}_1, \cdots \bm{e}_n \in V$があり、それぞれが

$$
\bm{e}_1 = \begin{pmatrix}
1 \\
0 \\
\vdots \\
0
\end{pmatrix},
\bm{e}_2 = \begin{pmatrix}
0 \\
1 \\
\vdots \\
0
\end{pmatrix}, 
\cdots, 
\bm{e}_n = \begin{pmatrix}
0 \\
0 \\
\vdots \\
1
\end{pmatrix}
$$

のように大きさが1になるようなベクトルを、{\bf 単位ベクトル}という。この単位ベクトル$\bm{e}_1, \cdots \bm{e}_n$を使えば、任意の$V$に対して

$$
\bm{x} = \sum_{1 \leq i \leq n} x_ie_i
$$

と一意に表現できる。
\end{Proof*}

\subsection{部分空間}

\defi{$V$をn次元ベクトル空間とする。$V \supset W \neq \phi$とする。$W$が以下の条件を満たすとき、$W$を$V$の部分空間という}\label{defi:subspace}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{x}, \bm{y} \in W \Rightarrow \bm{x} + \bm{y} \in W$
\item $\bm{x} \in W, a \in \mathbb{R} \Rightarrow a\bm{x} \in W$
\end{enumerate}

これらの条件は、Wの中でベクトルとしての演算が閉じていることを示している。

\lemm{$W$が$V$の部分空間であれば、$W$もベクトル空間である}

\begin{Proof*}
仮定により$\bm{x} \in W$であるから、定義\ref{defi:subspace}(2)を使うと、$0 \in \mathbb{R}, 0 \cdot \bm{x} = \bm{0} \in W$と$-1 \in \mathbb{R}, (-1)\bm{x} = -\bm{x} \in W$である。したがって、任意の$\bm{x}, \bm{y}$に対して、$\bm{x} - \bm{y} = \bm{x} + (-\bm{y}) \in W$である。ベクトルの演算の公理が$V$で成立しているので、$W$においても成立することは明らか。また零ベクトルと任意のベクトル$\bm{x}の$逆ベクトルも$W$に存在することは分かっているので、$W$もまたベクトル空間である。

つまり、部分空間はベクトル空間として扱うことができるということである。
\end{Proof*}

\prop{$\{\bm{0}\}$、$V(=\vecSpace{n})$は部分空間である}

\begin{Proof*}
$\bm{0} + \bm{0} = \bm{0}, \quad a\bm{0} = \bm{0} \quad (\forall a \in \mathbb{R}, \bm{0} \in \{\bm{0}\})$より、$\{\bm{0}\}$は部分空間である。また、$V$はベクトル空間なので、部分空間の条件を満たすことは明らか。これらの部分空間を{\bf 自明な部分空間}と呼ぶ。
\end{Proof*}

\exercise{$V = \vecSpace{3}$において、以下は部分空間か判定せよ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $$
W_1 = \left\{ 
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} \mid x_1 + x_2 + x_3 = 0
\right\}
$$
\item $$
W_2 = \left\{ 
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} \mid 2x_1 + x_2 = 5
\right\}
$$
\end{enumerate}

{\bf 解答}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item 部分空間である。実際、
$$
\bm{x} = \begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix},
\bm{y} = \begin{pmatrix}
y_1 \\
y_2 \\
y_3
\end{pmatrix} \in W_1
$$

をとってきて、$\bm{x} + \bm{y}$を計算すると、$(x_1 + x_2 + x_3) = 0, (y_1 + y_2 + y_3) = 0$より、$(\bm{x}_1 + \bm{y}_1) + (\bm{x}_2 + \bm{y}_2) + (\bm{x}_3 + \bm{y}_3) = \bm{0} \in W_1$である。同様に、$\forall a \in \mathbb{R}$を取ってきて、$a\bm{x} = \bm{0} \in W_1$である。

\item 部分空間ではない。凡例をあげる。

$$
\bm{x} = \begin{pmatrix}
1 \\
3 \\
0
\end{pmatrix},
\bm{y} = \begin{pmatrix}
\dfrac{1}{2} \\[1.5ex]
4 \\
0
\end{pmatrix} \in W_2
$$をとってくる。両方とも上の条件を満たすが、

$$
\bm{x} + \bm{y} = \begin{pmatrix}
\dfrac{3}{2} \\[1.5ex]
7 \\
0
\end{pmatrix}
$$

$2 \cdot \dfrac{3}{2} + 7 \neq 5$であるため、$\bm{x} + \bm{y} \notin W_2$となり、部分空間の必要十分条件に反する。
\end{enumerate}

\defi{
$V = \vecSpace{n}$とする。任意の$\bm{x} \in V$に対し、そのスカラー倍全体の集合$\{a\bm{x}\mid a \in \mathbb{R}\}$は明らかに$V$の部分空間になる。さらに一般的に、$\bm{x_1}, \cdots, \bm{x_r} \in V$が与えられたとき、

$$
\bm{x} = \sum_{1 \leq i \leq r} a_{i}\bm{x}_i \quad (a_1, \cdots, a_r \in \mathbb{R})
$$

のような形で$\bm{x}$が表されるとき、これを$\bm{x_1}, \cdots, \bm{x_r}$の{\bf 線形結合}(linear combination)または{\bf 1次結合}という。
}\label{defi:linearCombination}

$\bm{x_1}, \cdots, \bm{x_r}$の線形結合全体の集合を$W = \{a_1\bm{x_1} + \cdots + a_r\bm{x_r} \mid a_1, \cdots, a_2 \in \mathbb{R} \}$とすると、$W$は$V$の一組の部分空間になる。これを$\bm{x_1}, \cdots, \bm{x}_r$によって張られる部分空間といい、$\langle \bm{x_1}, \cdots, \bm{x_r} \rangle$のように書く。（内積と混同しないように注意したい。）

\begin{Proof*}
$$
\bm{0} = \sum_{1 \leq i \leq r} 0\bm{x}_i \in W
$$

より$W \neq \phi$である。$\bm{x}, \bm{y} \in W$とすれば、

$$
\bm{x} = \sum_{1 \leq i \leq r} a_i\bm{x}_i, \quad \bm{y} = \sum_{1 \leq i \leq r} b_i\bm{x}_i \quad (a_i, b_i \in \mathbb{R})
$$

と書くことができる。これより、

$$
\bm{x} + \bm{y} = \sum_{1 \leq i \leq r} (a_i + b_i)\bm{x}_i
$$

とそれぞれのベクトルを$\bm{x}_i$の線形結合で表すことができる。

また、$c \in \mathbb{R}$に対し、

$$
c\bm{x} = c \left( \sum_{1 \leq i \leq r} a_i\bm{x}_i \right) = \sum_{1 \leq i \leq r} (ca_i)\bm{x}_i
$$とこれも$\bm{x}_i$の線形結合で表すことができる。

したがって、$\bm{x} + \bm{y}, \quad c\bm{x} \in W$となることから、Wは部分空間である。
\end{Proof*}

\subsubsection{部分空間の共通部分}

\prop{$W_1, W_2$を$V$の二つの部分空間とするとき、その共通部分$W_1 \cap W_2$は部分空間になる。}

\begin{Proof*}
$\bm{0} \in W_1 \cap W_2$であるから、$W_1 \cap W_2 \neq \phi$である。$\bm{x}, \bm{y} \in W_1 \cap W_2$とすれば、$\bm{x} + \bm{y} \in W_1$であり、$\bm{x} + \bm{y} \in W_2$である。（$\bm{x}, \bm{y} \in W_1, \bm{x}, \bm{y} \in W_2$だから。）したがって、$\bm{x} + \bm{y} \in W_1 \cap W_2$。同様に、$a \in \mathbb{R}$に対して、$a\bm{x} \in W_1 \cap W_2$である。
\end{Proof*}

\subsection{部分空間の和}

部分空間の共通部分を考えたときに、今度は和集合（ベクトル空間どうしの和）も部分空間になるかを考えたくなるものである。$W_1, W_2$を$V = \vecSpace{3}$の部分空間とし、$W_1$と$W_2$の和集合$W_1 \cup W_2$を考えてみる。

$W_1 = \langle \bm{e}_1 \rangle, \quad W_2 = \langle \bm{e}_2 \rangle$とおくと、
$$
\begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}, 
\begin{pmatrix}
0 \\
1 \\
0
\end{pmatrix}
\in W_1 \cup W_2
$$

であるが、すべての$a, b \in \mathbb{R}$に対して、

$$
a\bm{e}_1 + b\bm{e}_2 = 
\begin{pmatrix}
a \\
b \\
0
\end{pmatrix}
\notin W_1 \cup W_2
$$

となる。（厳密に言うと、和集合の構成次第で含まれているところもあるかもしれないが、含まれていないところもある。）

そこで、$a\bm{e}_1 + b\bm{e}_2$をすべて含むようにすることを考えるために、$\langle \bm{e}_1, \bm{e}_2 \rangle$で張られる空間を考えると良さそうである。

\defi{一般に二つの部分空間$W_1, W_2$の和を以下のように定義する。}\label{defi:VecSpaceUnion}

$$
W_1 + W_2 = \{ \bm{x} + \bm{y} \mid \bm{x} \in W_1, \bm{y} \in W_2 \}
$$

この演算で定義される部分空間は$V$の最小の部分空間となる。それを次の定理で示す。

\rem{$V = \vecSpace{n}$とし、$W_1, W_2$を$V$の部分空間とする。$W_1 + W_2$は$W_1$と$W_2$を含む最小の部分空間となる。}

\begin{Proof*}
まず、$W_1 + W_2$が部分空間であることを示す。

$\bm{0} =\bm{0} + \bm{0} \in W_1 + W_2$より、$W_1 + W_2 \neq \phi$。$\bm{x}, \bm{y} \in W_1 + W_2$とすれば、定義\ref{defi:VecSpaceUnion}により、$\bm{x}',\bm{y}' \in W_1, \bm{x}'', \bm{y}'' \in W_2$と書くことができる。（定義\ref{defi:linearCombination}で示した形式を思い出すと良い。）よって、$\bm{x} + \bm{y} = (\bm{x}' + \bm{x}'') + (\bm{y}' + \bm{y}'') = (\bm{x}' + \bm{y}') + (\bm{x}'' + \bm{y}'')$であり、$a \in \mathbb{R}$に対して、$\bm{x}' \in W_1$より$a\bm{x}' \in W_1$また、$\bm{x}'' \in W_2$より$a\bm{x}'' \in W_2$である。

これらより、$\bm{x}' + \bm{y}' \in W_1, \bm{x}'' + \bm{y}'' \in W_2$であるから、$\bm{x} + \bm{y} \in W_1 \cup W_2$であり、$a\bm{x} \in W_1 \cup W_2$である。($a\bm{x} \in W_1$であるから、和集合である$W_1 \cup W_2$に属するのは当然) よって、$W_1 + W_2$は$V$の部分空間である。

次に、$W_1 + W_2$が最小の部分空間であることを示す。

$\bm{x} \in W_1$とすれば、$\bm{x} = \bm{x} + \bm{0} \quad (\bm{x} \in W_1, \bm{0} \in W_2)$より、$\bm{x} \in W_1 + W_2$である。よって、$W_1 \subset W_1 + W_2$。同様に$W_2 \subset W_1 + W_2$が言え、$W_1 + W_2$は$W_1$と$W_2$を含むことが分かる。ここで、$W$を$W1, W_2$を含む部分空間とすれば、任意の$\bm{x} \in W_1, \bm{y} \in W_2$に対し、$\bm{x}, \bm{y} \in W$であるから、$\bm{x} + \bm{y} \in W$。よって、$W_1 + W_2 \subset W$となり、$W_1 + W_2$は$W_1$と$W_2$を含む最小の部分空間である。
\end{Proof*}

\exercise{
一般に二つの部分空間$W_1, W_2$に対し、$W_1 \subset W_2 \Longleftrightarrow W_1 + W_2 = W_2$であることを示せ。
}

$W_1 \subset W_2$とすれば、$W_1 + W_2 \subset W_2 + W_2 = W_2$。$W_1 + W_2 \supset W_2$は自明であるから、$W_1 + W_2 = W_2$。逆に、$W_1 + W_2 = W_2$とすると、$W_1 \subset W_1 + W_2$であるから、$W_1 \subset W_2$である。

\exercise{
$W_1, W_2, W_3$を$V = \vecSpace{n}$の部分空間とするとき、$(W_1 \cap W_3) + (W_2 \cap W_3) \subset (W_1 + W_2) \cap W_3$を示せ。
}

$\bm{x} \in (W_1 \cap W_3) + (W_2 \cap W_3)$とする。このとき、$\bm{x} = \bm{x}' + \bm{x}''$と分解でき、$\bm{x}' \in W_1 \cap W_3 \hspace{3pt} \& \hspace{3pt} \bm{x}'' \in W_2 \cap W_3$. $\bm{x}', \bm{x}''$が両方とも$W_3$の元より、$\bm{x} \in W_3$また、$W_1, W_2$の定義より、$\bm{x} \in W_1 + W_2$。したがって、$\bm{x} \in (W_1 + W_2) \cap W_3$.

\subsection{線形独立と線形従属}

\defi{r個のベクトル$\bm{x}_1, \cdots \bm{x}_r$は、次の条件が満たされるとき{\bf 線形独立}(linearly independent)または{\bf 一次独立}であるという。}\label{defi:linearlyIndependent}

$$
\sum_{1 \leq i \leq r} a_i\bm{x}_i = 0 \quad (a_i \in \mathbb{R}) \Longrightarrow a_1 = \cdots = a_i = 0
$$

一方、線形独立ではない$\bm{x}, \cdots \bm{x}_r$を{\bf 線形従属}(linearly dependent)または{\bf 1次従属}であるという。$\bm{x}, \cdots \bm{x}_r$が線形従属であるとき以下の1次関係式が成立する。

$$
\sum_{1 \leq i \leq r} a_i\bm{x}_i = 0, \quad a_i \in \mathbb{R} \hspace{3pt} \& \hspace{3pt} \exists a_i \neq 0
$$

\subsubsection{線形独立と線形従属の定義の成り立ち}

線形独立と線形従属の定義は初見では理解が難しいので、この定義の成り立ちを考えてみることにする。

$V = \vecSpace{3}$において3つの基本ベクトル

$$
\bm{e}_1 = \begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}, 
\bm{e}_2 = \begin{pmatrix}
0 \\
1 \\
0
\end{pmatrix}, 
\bm{e}_3 = \begin{pmatrix}
0 \\
0 \\
1
\end{pmatrix}
$$

はそれぞれ独立した方向を向いていると考えられる。つまり、$\langle \bm{e}_1, \bm{e}_2 \rangle$を$xy$平面とすると、$\bm{e}_3$はそれに属さない、つまり$z$軸と考えることができる。これは$\bm{e}_3$は$\bm{e_1}, \bm{e_2}$の線形結合で表されないということである。同様に$\bm{e}_1, \bm{e}_2$のいずれかと$\bm{e}_3$が張る部分空間を考えたときに、残りの単位ベクトルはその部分空間に属さない。つまりこれは

\begin{equation}
    a\bm{e}_1 + b\bm{e}_2 + c\bm{e}_3 = 0 \quad (a, b, c, \in \mathbb{R}) 
\end{equation}

ならば$a = b = c = 0$と言い換えることができる。これは定義\ref{defi:linearlyIndependent}そのものである。なぜ$a = b = c = 0$であれば線形独立なのかを考えたいのであれば、逆に$c \neq 0$とすれば式(2)から

$$
\bm{e}_3 = \left(-\frac{a}{c}\right)\bm{e}_1 + \left(-\frac{b}{c}\right)\bm{e}_2
$$

のように$\bm{e}_3$を$\bm{e}_1$と$\bm{e}_2$の線形結合で表すことが出来てしまう。つまり、$\langle \bm{e}_1, \bm{e}_2 \rangle$に$\bm{e}_3$が含まれていることを意味する。$a \neq 0, b \neq 0$のときも同様である。

\prop{$\bm{x}_1, \bm{x}_2, \cdots \bm{x}_r$が線形独立ならばその1部分も線形独立である。}
線形独立の定義よりこれは明らかなので証明は割愛する。

\exercise{
$V = \vecSpace{3}$において

$$
\bm{x} = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}, 
\bm{z} = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}
$$

が線形独立であることを示せ。
}

$a\bm{x} + b\bm{y} + c\bm{z} = 0 \quad (a, b, c \in \mathbb{R})$であれば、$a = b = c = 0$を示す。

\begin{numcases}
  {}
  a + (-b) = 0 & \\
  a + (-c) = 0 & \\
  a + b + c = 0 &
\end{numcases}

という連立1自方程式を解けば、$a = b = c = 0$が唯一の解となるので、$\bm{x}, \bm{y}, \bm{z}$は線形独立である。

\exercise{
$V = \vecSpace{3}$において

$$
\bm{x} = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
0 \\
1 \\
-1
\end{pmatrix}, 
\bm{z} = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}
$$
は線形独立であるか調べよ。
}

$a\bm{x} + b\bm{y} + c\bm{z} = 0 \quad (a, b, c \in \mathbb{R})$とすると、$a, b, c$は

\begin{numcases}
  {}
  a + c = 0 \\
 -a + b = 0 \\
 -b + c = 0 
\end{numcases}

を満たす。このときこの連立一次方程式は$a = 1, b = 1, c = -1$の解を持つため、$\bm{x}, \bm{y}, \bm{z}$は線形従属である。

\lemm{$\bm{x}_1, \bm{x}_2, \cdots \bm{x}_r \in V$に関して次の2つの条件は同値である。}\label{lemm:linearlyIndependent}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{x}_1, \cdots \bm{x}_r$は線形独立である
\item $\bm{x}_1, \cdots \bm{x}_{r-1}$は線形独立で、$\bm{x}_r \notin \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$
\end{enumerate}

\begin{Proof*}
(1) $\Rightarrow$ (2)を示す。(1)を仮定すると、$a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} + a_r\bm{x}_r = 0$ならば、$a_i = 0 \quad (1 \leq i \leq r)$である。(2)が成り立たないとすれば、$\bm{x}_r \in \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$であるから$\bm{x}_r = a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} = 0$と表せる。よって、$a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} + (-1)\bm{x}_r = 0$が成立し$a_r \neq 0$であるから(1)に反する。よって、$\bm{x}_r \notin \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$

(2) $\Rightarrow$ (1)を示す。$\bm{x}_1, \cdots \bm{x}_r$が線形独立でないとすると、ある$a_i \neq 0 \quad (1 \leq i \leq r)$が存在し、$a_1\bm{x}_1 + \cdots + a_{r}\bm{x}_{r} = 0$が成立する。もし$a_r = 0$とすると$a_1, \cdots, a_{r-1}$のいずれかが$0$ではなくなる。これは、(2)の$\bm{x}_1, \cdots \bm{x}_{r-1}$が線形独立であるという仮定に反する。よって、$a_r \neq 0$でなければならない。しかしこのとき、

$$
 \left(-\frac{a_1}{a_r}\right)\bm{x}_1 + \cdots +  \left(-\frac{a_{r-1}}{a_r}\right)\bm{x}_{r-1} = \bm{x}_r \in \langle \bm{x}_1 + \cdots + \bm{x}_{r-1} \rangle
$$
となり(2)の仮定に反する。$\bm{x}_1, \cdots, \bm{x}_{r}$は線形独立である。
\end{Proof*}

\rem{
$\bm{x}_1, \cdots \bm{x}_r$が線形独立であるための必要十分条件は$\bm{x}_1 \neq 0$かつ、任意の$i$に対して$\bm{x}_i \notin \langle \bm{x}_1, \cdots \bm{x}_{i-1} \rangle \quad (2 \leq i \leq r)$である。
}\label{rem:independent}

\begin{Proof*}
($\Rightarrow$): 補題\ref{lemm:linearlyIndependent}により、$\bm{x}_1, \cdots, \bm{x}_r$が線形独立であれば、$\bm{x}_1, \cdots \bm{x}_i$も線形独立であるから、この主張が成立する。

($\Leftarrow$): この主張が成立すると仮定する。$\bm{x}_1, \cdots, \bm{x}_i$が線形独立であることは、補題\ref{lemm:linearlyIndependent}を再帰的に行うことで判断できる。まず$\bm{x}_1 \neq 0$から$\bm{x}_1$は線形独立である。次に、$\langle \bm{x}_1 \rangle$に$\bm{x}_2$が含まれていないか、さらにその次に$\langle \bm{x}_1, \bm{x}_2 \rangle$に$\bm{x}_3$が含まていないかといった判定を$\bm{x}_{i-1}$まで再帰的に行っていく。このとき、$\bm{x}_1, \cdots \bm{x}_{i-1}$が線形独立であるとすれば、補題\ref{lemm:linearlyIndependent}により、$\bm{x}_1, \cdots \bm{x}_i$も線形独立である。この判定方法は$i = r$まで成立する。(この操作は高々r回で終わる。)
\end{Proof*}

\subsection{部分空間の基底}

基底の定義をする前に、以下の補題にてベクトル空間を張る線形独立なベクトルは別の線形独立なベクトルに置き換えることができることを示す。

\lemm{$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$とする。$\bm{y}_1, \cdots, \bm{y}_s \in W$が線形独立であるとすれば、$s \leq r$である。}\label{lemm:basisReplacing}

\begin{Proof*}
$\{i_1, \cdots, i_{s}\} \subset \{1, \cdots, t\}$をとなる$\{i_1, \cdots, i_{s}\}$とってくる。($\{i_1, \cdots, i_s\}$は、この時点では順不同である。) Wを張る$\bm{x}_1, \cdots, \bm{x}_t$のうち、$\bm{x}_{i_1}, \cdots, \bm{x}_{i_s}$を$\bm{y}_1, \cdots, \bm{y}_s$に置き換える、つまり各$\bm{y}_i$は

$$
\bm{y}_i = \sum_{1 \leq j \leq s} a_j\bm{x}_{i_j}
$$

とかけることを帰納的に示す。まず、$s = 0$のときは自明である。$s \geq 1$として、$\bm{y}_1, \cdots, \bm{y}_{s-1}$に対してこの主張が言えたとする。つまり$\{i_1, \cdots, i_{s-1}\} \subset \{1, \cdots, t\}$をとってきて、$\bm{x}_1, \cdots, \bm{x}_t$の中の$\bm{x}_{i_1}, \cdots, \bm{x}_{i_{s-1}}$が$\bm{y}_1, \cdots, \bm{y}_{s-1}$に替えられたとする。これを、

\begin{eqnarray*}
  x_i' = \left\{
    \begin{array}{l}
      \bm{y}_k \quad i = i_k (1 \leq k \leq s -1) \\
      \bm{x}_i \quad otherwise
    \end{array}
  \right.
\end{eqnarray*}

とおく。このとき、$W = \langle \bm{x}_1', \cdots, \bm{x}_t' \rangle, \bm{y}_s \in W$であるから、$\bm{y}_s = a_1\bm{x}_1' + \cdots + a_t\bm{x}_t'$と表される。しかし、補題\ref{lemm:linearlyIndependent}により$\bm{y}_s \notin \langle \bm{y}_1, \cdots, \bm{y}_{s-1} \rangle = \langle \bm{x}_{i_1}', \cdots, \bm{x}_{i_{s-1}}' \rangle$であるから、ある$i \notin \{i_1, \cdots, i_{s-1} \}$に対して$a_i = 0$である。そのような$i$の1つを$i_s$とする。そのとき、$\bm{x}_{i_s}' = \bm{x}_{i_s}$は、$\bm{x}_1', \cdots, \bm{x}_{i_{s-1}}', \bm{y}_s, \bm{x}_{i_{s + 1}}', \cdots, \bm{x}_t'$の線形結合で表される。よって、$x_{i_s}'$を$\bm{y}_s$で置き換えることができる。
\end{Proof*}

この補題により$1$から$r = s$まで順番にこの置き換えを行うことで、次の定理が得られる。

\rem{$W$を$V = \vecSpace{n}$の部分空間とする。$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle = \langle \bm{y}_1, \cdots, \bm{y}_s \rangle$}で、$\bm{x}_1, \cdots, \bm{x}_r$と$\bm{y}_1, \cdots, \bm{y}_s$がともに線形独立であるとすれば、$r = s$である。

\defi{$W$を$V = \vecSpace{n}$の部分空間とする。$W$に対し、次の条件を満たす順序付けられたベクトルの集合をWの{\bf 基底}(basis)という。}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$
\item $\bm{x}_1, \cdots, \bm{x}_r$は線形独立である。
\end{enumerate}

\rem{$V = \vecSpace{n}$の任意の部分空間$W$に対して基底${x_1, \cdots, x_r}$が存在する。基底をなすベクトルの個数は一定で、$r \leq n$となる}

\begin{Proof*}
$W \subset V = \langle \bm{e}_1, \cdots, \bm{e}_n \rangle$であるから、補題\ref{lemm:basisReplacing}により$W$に含まれる線形独立なベクトルの個数は高々nである。$\bm{x}_1, \cdots, \bm{x}_r$をWに含まれる最大個数の線形独立なベクトルの集合とする。そのとき、$r \leq n$で任意の$\bm{x} \in W$に対し、$\bm{x} \in \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$が成立する。よって、$W \subset \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$である。$\langle \bm{x}_1, \cdots, \bm{x}_r \rangle \subset W$は明らかであるから、$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$。よって、$\{\bm{x}_1, \cdots, \bm{x}_r \}$は$W$の基底である。補題\ref{lemm:basisReplacing}により、rは一意的に定まる。
\end{Proof*}

この定理により部分空間Wに対して定まる($r \in \mathbb{R}$)をWの{\bf 次元}(dimension)といい、$\dim W$と書く。

\rem{
$\{\bm{x}_1, \cdots, \bm{x}_n\}$を線形独立なベクトルの組とする。このとき、$\{\bm{x}_1, \cdots, \bm{x}_n\}$の線形結合としての表し方は一意的である。すなわち、$\bm{x} = a_1\bm{x}_1 + \cdots + a_n\bm{x}_n \quad (a_i \in \mathbb{R})$において、$a_i$は$\bm{x}$に対して一意的に定まる。
}\label{rem:VectorExpressionUniquness}

\begin{Proof*}
$\bm{x}$の線形結合を$a_i$と$a_i'$を使って以下のように表せたとする。
\begin{eqnarray*}
\bm{x} = a_1\bm{x}_1 + \cdots + a_n\bm{x}_n \\
\bm{x} = a_1'\bm{x}_1 + \cdots + a_n'\bm{x}_n
\end{eqnarray*}

これらについて辺々引くと、

$$
0 = (a_1 - a_1')\bm{x}_1 + \cdots + (a_n - a_n')\bm{x}_n
$$

である。$\{\bm{x}_1, \cdots, \bm{x}_n\}$は線形独立なベクトルの組であるから、$a_1 - a_1' = \cdots = a_n - a_n' = 0$より、$a_1 = a_1', \cdots, a_n = a_n'$である。
\end{Proof*}

\exam{
基本ベクトルの集合, $\langle \bm{e}_1, \cdots, \bm{e}_n \rangle$は明らかに$V = \vecSpace{n}$の基底であるから、$\dim V = n$である。これを$\vecSpace{n}$の{\bf 標準基底}という。
}

\exam{
$\{0\}$は線形独立なベクトルを含まないから、$\dim \{0\} = 0$である
}

\exam{  
$V = \vecSpace{n}$とする。$W, W' \subset V$のとき、$W \subset W', \dim W \leq \dim W'$である。
}

\rem{
$V = \vecSpace{n}$とする。$W, W' \subset V$とする。$W \subset W'$のとき、$\dim W = \dim W' \Longrightarrow W = W'$である。

\begin{Proof*}
$r \leq n$とする。$W = \linearCombination{\bm{x}_1}{\bm{x}_n}, W' = \linearCombination{\bm{y}_1}{\bm{y}_r}$とする。$W \subset W', \dim W = \dim W'$であれば、$W$の基底$\vecSet{x}$と$W'$の基底$\{\bm{y}_1, \cdots, \bm{y}_r\}$の個数は一致するから、補題\ref{lemm:basisReplacing}により、

$$
\bm{y}_i = \sum_{1 \leq j \leq n} a_j\bm{x}_{j} \quad (a \in \mathbb{R})
$$

と表すことができる。これより、$W = \linearCombination{\bm{x}_1}{\bm{x}_n} = W' = \linearCombination{\bm{y}_1}{\bm{y}_r}$とすることができる。
\end{Proof*}
}\label{rem:dimMatching}

\exercise{
$V = \vecSpace{3}$において、$V$の部分空間
$$
W = \left\{
\begin{pmatrix}
x \\
y \\
z
\end{pmatrix} \mid x + 2y + z = 0
\right\}
$$
の一組の基底を求めよ。
}

$\newline$
$x + 2y + z = 0$を$x$について解くと、$x = -2y - z$であるから、Wの元は

$$
\begin{pmatrix}
-2y -z \\
y \\
z
\end{pmatrix} = 
y \begin{pmatrix}
-2 \\
1 \\
0
\end{pmatrix} + 
z \begin{pmatrix}
-1 \\
0 \\
1
\end{pmatrix} 
$$

と表すことができる。
ここで、

$$
\begin{pmatrix}
-2 \\
1 \\
0
\end{pmatrix}, \begin{pmatrix}
-1 \\
0 \\
1
\end{pmatrix} 
$$

は線形独立であり、$\vecSpace{2}$を張ることから、基底である。

\subsection{基底の変換}

$V = \vecSpace{n}$の基底として、$\{\bm{a}_1, \cdots, \bm{a}_2\}$と$\{\bm{b}_1, \cdots, \bm{b}_n\}$があったとする。これら2組の基底が、

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} =
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}P
$$

というような関係式であれば、行列$P = (p_{ij}) \in M_n(\mathbb{R})$を$\{\bm{a}_1, \cdots, \bm{a}_2\}$から$\{\bm{b}_1, \cdots, \bm{b}_n\}$への{\bf 基底の変換行列}という。基底の変換行列は次のように求められる。

まず、それぞれの基底は$V$に属しているので、$\{\bm{a}_1, \cdots, \bm{a}_2\}$の各ベクトルは、$\{\bm{a}_1, \cdots, \bm{a}_2\}$の線形結合で表すことができる。

$$
\bm{b}_j = \sum_{1 \leq i \leq n} p_{ij}\bm{a}_j \quad (1 \leq j \leq n)
$$

である。これを書き直すと、

\begin{eqnarray*}
&\bm{b}_1 = P_{11}\bm{a}_1 + \cdots + P_{n1}\bm{a}_n = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{11} \\
\vdots \\
P_{n1}
\end{pmatrix} \\
&\vdots \\
&\bm{b}_n = P_{1n}\bm{a}_1 + \cdots + P_{nn}\bm{a}_n = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{1n} \\
\vdots \\
P_{nn}
\end{pmatrix}
\end{eqnarray*}

となり、これらをまとめると

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{11} & \cdots & P_{1n} \\
\vdots & \vdots & \vdots \\
P_{n1} & \cdots & P_{nn} \\
\end{pmatrix}
$$

だから、

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}P
$$

が得られた。この関係式から

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix}P^{-1} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
$$

も容易に導ける。

\exercise{$\vecSpace{3}$の２つの底を

\begin{eqnarray}
\left\{ 
\bm{a}_1 = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix},
\bm{a}_2 = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix},
\bm{a}_3 = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix} 
\right\} \\
\left\{
\bm{b}_1 = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix},
\bm{b}_2 = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix},
\bm{b}_3 = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix} 
\right\}
\end{eqnarray}

とするとき、(10)から(11)への基底の変換行列を求めよ。
\newline
}\label{exercise:basisTransformation1}

基底の変換行列を$P$とすると、$\bm{b}_i$は$\bm{a}_i$によって以下のように表される。

\begin{eqnarray*}
\bm{b}_1 = P_{11}\bm{a}_1 + P_{21}\bm{a}_2 + P_{31}\bm{a}_3 \\
\bm{b}_2 = P_{12}\bm{a}_1 + P_{22}\bm{a}_2 + P_{32}\bm{a}_3 \\
\bm{b}_2 = P_{13}\bm{a}_1 + P_{23}\bm{a}_2 + P_{33}\bm{a}_3
\end{eqnarray*}

これらをそれぞれ連立一次方程式の形式で表し、それらを解くと

$$
P = \begin{pmatrix}
-\frac{2}{3} & \frac{4}{3} & -\frac{2}{3} \\[1.5ex]
-\frac{2}{3} & -\frac{2}{3} & \frac{4}{3} \\[1.5ex]
\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
\end{pmatrix} 
$$

が得られる。

\section{線形写像}

線形写像とはベクトル空間からベクトル空間への（準同型）写像である。

\subsection{線形写像の定義}

\defi{
$V = \vecSpace{n}, V' = \vecSpace{m}$とする。$V$から$V$'への写像を$f$とし、以下の2つの性質を満たす時$f$は{\bf 線形}(一次)であるという。
}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $f(\bm{x} + \bm{y}) = f(\bm{x}) + f(\bm{y}) \quad (\bm{x}, \bm{y} \in V)$
\item $f(c\bm{x}) = cf(\bm{x}) \quad (\bm{x} \in V, c \in \mathbb{R})$
\end{enumerate}

これらの性質は$f$がベクトル空間上の加法とスカラー倍の2つの演算を保存することを意味する。
（つまり線形性とは、$f$が加法とスカラー倍において準同型であるということである。準同型は群の概念であるため、詳しい話は割愛する。詳しく知りたい方は、群のレポート: https://github.com/noppoMan/math-report/blob/master/group/group.pdfを読まれることを推奨する。)

特に、$V$から$V$自身への線形写像を$V$の{\bf 線形変換}(linear transformation)または${\bf 一次変換}$という

\exam{
$V = \vecSpace{3}, V' = \vecSpace{2}, V'' = \mathbb{R}$とするとき写像

$$
\begin{array}{ccc}
V & \stackrel{f_1}{\longrightarrow}  & V' \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} & \longmapsto & \begin{pmatrix}
x_1 - x_2 \\
x_1 - x_3
\end{pmatrix}
\end{array}
$$

$\newline$

$$
\begin{array}{ccc}
V & \stackrel{f_2}{\longrightarrow}  & V'' \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} & \longmapsto & (x_1 + x_2 + x_3)
\end{array}
$$

$\newline$

などは線形である。$f_2$のように$\mathbb{R}$への線形写像を{\bf 線形汎関数}ともいう。
}

\exam{
$\vecSpace{2}$（平面ベクトル）の原点を中心とした時計、反時計周りに$\theta$だけ回転させるような写像は線形変換である。($\vecSpace{2}$から$\vecSpace{2}$への線形写像)

$$
f_3: \begin{pmatrix}
x \\
y
\end{pmatrix} 
\longmapsto
\begin{pmatrix}
cos\theta - sin\theta \\
sin\theta + cos\theta \\
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
= 
\begin{pmatrix}
x cos\theta - y sin\theta \\
x sin\theta + y cos\theta \\
\end{pmatrix}
$$
}

この線形変換は三角関数の加法定理から導かれる。

\begin{numcases}
  {}
  rcos(\alpha + \beta) = rcos\alpha cos\beta - sin\alpha sin\beta \\
  rsin(\alpha + \beta) = rsin\alpha cos\beta + cos\alpha sin\beta
\end{numcases}

変換前の点P$(x, y)$、変換後の点P'$(rcos(\alpha + \beta), rsin(\alpha, \beta))$とする。

Pの座標$(x, y)$を(9), (10)にそれぞれ代入して、

\begin{eqnarray*}
rcos(\alpha + \beta) = xcos\beta - ysin\beta \\
rsin(\alpha + \beta) = ycos\beta + xsin\beta
\end{eqnarray*}

したがって、これを行列表示すると

$$
\begin{pmatrix}
x cos\beta & -y sin\beta \\
x sin\beta & y cos\beta \\
\end{pmatrix}
$$

となる。

\subsection{線形写像と行列の対応}

一般に$A = (a_{ij}) \in M_{m,n}(\mathbb{R})$が与えられた時、$V = \vecSpace{n}, V' = \vecSpace{m}$への写像$f_A$を

$$
f_A = \bm{x} = \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix} 
\longmapsto 
A\bm{x} = 
\begin{pmatrix}
{\displaystyle \sum_{1 \leq j \leq n} a_{1j}x_j} \\
\vdots \\
{\displaystyle \sum_{1 \leq j \leq n} a_{mj}x_j}
\end{pmatrix}
$$

によって定義すれば、$f_A$は線形である。つまり、

\begin{eqnarray*}
f_A(\bm{x} + \bm{y}) = A(\bm{x} + \bm{y}) = A\bm{x} + A\bm{y} \quad (\bm{x}, \bm{y} \in V) \\
f_A(c\bm{x}) = A(c\bm{x}) = cA\bm{x} \quad (c \in \mathbb{R}, \bm{x} \in V)
\end{eqnarray*}

が成立する。

したがって、上で挙げた例に登場した$f_1, f_2, f_3$はそれぞれ行列

$$
A_1 = \begin{pmatrix}
1 & -1 & 0 \\
0 & 1 & -1
\end{pmatrix} \in M_{2,3}\mathbb{R}, \quad
A_2 = (1, 1, 1) \in M_{1,3}\mathbb{R}, \quad
A_3 = \begin{pmatrix}
cos\theta & -sin\theta \\
sin\theta & cos\theta
\end{pmatrix}
$$

に対応する線形写像になっている。

\subsubsection{表現行列}

$V = \vecSpace{n}, V = \vecSpace{m}$とする。$f: V \to V'$を線形写像とする。$\{\bm{e}_1, \cdots, \bm{e}_n\}, \{\bm{e}_1', \cdots, \bm{e}_n'\}$をそれぞれ$V, V'$の標準基底とし、

$$
f(\bm{e}_j) = \sum_{1 \leq i \leq n} a_{ij}\bm{e}_{i}'
$$

とする。$f(\bm{e}_j) = \bm{a}_j$とすれば、この関係式は以下の写像$f$

$$
\begin{array}{ccc}
V & \stackrel{f}{\longrightarrow}  & V \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\bm{e}_1 & \longmapsto & \bm{a}_1 \\
\bm{e}_2 & \longmapsto & \bm{a}_2 \\
\vdots & & \vdots \\
\bm{e}_n & \longmapsto & \bm{a}_n
\end{array}
$$

を定義する。つまり、

$$
f(\bm{e}_1) = \bm{a}_1 = \begin{pmatrix}
a_{11} \\
\vdots \\
a_{m1}
\end{pmatrix}, 
f(\bm{e}_2) = \bm{a}_2 = \begin{pmatrix}
a_{12} \\
\vdots \\
a_{m2}
\end{pmatrix}, 
\cdots,
f(\bm{e}_n) = \bm{a}_n = \begin{pmatrix}
a_{n1} \\
\vdots \\
a_{mn}
\end{pmatrix}
$$

と$n$個のベクトルが得られる。これを並べると、

$$
A = (\bm{a}_1, \cdots, \bm{a}_n) = 
\begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots \\
a_{m1} & \cdots & a_{mn} \\
\end{pmatrix}
$$

のような行列$A$が得られる。つまり、
$$
f(\bm{e}_1) = A\bm{e}_1, f(\bm{e}_2) = A\bm{e}_2, \cdots, f(\bm{e}_n) = A\bm{e}_n
$$
である。

ここで、$V$の任意のベクトル$\bm{x}$は
$$
\bm{x} = \sum_{1 \leq j \leq n} x_{j}\bm{e}_j = x_1\bm{e}_1 + \cdots x_n\bm{e}_n
$$で表すことができることを思い出してほしい。これらを利用すると以下の関係式が得られる。

\begin{eqnarray*}
f(\bm{x}) &= &f(\sum_{1 \leq j \leq n} x_{j}\bm{e}_j)  \\
&= &f(x_1\bm{e}_1 + \cdots x_n\bm{e}_n) \\
&= &x_1f(\bm{e}_1) + \cdots + x_nf(\bm{e}_n) \\
&= &x_1A\bm{e}_1 + \cdots + x_nA\bm{e}_n \\
&= &Ax_1\bm{e}_1 + \cdots + Ax_n\bm{e}_n \\
&= &A(x_1\bm{e}_1 + \cdots + x_n\bm{e}_n) \\
&= &A\bm{x}
\end{eqnarray*}

つまり、$f$は$A = (a_ij)$に対応する線形写像$f_A$と一致するということである。

\defi{
$V$から$V'$への線形写像を$f$とする。$V$の基底$\{\bm{e}_1, \cdots, \bm{e}_n\}$, V'の基底$\{\bm{e}_1', \cdots, \bm{e}_n'\}$に関して、
$$
(f(\bm{e}_1), f(\bm{e}_2), \cdots, f(\bm{e}_n)) = (\bm{e}_1', \bm{e}_2', \cdots, \bm{e}_n')A
$$

の形に表される時、行列$A$を$f$の{\bf 表現行列}と呼ぶ。
}

\prop{表現行列は一意的である}

\begin{Proof*}
定理\ref{rem:VectorExpressionUniquness}より明らか。表現行列を表すベクトル$a_1, \cdots, a_n$はすべて一意的に表せるので、表現行列も一意的である。
\end{Proof*}

\exercise{
$\vecSpace{2}$のベクトル$\bm{x}, \bm{y}$と線形変換$f$の標準基底での表現行列$A$を
$$
\bm{x} = \begin{pmatrix}
2 \\
5
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
1 \\
3
\end{pmatrix}, 
A = \begin{pmatrix}
-3 & 1 \\
2 & -2
\end{pmatrix}
$$

とする。この線形変換$f$で$f(a\bm{x} + b\bm{y}) = a'\bm{x} + b'\bm{y}$となるとき、

$$
B\begin{pmatrix}
a \\
b
\end{pmatrix} = 
\begin{pmatrix}
a' \\
b'
\end{pmatrix}
$$
となる行列を求めよ。
}

$\bm{x}, \bm{y}$は$\vecSpace{2}$を張る標準基底${\bm{e}_1, \bm{e}_2}$で

$$
\begin{pmatrix} 
\bm{x} & \bm{y} 
\end{pmatrix} = 
\begin{pmatrix} 
\bm{e}_1 & \bm{e}_2
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix}
$$
と表すことができる。

$$
P = \begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix}
$$とする。


与式$f(a\bm{x} + b\bm{y}) = a'\bm{x} + b'\bm{y}$より、

$$
f(a\bm{x} + b\bm{y}) = A\begin{pmatrix} \bm{x} & \bm{y} \end{pmatrix}
\begin{pmatrix}
a \\
b
\end{pmatrix} = 
AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

また、

$$
a'\bm{x} + b'\bm{y} = 
\begin{pmatrix}
\bm{x} & \bm{y}
\end{pmatrix}
\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P\begin{pmatrix}
a' \\
b'
\end{pmatrix}
$$

の関係式が得られる。これより、

$$
P\begin{pmatrix}
a' \\
b'
\end{pmatrix} = 
AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

である。これに左から$P^{-1}$を掛けて（ここでは、$P$が可逆であることを前提に解答を進める。可逆の判定は行列式の章で解説する。）

$$
P^{-1}P\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P^{-1}AP\begin{pmatrix}
a \\
b
\end{pmatrix}
\Longleftrightarrow
\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P^{-1}AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

つまり、

$$
P^{-1}AP = 
\frac{1}{2 \cdot 3 -5 \cdot 1}\begin{pmatrix}
3 & -1 \\
-5 & 2
\end{pmatrix}
\begin{pmatrix}
-3 & -1 \\
2 & -2
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix} = 
\begin{pmatrix}
3 & 4 \\
-7 & -8
\end{pmatrix}
$$

この例題より以下の定理が得られる。

\rem{
標準基底での$\vecSpace{n}$上の線形変換$f$の表現行列を$A$とする。標準基底$\{\bm{e}_1, \cdots, \bm{e}_n\}$を、基底$\{\bm{x}_1, \cdots, \bm{x}_n\}$に取り替えたとする。
基底の変換行列をPとおくと、基底$\{\bm{x}_1, \cdots,  \bm{x}_n\}$を座標系とした新座標での線形変換$f$の表現行列は$P^{-1}AP$である。
}

\defi{
実数係数を持つ$x$の多項式全体の集合も、通常の加法・スカラー倍に関してベクトル空間である。これを

$$
\polynomialVecSet{n} = \{ a_nx^n + a_{n-1} + \cdots + a_1x + a_0 \mid a_i \in \mathbb{R} (1 \leq i \leq n)\}
$$
と表記する。
}

\exercise{
$\polynomialVecSet{4} = \{a_4x^4 + a_3x^3 + a_2x^2 + a_1x + a_0 \mid a_i \in \mathbb{R}\}$を実数係数の4次以下の多項式全体とする。線形写像$F: \polynomialVecSet{4} \to \polynomialVecSet{4}$を

$$
F(f) = \frac{df(\bm{x})}{dx}
$$
とするとき、基底$\{1, x, x^2, x^3, x^4\}$に関する$F$の表現行列を求めよ。
}

$\newline$

$$
\bm{a} = \begin{pmatrix}
a_1 \\
a_2 \\
a_3 \\
a_4 \\
a_5
\end{pmatrix}
$$を$\vecSpace{5}$の標準基底とする。写像$g: \vecSpace{5} \to \polynomialVecSet{4}$を

$$
g(\bm{a}) = a_1 + a_2 \cdot x + a_3 \cdot x^2 + a_4 \cdot x^3 + a_5 \cdot x^4
$$

と定めると、これは$\vecSpace{5}$の標準基底に$\polynomialVecSet{4}$の基底$\{1, x, x^2, x^3, x^4\}$を対応させる線形写像である。線形写像$g$に対し、$F$を適用すると

\begin{eqnarray*}
F(g(\bm{a})) &= &F(a_1 + a_2 \cdot x + a_3 \cdot x^2 + a_4 \cdot x^3 + a_5 \cdot x^4) \\
&= &a_1F(1) + a_2F(x) + a_3F(x^2) + a_4F(x^3) + a_5F(x^4) \\
&= &a_1 \cdot 0 + a_2 \cdot 1 + a_3 \cdot 2x + a_4 \cdot 3x^2 + a_5 \cdot 4x^3
\end{eqnarray*}

となる。この結果を書き直すと

$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 0 & 4 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
\begin{pmatrix}
1 & x & x^2 & x^3 & x^4
\end{pmatrix}\bm{a}
$$

となっているので、Fの表現行列

$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 0 & 4 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
$$

が得られる。この表現行列より、

$$
(F(1), F(x), F(x^2), F(x^3), F(x^4)) = (1, x, x^2, x^3, x^4)F
$$

の関係式が成立する。


\subsection{線形写像の合成と行列の積}

線形写像$f_A$と$f_B$があるとき、$f_A \circ f_B = f_{AB}$が成立する。実際、任意の$\bm{x} \in \vecSpace{n}$に対して

$$
f_A \circ f_B(\bm{x}) = f_A(f_B(\bm{x})) = A(B\bm{x}) = (AB)\bm{x} = f_{AB}(\bm{x})
$$

であるから、$f_A \circ f_B = f_{AB}$が得られた。

この関係式を具体的な線形写像$f_A, f_B$を使って表すと面白いことが分かる。

$$
A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix}, 
B = \begin{pmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{pmatrix}
$$

を表現行列とし、線形写像$f_A$と$f_B$の合成を考える。

\begin{eqnarray*}
f_A \circ f_B(\bm{x}) = f_A(f_B(\bm{x})) &= &f_A(\begin{pmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{pmatrix} \begin{pmatrix}
x \\
y
\end{pmatrix}) \\
&= &f_A(\begin{pmatrix}
xb_{11} + yb_{12} \\
xb_{21} + yb_{22}
\end{pmatrix}) \\
&= &\begin{pmatrix}
a_{11} + a_{12} \\
a_{21} + a_{22}
\end{pmatrix} 
\begin{pmatrix}
xb_{11} + yb_{12} \\
xb_{21} + yb_{22}
\end{pmatrix} \\
& = &\begin{pmatrix}
a_{11}(xb_{11} + yb_{12}) + a_{12}(xb_{21} + yb_{22}) \\
a_{21}(xb_{11} + yb_{12}) + a_{22}(xb_{21} + yb_{22}) \\
\end{pmatrix} \\
& = &\begin{pmatrix}
(a_{11}b_{11} + a_{12}b_{21})x + (a_{11}b_{12} + a_{12}b_{22})y \\
(a_{21}b_{11} + a_{22}b_{21})x + (a_{21}b_{12} + a_{22}b_{22})y
\end{pmatrix} \\
& = &\begin{pmatrix}
a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix} \\
&= &(AB)\bm{x} \\
\end{eqnarray*}

が得られる。つまり、行列の積は線形写像の合成に等しいということが分かったのである。

したがって、行列の積は線形写像の合成に対応するように定義されていたのだ。

\subsection{線形写像の核と像}

$V = \vecSpace{n}, V' = \vecSpace{m}$とし、線形写像$f: V \to V'$が与えられたとする。
$f$によって、$V'$のベクトル$0_{V'}$に移される$V$のベクトル全体の集合

$$
\ker(f) = \{\bm{x} \in V \mid f(\bm{x}) = \bm{0}\}
$$

を$f$の{\bf 核}(Kernel, カーネル)という。また、$V$の$f$による像全体の集合

$$
\img(f) = \{f(\bm{x}) \mid \bm{x} \in V\}
$$

を$f$の{\bf 像}(Image, イメージ)という。

この定義から、$f$が全射$\Longleftrightarrow \img(f) = V'$である。また、$f$が単射であれば、$\ker(f) = \{0\}$である。これを証明する。

$f$が単射であるとする。$\forall \bm{x} \in \ker(f)$をとると、$f(\bm{x}) = \bm{0}$.ここで、$f$は線形写像より、$f(\bm{0}) = \bm{0}$.よって、$f(\bm{x}) = \bm{0}$ .仮定より、$f$は単射なので$\bm{x} = \bm{0}$.よって、$\ker(f) = \{0\}$となる。

逆に、$f(\bm{x}) = f(\bm{y})$となる$\forall \bm{x}, \bm{y} \in V$に対し、$f(\bm{x}) = f(\bm{y}) = 0$。ここで$f$は線形写像なので、$f(\bm{x} - \bm{y}) = \bm{0}$が言え、$\bm{x} - \bm{y} \in \ker(f) = \{0\}$である。したがって、$\bm{x} - \bm{y} = \bm{0}$より、$\bm{x} = \bm{y}$.

\rem{
$\img(f), \ker(f)$はそれぞれ$V', V$の部分空間である。
}

\begin{Proof*}
$\img(f) \subset V'$を示す。$\bm{x}, \bm{y} \in \img(f)$とすれば、ある$\bm{x}, \bm{y} \in V$があって、$\bm{x}' = f(\bm{x}), \bm{y}' = f(\bm{y})$と表せる。よって、

\begin{eqnarray*}
\bm{x}' + \bm{y}' = f(\bm{x}) + f(\bm{y}) = f(\bm{x} + \bm{y}) \in \img(f) \\
c\bm{x}' = cf(\bm{x}) = f(c\bm{x}) \in \img(f)
\end{eqnarray*}

より、$\img(f)$は$V'$の部分空間である。

$\ker(f) \subset V$を示す。$\bm{x}, \bm{y} \in Ker(f)$とすれば、$f(\bm{x}) = f(\bm{y}) = 0$であるから

\begin{eqnarray*}
f(\bm{x} + \bm{y}) = f(\bm{x}) + f(\bm{y}) = \bm{0} \in Ker(f) \\
cf(\bm{x}) = f(c\bm{x}) = \bm{0} \in Ker(f)
\end{eqnarray*}

より、$\ker(f)$は$V$の部分空間である。
\end{Proof*}

\subsection{次元定理}

\rem{
$V = \vecSpace{n}, \dim V = n$のとき、次の等式が成立する。

$$
\dim \img(f) = n - \dim \ker(f)
$$

\begin{Proof*}
$\dim \ker(f) = s$とし、$\{\bm{x}_1, \cdots, \bm{x}_s\}$を$\ker(f)$の1つの基底とする。$s \leq n$で$V$の元$\bm{x}_{s + 1}, \cdots, \bm{x}_n$を適当に選び、$\vecSet{x}$を$V$の基底とすることができる。このとき、$f(\bm{x}_i) = \bm{0} \quad (1 \leq i \leq s)$であるから、

$$
\img(f) = \linearCombination{f(\bm{x}_1)}{f(\bm{x}_n)} = \linearCombination{f(\bm{x}_{s + 1})}{f(\bm{x}_n)}
$$

である。$f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$が線形独立であれば、このベクトルの組が$\img(f)$の基底となり、$\dim \img(f) = n - \dim \ker(f)$が証明される。$f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$が線形独立であることを示す。

$$
\sum_{s + 1 \leq j \leq n} c_jf(\bm{x}_j) = \bm{0} \quad (c_j \in \mathbb{R})
$$
と仮定すると、$f$は線形より、

$$
f(\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j) = \bm{0}
$$

であるから、

$$
\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j \in \ker(f)
$$

したがって、

$$
\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j = \sum_{1 \leq i \leq n} k_i\bm{x}_i \quad (k_i \in \mathbb{R})
$$

とかける。つまり、$\{\bm{x}_1, \cdots, \bm{x}_s\} \subset \vecSet{x}$であるが、$\{\bm{x}_1, \cdots, \bm{x}_s\}$は$f$により$\bm{0}$に潰れてしまうので、$s \leq n$であっても右辺と左辺の$\bm{0}$に移されないベクトルの個数は
一致するということを言っている。

ここで、$\bm{x}_1, \cdots, \bm{x}_n$は$V$の基底より、線形独立であるから、すべての$j$で$c_j = 0$、すべての$i$で$k_i = 0$でなければならない。よって、$f$によって移される新たな$\img(f)$の基底 $f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$も線形独立である。

また、この定理により次の等式が成立することが直ちに分かる。

$$
\dim V = \dim \ker(f) + \dim \img(f)
$$
\end{Proof*}
}\label{rem:rankNullity}

$\newline$

\exercise{
$$
A = \begin{pmatrix}
-2 & 1 & 1 \\
1 & -2 & 1 \\
1 & 1 & -2 
\end{pmatrix}
$$
によって定義される線形写像$f: \bm{x} \mapsto A\bm{x} \quad (\bm{x} \in \vecSpace{3})$に対する$\ker(f)$と$Im(f_A)$とぞれぞれの次元を求めよ。
}\label{exercise:FindingTheImageAndKernel1}

$\ker(f)$とその次元を求める。$f$のカーネルは$f$に写すと$\bm{0}$になるベクトルの集合であるから、$A\bm{x} = \bm{0}$.
つまり

$$
\begin{cases}
-2x + y + z = 0 \\
x - 2y + z = 0 \\
x + y - 2z = 0
\end{cases}
$$

を意味する。この連立方程式の解は$x = y = z$であるから

$$
\ker(f) = \left\{ \bm{x} = 
\begin{pmatrix} 
x \\
y \\
z
\end{pmatrix}
\mid x = y = z
\right\}
$$

である。ここから基底を取り出すことを考える。$\bm{x}$の成分はすべて同じ値なので、いずれかの成分を固定する（例えば、$x$で$y,z$を置き換える）ことで

$$
\bm{x} = x\begin{pmatrix} 
1 \\
1 \\
1
\end{pmatrix}
$$

と表すことができる。この1つのベクトル$\bm{x}$は線形独立であり、カーネルを張る基底であるから、$\dim \ker(f) = 1$である。

次に、$\img(f)$とその次元を求める。次元定理を使えば、$\dim \img(f) = 2$となるはずであるから、

$$
\img(f) = \left\{ \bm{x} = 
\begin{pmatrix} 
x \\
y \\
z
\end{pmatrix}
\mid x + y + z = 0
\right\}
$$

と仮定する。ここで、$x$を$x = -y + (-)z$とおけば$y$と$z$で$\bm{x}$を表現できるから、
$$
\bm{x} = y\begin{pmatrix} 
-1 \\
1 \\
0
\end{pmatrix} + z\begin{pmatrix} 
-1 \\
0 \\
1
\end{pmatrix}
$$

より、2つの線形独立なベクトル
$$
\left\{
\begin{pmatrix} 
-1 \\
1 \\
0
\end{pmatrix}, 
\begin{pmatrix} 
-1 \\
0 \\
1
\end{pmatrix}
\right\}
$$

が得られる。これらは$\vecSpace{2}$を張る基底となるから、仮定は正しいことが証明された。


\subsection{行列の階数}

\defi{
線形写像$f: V \to V'$に対し、$\dim \img(f)$を$f$の{\bf 階数}（rank, ランク）といい、$\rank f$とかく。$\rank f = \dim \img(f)$である。

また、$A \in M_{m,n}(\mathbb{R})$に対してその階数を$\rank A = \rank f_A = \dim A \vecSpace{n}$と定義する。したがって、$\rank A$は$A$の列ベクトルの中で線形独立なものの最大個数である。
}\label{defi:rankOfMatrix}

なお、$A \vecSpace{n}$の定義は次である。
$V = \vecSpace{n}$とする。$f = A$であることに着目すると
$$
Im(f) = f(V) = \{f(\bm{x}) \mid \bm{x} \in V\} = AV = \{A\bm{x} \mid \bm{x} \in V\} \subset V'
$$

\exam{
例題\ref{exercise:FindingTheImageAndKernel1}の行列$A$のランクは$\rank A = 2$である。
実際、定義\ref{defi:rankOfMatrix}に従えば、$\dim \img(f) = 2$なので、$\rank f = \dim \img(f)$である。
}

\exam{
$A = \bm{x}{}^t\bm{y}, \bm{x} \in \vecSpace{m}, \bm{y} \in \vecSpace{n}, \bm{x}, \bm{y} \neq \bm{0}$とすれば、$\rank A = 1$である。

$\bm{y} = (y_i)$とすれば、$A$の列ベクトルは$y_1\bm{x}, \cdots, y_n\bm{x}$であるから、

$$
\rank A = \dim \langle y_1\bm{x}, \cdots, y_n\bm{x} \rangle = \dim \langle \bm{x} \rangle = 1
$$
である。
}

\rem{
$A \in M_{m,n}(\mathbb{R}), f_A: V \to V'$とすれば、
\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\rank A \leq \min(m, n)$
\item $\rank A = m \Longleftrightarrow f$は全射 (このとき、$n \geq m$)
\item $\rank A = n \Longleftrightarrow f$は単射 (このとき、$n \leq m$)
\end{enumerate}

\begin{Proof*}
\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $AV \subset V'$であるから、$\dim AV \leq \dim V' = m$.よって、$\rank A \leq m$である。しかし、$\dim AV = n - \dim \ker(f)$より、$\dim AV \leq n$.よって、$\rank A \leq \min(m, n)$

\item $\rank A = m$ならば、
$$
AV \subset V', \dim AV = \dim V'
$$
であるから、定理\ref{rem:dimMatching}より$AV = V'$となり、$f_A$は全射である。逆に$f_A$が全射であるとすれば、$n \geq m$より、$\rank A = m$は明らか。

\item $\rank A = n$ならば、定理\ref{rem:rankNullity}より

$$
\dim \ker(f) = 0
$$

したがって、$\ker(f) = \{0\}$. つまり、$f_A$は単射である。逆に$f_A$が単射とする。$n \leq m$より、$\rank A = n$は明らか。

\end{enumerate}
\end{Proof*}
}\label{rem:dimentionRule}

\exam{
$\rank A = m$で$f$は全射の例

$f: V \to V'$を以下の写像とすれば、$f$は全射である。

$$
A = \begin{pmatrix} 
1 & 2 & 3 \\
4 & 5 & 6
\end{pmatrix} \in M_{2, 3}(\mathbb{R})
$$

$$
\begin{array}{ccc}
\vecSpace{3} & \stackrel{f = A}{\longrightarrow} & \vecSpace{2} \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} & \longmapsto & \begin{pmatrix}
x_1  + 2x_2 + 3x_3 \\
4x_1 + 5x_2 + 6x_3
\end{pmatrix}
\end{array}
$$
}

\exam{
$\rank A = n$ならば$f$は単射の例

$f: V \to V'$を以下の写像とすれば、$f$は単射である。

$$
A = \begin{pmatrix} 
1 & 4 \\
2 & 5 \\
3 & 6
\end{pmatrix} \in M_{3, 2}(\mathbb{R})
$$

$$
\begin{array}{ccc}
\vecSpace{2} & \stackrel{f = A}{\longrightarrow} & \vecSpace{3} \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2
\end{pmatrix} & \longmapsto & \begin{pmatrix}
x_1  + 4x_1 \\
2x_2 + 5x_2 \\
3(x_1 + x_2) + 6(x_1 + x_2)
\end{pmatrix}
\end{array}
$$
}

\subsection{基本変形}

具体的な行列の階数は基本変形を用いて求めることができる。ここでは基本変形の定義とそれを使った行列の階数の求め方を示す。

$A = (a_{ij}) \in M_{m,n}(\mathbb{R})$とする。$A$に対する{\bf 基本変形}(初等変形)とは以下の3つの操作とその組み合わせのことである。


\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A$のi行(列)を定数倍する。
$$
A \longrightarrow cA = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
ca_{i1} & ca_{i2} & \cdots & ca_{in} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix} \quad (c \in \mathbb{R})
$$

の変換を指す。

\item $A$の第$i$行(列)と第$j$行(列)を入れ替える。

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{i1} & a_{i2} & \cdots & a_{in} \\
\vdots & \vdots & \vdots & \vdots \\
a_{j1} & a_{j2} & \cdots & a_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix} \\
\longrightarrow \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{j1} & a_{j2} & \cdots & a_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{i1} & a_{i2} & \cdots & a_{in} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix}
$$

の変換を指す。

\item $A$の第$i$行(列)に第$j$行(列)の定数倍を加える。

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{i1} & a_{i2} & \cdots & a_{in} \\
\vdots & \vdots & \vdots & \vdots \\
a_{j1} & a_{j2} & \cdots & a_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix} \\
\longrightarrow \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{i1} + ca_{j1} & a_{i2} + ca_{j2} & \cdots & a_{in} + ca_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{j1} & a_{j2} & \cdots & a_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix} \quad (c \in \mathbb{R})
$$

の変換を指す。
\end{enumerate}

行に関する基本変形を行基本変形、列に関する基本変形を列基本変形という。

この基本変形を適当な組み合わせで繰り返し$A$に適用することで、$A$を標準形

$$
\begin{pmatrix}
E_r & 0 \\
0 & 0
\end{pmatrix}
$$


(ここで、$E_r$は$A$の階数における単位行列である)に変換することができる。$E_r$が階数を表すので、標準形に変換する行為は行列の階数を求めることにほかならない。

% $\newline$

% また、この操作を列に適用することを列基本変形という。

\exam{
行列$A = \begin{pmatrix}
0 & 1 & 2 \\
-1 & 0 & 3 \\
-2 & -3 & 0
\end{pmatrix}$の階数を基本変形を用いて求める方法は以下である。


\begin{eqnarray*}
\begin{pmatrix}
0 & 1 & 2 \\
-1 & 0 & 3 \\
-2 & -3 & 0
\end{pmatrix} &\xrightarrow{第1列と第2列を入れ替え} &\begin{pmatrix}
1 & 0 & 2 \\
0 & -1 & 3 \\
-3 & -2 & 0
\end{pmatrix} \\[1ex]
&\xrightarrow{第1行を3倍して第3行に加える} &\begin{pmatrix}
1 & 0 & 2 \\
0 & -1 & 3 \\
0 & -2 & 6
\end{pmatrix} \xrightarrow{第1列を-2倍して第3列に加える} \begin{pmatrix}
1 & 0 & 0 \\
0 & -1 & 3 \\
0 & -2 & 6
\end{pmatrix} \\[1ex]
&\xrightarrow{第2行に-1倍する} &\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & -3 \\
0 & -2 & 6
\end{pmatrix} \xrightarrow{第2行を-2倍し、第3行に加える} \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & -3 \\
0 & 0 & 0
\end{pmatrix}
\end{eqnarray*}

この基本変形より、$\rank A = 2$である。

この例より、行列の基本変形は連立方程式の変形であることが分かる。よって、連立方程式の変形の際に出来ないような操作（e.g.行基本変形と列基本変形が同時に行われる）は行基本変形においても行うことが出来ない。
}

\subsection{基底の延長}

$V = \vecSpace{n}$とする。$W \subset V, \rank W = r$とする。このとき$W$を張る基底を$\{\bm{a}_1, \cdots, \bm{a}_r\}$とすると、$V$の基底は$\{\bm{a}_1, \cdots, \bm{a}_r, \bm{v}_{r + 1}, \cdots, \bm{v}_n \} \quad (\{ \bm{v}_i \} \in V)$と表すことができる。これを{\bf 基底の延長}という。これは以下により求められる。
$\newline$

$A = (\bm{a}_1, \cdots, {}\bm{a}_r) \in M_r(\mathbb{R})$とする。$A$の横に$n$次単位行列$E$を並べて

$$
B = (AE) = (\bm{a}_1, \cdots, {}\bm{a}_r, \bm{e}_1, \cdots, {}\bm{e}_n) \in M_{n, (r + n)}(\mathbb{R})
$$

を作る。このとき、$B$の階数は$E$の階数と同様であるから$\rank B = n$である。ここで、$B$を基本変形すると先頭から$r$個までのベクトル$\{\bm{a}_i\}$は$W$の基底より、$W$の標準基底$\{\bm{e}_1, \cdots, \bm{e}_r\}$に変形し

$$
C = (\bm{e}_1, \cdots, {}\bm{e}_r, \bm{v}_1, \cdots, \bm{v}_n)
$$

とすることができる。残りの$\bm{v}_1, \cdots, \bm{v}_n$は線形独立ではないが、$B$の階数が$n$であることから、$\bm{e}_1, \cdots, {}\bm{e}_r$とともに$V$を張る$n-r$個の線形独立なベクトルが含まれていなければならない。そのようなベクトルを$\bm{v}_{r + 1}, \cdots, \bm{v}_n$として選び出すと

$$
V = <\bm{a}_1, \cdots, \bm{a}_r, \bm{v}_{r + 1}, \cdots, \bm{v}_n>
$$

として、$W$の基底を延長し、$V$を作ることができる。


\section{行列式}

\subsection{2次行列における行列式}

\defi{2次行列における行列式を次のように定義する。}

$$
A = \begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
$$
に対して、

$$
\det(A) = \abs{A} = \begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix} = ad - bc
$$

である。また、Aの列ベクトル

$$
\bm{a}_1 = \begin{pmatrix}
a \\
c
\end{pmatrix},
\bm{a}_2 = \begin{pmatrix}
b \\
d
\end{pmatrix}
$$に対して、

$$
det(A) = D(\bm{a}_1, \bm{a}_2)
$$
とする。

\subsubsection{行列式の性質}

上で定義した行列式$D$を$V = \vecSpace{2}$上の2変数関数と考えると、次の性質を持つ。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item {\bf $\bm{a}_1, \bm{a}_2$について線形(多重線形性)}

$\bm{a}_1$について

\begin{eqnarray*}
&D(\bm{a}_1 + \bm{a}_1', \bm{a}_2) = D(\bm{a}_1, \bm{a}_2) + D(\bm{a}_1',  \bm{a}_2) \\
&D(c\bm{a}_1, \bm{a}_2) = cD(\bm{a}_1, \bm{a}_2) \quad (c \in \mathbb{R})
\end{eqnarray*}

$\bm{a}_2$について

\begin{eqnarray*}
&D(\bm{a}_1, \bm{a}_2 + \bm{a}_2') = D(\bm{a}_1, \bm{a}_2) + D(\bm{a}_1,  \bm{a}_2') \\
&D(\bm{a}_1, c\bm{a}_2) = cD(\bm{a}_1, \bm{a}_2) \quad (c \in \mathbb{R})
\end{eqnarray*}


\item {\bf $\bm{a}_1, \bm{a}_2$について交代的}

$$
D(\bm{a}_2, \bm{a}_1) = -D(\bm{a}_1, \bm{a}_2)
$$

これは以下のように展開できる。

$$
D(\bm{a}_2, \bm{a}_1) = \begin{vmatrix}
b & a \\
d & c
\end{vmatrix} = bc - ad = -D(\bm{a}_1, \bm{a}_2)
$$

\item $\vecSpace{2}$の単位ベクトルを$\bm{e}_1, \bm{e}_2$とすれば、

$$
D(\bm{e}_1, \bm{e}_2) = 1
$$

これは以下のように展開できる。

$$
D(\bm{e}_1, \bm{e}_2) = \begin{vmatrix}
1 & 0 \\
0 & 1
\end{vmatrix} = 1
$$

\end{enumerate}

この性質から2次行列式はこれらの性質によって導くことができる。$\vecSpace{2}$上の2変数関数$D(\bm{a}_1, \bm{a}_2)$が(1), (2), (3)の性質を持つとすると、(1)より

\begin{eqnarray*}
D(\bm{a}_1, \bm{a}_2) &= &D(a\bm{e_1} + c\bm{e_2}, b\bm{e_1} + d\bm{e_2}) \\
&= &aD(\bm{e_1}, b\bm{e_1} + d \bm{e}_2) + cD(\bm{e_2}, b\bm{e_1} + d\bm{e_2}) \\
&= &abD(\bm{e}_1, \bm{e}_1) + adD(\bm{e}_1, b\bm{e}_2) + cbD(\bm{e}_2, \bm{e}_1) + cdD(\bm{e}_2, \bm{e}_2)
\end{eqnarray*}

(2), (3)より

$$
D(\bm{e}_1, \bm{e}_2) = 1, \quad D(\bm{e}_2, \bm{e}_1) = -D(\bm{e}_1, \bm{e}_2) = -1
$$

$D(\bm{e}_1, \bm{e}_1) = -D(\bm{e}_1, \bm{e}_1)$であるから、$D(\bm{e}_1, \bm{e}_1) = 0$.同様に、$D(\bm{e}_2, \bm{e}_2) = 0$となる。

したがって、(1)で得た関係式に代入すると

\begin{equation*}
\begin{split}
D(\bm{a}_1, \bm{a}_2) &= abD(\bm{e}_1, \bm{e}_1) + adD(\bm{e}_1, b\bm{e}_2) + cbD(\bm{e}_2, \bm{e}_1) + cdD(\bm{e}_2, \bm{e}_2) \\
&= ab \cdot 0 + ad \cdot -1 + cb \cdot + 1 + cd \cdot 0 \\
&= ad - bc
\end{split}
\end{equation*}

である。

\subsection{置換}

前節で、2変数の関数$D(\bm{a}_1, \bm{a}_2)$を紹介したが、一般には2次行列式の性質(1),(2),(3)と類似の性質を持つn変数の関数
$$
D(\bm{a}_1, \bm{a}_2, \cdots, \bm{a}_n) \quad (\bm{a}_1, \bm{a}_2, \cdots, \bm{a}_n \in \vecSpace{n})
$$
が存在する。これをn次の行列式という。ここでは、このn次の行列式を導くために必要な置換について概説する。

n個の文字の集合

$$
X = \{1, 2, \cdots, n\}
$$

を考え、$X$のそれ自身への全単射

$$
\sigma: X \to X
$$

をXの{\bf 置換(permutation)}という。($X$は高々加算な有限集合として考える。)
このn文字の置換の集合を$S_n$とかく。任意の置換$\sigma \in S_n$が$1, 2, \cdots, n$から$i_1, i_2, \cdots, i_n$へ1対1で対応しているとき、つまり$\sigma(k) = i_k \quad (1 \leq k \leq n)$のとき

$$
\sigma = \begin{pmatrix}
1 & 2 & \cdots & n \\
i_1 & i_2 & \cdots &i_n
\end{pmatrix}
$$

と書く。特に、$i_1, i_2, \cdots, i_n$が自分自身であるような置換

$$
\sigma = \begin{pmatrix}
1 & 2 & \cdots & n \\
1 & 2 & \cdots & n \\
\end{pmatrix}
$$

を{\bf 恒等置換}という。これを単に1と表す。
$\newline$

置換に対し、$i,j$の2つの文字のみを入れ替え、その他の文字は不変にするような置換

$$
\sigma = \begin{pmatrix}
1 & \cdots & i & \cdots & j \cdots & n \\
1 & \cdots & j & \cdots & i \cdots & n \\
\end{pmatrix}
$$

を{\bf 互換(transposition)}といい、(i, j)とかく。具体的には$S_4$であれば、

$$
\sigma = \begin{pmatrix}
1 & 2 & 3 & 4 \\
1 & 3 & 2 & 4 \\
\end{pmatrix} = (2, 3)
$$
のように書き、不変な置換$\sigma(1) = 1, \sigma(4) = 4$は省略することができる。この書き方の理由として、
$\sigma(i) = j, \sigma(j) = i$と$i \to j, j \to i$のように$i$と$j$の対応のみが交換されており、その他は不変であることから、$i, j$の2つの文字だけで$\sigma$を表現するに十分だからである。
$\newline$

また、２つの置換$\sigma, \gamma \in S_n$に対し、その合成

$$
(\sigma\gamma)(k) = (\sigma(\gamma(k))
$$

によって定義される写像$\sigma\gamma$もまた１つの置換である。これを$\sigma, \gamma$の{\bf 積}という。また$\sigma \in S_n$は全単射であるから、その逆写像$\sigma^{-1}$が存在し、それもまた置換となる。これを$\sigma$の{\bf 逆置換}という。

\exam{置換の積の例}

$$
\gamma = \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix}, \sigma = \begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix}
$$
の置換の積を考えると、

$$
\gamma(\sigma(1)) = 1, \quad \gamma(\sigma(2)) = 3, \quad \gamma(\sigma(3)) = 2
$$

より、

\begin{equation*}
\begin{split}
\gamma\sigma &= \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix} \begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix}\\[1ex]
&= \begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix}\\[1ex]
&= \begin{pmatrix}
2 & 3 \\
3 & 2
\end{pmatrix}\\[1ex]
&= (2, 3)
\end{split}
\end{equation*}

となる。

\exercise{$S_3$のすべての置換を答えよ。}\label{exercise:permutation1}

{\bf 解答}

置換は自身から自身への対応の並び替えであるから、その総数は$n!$個である。よって、$S_3$では、$3!$個の置換がありそれは以下となる。

\begin{eqnarray*}
\begin{pmatrix}
1 & 2 & 3 \\
1 & 2 & 3
\end{pmatrix} = 1, \quad \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix}, \quad \begin{pmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{pmatrix}, \\[1.5ex]
\begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix} = (2, 3), \quad 
\begin{pmatrix}
1 & 2 & 3 \\
2 & 1 & 3
\end{pmatrix} = (1, 2), \quad 
\begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix} = (1, 3)
\end{eqnarray*}

\subsubsection{互換の積}

$\sigma \in S_3$を

$$
\sigma = \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix}
$$

とする。この置換はいくつかの{\bf 互換の積}で

$$
\sigma = (1, 3)(1, 2) = (1,2)(2,3)
$$

のように表すことができる。実際、


$$
(1, 2) = \begin{pmatrix}
1 & 2 & 3\\
2 & 1 & 3
\end{pmatrix}, \quad 
(2, 3) = \begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix}
$$

であるから$\gamma = (1, 2), \rho = (2, 3)$としてその合成$\gamma\rho$を考えると

\begin{eqnarray*}
\gamma(\rho(1)) = \gamma(1) = 2 \\
\gamma(\rho(2)) = \gamma(3) = 3 \\
\gamma(\rho(3)) = \gamma(3) = 1
\end{eqnarray*}

となることから

$$
\gamma\rho = \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix} = \sigma
$$

である。（互換の合成も写像の合成同様に、右側の置換（互換）から評価する。）

\exercise{
$
\begin{pmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{pmatrix}
$を互換の積で表わせ。
}

{\bf 解答}

この置換は

$$
\begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix} = (1, 3),\quad 
\begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix} = (2, 3)
$$
および

$$
\begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix} = (2, 3),\quad 
\begin{pmatrix}
1 & 2 & 3 \\
2 & 1 & 3
\end{pmatrix} = (1, 2)
$$

の互換の積で

$$
\begin{pmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{pmatrix} = (1,3)(2,3) = (2,3)(1,2)
$$のように表すことができる。
$\newline$

実は、任意の$\sigma \in S_n$は表示は一定でないにしろ、互換の個数の偶奇は一定であることが知られている。
それを示すために、次の補題を証明する。

\lemm{
$S_n$上に定義された関数$sign(\sigma)$が次の性質のもと一意的に存在する。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $sign(\sigma) \in \{\pm 1\}$
\item $\sigma$が互換ならば、$sign(\sigma) = -1$
\item $sign(\sigma\gamma) = sign(\sigma)sign(\gamma)$
\end{enumerate}

\begin{Proof*}
このような関数は
$$
sign(\sigma) = \prod_{i \neq j} \frac{\sigma(i) - \sigma(j)}{i-j}
$$
によって与えられる。

$\sigma \in S_n$とする。$\sigma$を互換とする。$\sigma$は互換より、$i \neq j$のとき、
$\sigma(k) = k, \sigma(j) = i, \sigma(i) = j$. それ以外、つまり$i = j$ですべての$sign(k) = k$であるから、右辺の積は

$$
\frac{i - j}{i - j} = 1
$$

となる。これより、$i \neq j$のときに

$$
\frac{j - i}{i - j} = -1
$$

となることから、(2)を満たす。

したがって、$\sigma$が$k$個の互換の積$\tau_1, \cdots, \tau_k$の積に等しければ、関数$sign$は次のように展開できる。

$$
sign(\sigma) = sign(\tau_1) \cdots sign(\tau_k) = (-1)^k \quad (k \in \mathbb{N})
$$

これより(1)を満たす。（この関数はwell-definedであることが分かる。）
また、$(\tau_1, \cdots, \tau_k) = \sigma$より、$sign(\tau_1, \cdots, \tau_k) = sign(\tau_1) \cdots sign(\tau_k)$であるから、(3)を満たすことも確かめられる。

$sign(\sigma) = 1$のとき互換の数は偶数、$sign(\sigma) = -1$のとき互換の数は奇数である。kの偶奇により、$\sigma$を{\bf 偶置換}、{\bf 奇置換}という。
\end{Proof*}

}\label{lemm:signFunction}

この補題より、次の定理が証明される。

\rem{
任意の$\sigma \in S_n$は高々$n-1$個の互換の積に表される。このような表示は一意的ではないが、互換の個数の偶奇は一定である。
}

第一の主張はnの帰納法で容易に証明され、第二の主張は補題\ref{lemm:signFunction}により証明される。
$\newline$

\exam{
$\begin{pmatrix}
1 & 2 & 3 \\
1 & 2 & 3
\end{pmatrix}, \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix}, \begin{pmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{pmatrix}$
は偶置換、
$\begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix}, \begin{pmatrix}
1 & 2 & 3 \\
2 & 1 & 3
\end{pmatrix}, \begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix}$は奇置換である。

このように$\sigma \in S_n$の遇奇の個数は一致する。（証明は割愛する）
}

\subsubsection{巡回置換}

相異なる文字$i_1, i_2, \cdots, i_k$を$i_1$を$i_2$に$i_2$を$i_3$に...と順次次の文字に移し、最後の$i_k$を$i_1$に戻すような（ループを描くような）置換を$i_1, i_2, \cdots, i_k$の巡回置換(cycle)といい、$(i_1, i_2, \cdots ,i_k)$とかく。互換はこの特別の場合である。よって、巡回置換は

$$
(i_1, i_2, \cdots ,i_k) = (i_1, i_k)(i_1, i_{k-1})\cdots(i_1, i_2)
$$
のように、互換の積に分解できる。したがって、

$$
sign(i_1, i_2, \cdots ,i_k) = (-1)^{k-1}
$$
である。

\subsection{n次行列式の定義と基本性質}

\rem{$V = \vecSpace{n}$上に次の性質(D1), (D2), (D3)をもつようなn変数の関数$D(\bm{a}_1, \cdots, \bm{a}_n)$が一意的に存在する。

この関数をn次の{\bf 行列式(determinant)}といい、$A = (\bm{a}_1, \cdots, \bm{a}_n)$であるとき、

$$
D(\bm{a}_1, \cdots, \bm{a}_n) = \det(A) = \abs{A} = \abs{\bm{a}_1, \cdots, \bm{a}_n}
$$
などと書く。

この定理の証明の前に、まずは性質(D1), (D2), (D3)を解説する。

\begin{enumerate}
\renewcommand{\labelenumi}{(D\arabic{enumi})}
\item $D(\bm{a}_1, \cdots, \bm{a}_n)$は各変数$\bm{a}_i$に関して線形である。
\begin{eqnarray*}
&D(\bm{a}_1 + \bm{a}_1`, \cdots, \bm{a}_n) = D(\bm{a}_1, \cdots, \bm{a}_n) + D(\bm{a}_1`, \cdots, \bm{a}_n), \\
&D(c\bm{a}_1, \cdots, \bm{a}_n) = cD(\bm{a}_1, \cdots, \bm{a}_n)
\end{eqnarray*}

\item ２つの変数$\bm{a}_i, \bm{a}_j \hspace{2pt} (i \neq j)$を入れ替えると、Dの符号が変わる。つまり、$\bm{a}_1, \cdots, \bm{a}_n$に関して交代的である。

$$
D(\cdots, \bm{a}_j, \cdots, \bm{a}_i, \cdots) = -D(\cdots, \bm{a}_i, \cdots, \bm{a}_j, \cdots)
$$

\item $\vecSpace{n}$の基本ベクトル$\bm{e}_1, \cdots, \bm{e}_n$に対して

$$
D(\bm{e}_1, \cdots, \bm{e}_n) = 1
$$

\end{enumerate}
}\label{rem:detDef}

また、(D2)は(D1)の下に、次の(D2`)と同値である。

(D2') $\quad$ $\bm{a}_i = \bm{a}_j \hspace{1pt} (i \neq j)$のとき、

$$
D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) = 0
$$

実際、(D2)を仮定すると

$$
D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) = -D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots)
$$

であるから、この主張が言える。

逆に(D2')を仮定すると、(D1)により$D(\cdots, \bm{a}_i, \cdots, \bm{a}_j, \cdots)$の第$i, j$番目の変数に$\bm{a}_i + \bm{a}_j$を代入して


\begin{eqnarray*}
D(\cdots, \bm{a}_i + \bm{a}_j, \cdots, \bm{a}_i + \bm{a}_j, \cdots) &= &D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) + D(\cdots, \bm{a}_i, \cdots, \bm{a}_j, \cdots) \\
&+ &D(\cdots, \bm{a}_j, \cdots, \bm{a}_i, \cdots) + D(\cdots, \bm{a}_j, \cdots, \bm{a}_j, \cdots) \\
&= &0 + 1 - 1 + 0 \\
&= &0
\end{eqnarray*}

(D2')により

$$
D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) = D(\cdots, \bm{a}_j, \cdots, \bm{a}_j, \cdots) = 0
$$

であるから、右辺の$\bm{a}_j, \bm{a}_j$を入れ替えても

$$
D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) = -D(\cdots, \bm{a}_j, \cdots, \bm{a}_j, \cdots)
$$

とでき、(D2)の交代式を得る。よって、(D2)と(D2')はこの条件のもと同値である。

$\newline$
これらの性質より、定理\ref{rem:detDef}は次のように証明される。
$\newline$

\begin{Proof*}
(D1), (D2), (D3)の性質をもつ関数$D(\bm{a}_1, \cdots, \bm{a}_n) \quad (\bm{a}_j \in \vecSpace{n})$が存在すると仮定する。
また、$A = (\bm{a}_1, \cdots, \bm{a}_n) \in M_n(\mathbb{R})$とする。

$A$の各ベクトル$\bm{a}_j$は線形結合により

$$
\bm{a}_j = \sum_{1 \leq i \leq n} a_{ij}\bm{e}_i \quad (a_{ij} \in \mathbb{R})
$$

と表せる。(D1)より

\begin{equation}
D(\bm{a}_1, \cdots, \bm{a}_n) = \sum_{i_1, \cdots , i_n = 1}^{n} a_{{i_1}1} \cdots a_{{i_n}n}D(\bm{e}_{i_1}, \cdots, \bm{e}_{i_n})
\label{equation:Det1}
\end{equation}

ここで、$i_1, \cdots, i_n$は独立に1からnまで動く。(D2),(D3)より$D(\bm{e}_1, \cdots, \bm{e}_n)$は、$(i_1, \cdots, i_n)$が$(1, \cdots, n)$の順列になっているときには、置換$\sigma = \begin{pmatrix}
1 & \cdots & n \\
i_1 & \cdots & i_n
\end{pmatrix}$の符号に等しく、その他に$i_1, \cdots, i_n$の中に同じ数が含まれているときは$0$となる。したがって、

$$
D(\bm{e}_1, \cdots, \bm{e}_n) = \pm 1
$$

であるから、式(\ref{equation:Det1})は次のように表せる。

\begin{equation}
D(\bm{a}_1, \cdots, \bm{a}_n) = \sum_{\sigma \in S_n} sign(\sigma) \hspace{1pt} a_{\sigma(1)1} \cdots a_{\sigma(n)n}
\label{equation:Det2}
\end{equation}

このように、$D(\bm{a}_1, \cdots, \bm{a}_n)$は$a_{ij}$の多項式として明らかに一意的に定まる。($A$に対して$\sigma \in S_n$は$n!$個の組み合わせのみである。)

逆に、式(\ref{equation:Det1})によって$D(\bm{a}_1, \cdots, \bm{a}_n)$を定義すれば、それが(D1),(D2),(D3)の性質を持つことは、補題\ref{lemm:signFunction}で示したように$sign(\sigma)$の性質から確認できる。よって、行列式は式(\ref{equation:Det1})によって定義でき、それは一意的に存在する。
\end{Proof*}

\rem{
$V = \vecSpace{n}$上の関数Fが定理\ref{rem:detDef}における$D$の性質(D1),(D2)をもてば、ある定数cがあるとき

$$
F(\bm{a}_1, \cdots, \bm{a}_n) = c\abs{A}
$$

ただし、$c = F(\bm{e}_1, \cdots, \bm{e}_n)$
}

\exam{
$$
A = \begin{pmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{pmatrix} \in M_3(\mathbb{R})
$$の行列式は以下の要領で得られる。

例題\ref{exercise:permutation1}で得られた置換$\sigma \in S_n$を左上から順に、$\sigma_1, \cdots, \sigma_6$とラベリングすると、

\begin{equation*}
\begin{split}
  \det(A) 
  ={}&
  sign(\sigma_1)\hspace{1pt}a_{\sigma_{1}(1)1}a_{\sigma_{1}(2)2}a_{\sigma_{1}(3)3}
  + sign(\sigma_2)\hspace{1pt}a_{\sigma_{2}(1)1}a_{\sigma_{2}(2)2}a_{\sigma_{2}(3)3}
  + sign(\sigma_3)\hspace{1pt}a_{\sigma_{3}(1)1}a_{\sigma_{3}(2)2}a_{\sigma_{3}(3)3}
  \\&+
  sign(\sigma_4) \hspace{1pt}a_{\sigma_{4}(1)1}a_{\sigma_{4}(2)2}a_{\sigma_{4}(3)3}
  + sign(\sigma_5)\hspace{1pt}a_{\sigma_{5}(1)1}a_{\sigma_{5}(2)2}a_{\sigma_{5}(3)3}
  + sign(\sigma_6)\hspace{1pt}a_{\sigma_{6}(1)1}a_{\sigma_{6}(2)2}a_{\sigma_{6}(3)3}
  \\={}&
  a_{11}a_{22}a_{33} + a_{21}a_{32}a_{13} + a_{31}a_{12}a_{23}
  \\&- a_{11}a_{32}a_{23} - a_{21}a_{12}a_{33} - a_{31}a_{22}a_{13}
\end{split}
\end{equation*}

が得られる。
}


\rem{
$A \in M_n(\mathbb{R})$の転置行列を$\transposeMat{A}$とすれば、

$$
\abs{\transposeMat{A}} = \abs{A}
$$

となる。

\begin{Proof*}
$\transposeMat{A}$の列ベクトルを$\bm{a}'_1 \cdots, \bm{a}'_n$とすれば、$\transposeMat{\bm{a}'_1} \cdots, \transposeMat{\bm{a}'_n}$は$A$の行ベクトルである。よって、

$$
\bm{a}_j = \sum_{i=1}^{n}a_{ij}\bm{e}_i
$$

とすれば、

$$
\bm{a}'_j = \sum_{j=1}^{n}a_{ij}\bm{e}_i
$$

である。行列式の定義式を$\transposeMat{A}$に適用して

\begin{eqnarray*}
\abs{\transposeMat{A}} &= &D(\bm{a}'_1 \cdots, \bm{a}'_n) \\
&= &\sum_{\sigma \in S_n}sign(\sigma) \hspace{5pt} a_{\sigma(1)1} \cdots a_{\sigma(n)n}
\end{eqnarray*}


$\sigma$の逆置換を$\sigma^{-1}$とすれば、$sign(\sigma) = sign(\sigma^{-1})$は明らかであるから、

$$a_{\sigma(1)1} \cdots a_{\sigma(n)n} = a_{\sigma^{-1}(1)1} \cdots a_{\sigma^{-1}(n)n}
$$
である。これは、$\sigma$が$S_n$上を動くとき、$\sigma^{-1}$も$S_n$の上を動くことから

$$
\abs{\transposeMat{A}} = \abs{A}
$$

が得られる。
\end{Proof*}
}

\subsubsection{様々な行列式の例}

\exam{\bf 上半三角、下半三角行列の行列式}

n次行列$A = (a_{ij})$が上半三角で、n次行列$B = (b_{ij})$が下半三角であるとき、つまり

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
0 & a_{22} & \cdots & a_{2n} \\
\cdots & \cdots & \ddots & \cdots \\
0 & 0 & \cdots & a_{nn}
\end{pmatrix}
$$

$$
B = \begin{pmatrix}
b_{11} & 0 & \cdots & 0 \\
b_{21} & b_{22} & \cdots & 0 \\
\cdots & \cdots & \ddots & \cdots \\
b_{n1} & b_{n2} & \cdots & b_{nn} \\
\end{pmatrix}
$$

の形である時、AとBの行列式は

\begin{eqnarray*}
\abs{A} &= &a_{11}a_{22} \cdots a_{nn} \\
\abs{B} &= &b_{11}b_{22} \cdots b_{nn}
\end{eqnarray*}

のように対角成分の積に等しい。

\begin{Proof*}
$A, B$の形状より

\begin{eqnarray*}
i > j \Longrightarrow a_{ij} = 0 \\
i < j \Longrightarrow b_{ij} = 0
\end{eqnarray*}

したがって、式(\ref{equation:Det2})において

\begin{eqnarray*}
\sigma(i) \leq i \quad (1 \leq i \leq n) \\
\sigma(i) \geq i \quad (1 \leq i \leq n) \\
\end{eqnarray*}

であるような$\sigma$に対する項だけが残るが、この条件を満たす$\sigma$は恒等置換$\sigma(i) = i$のみであるから、ここで主張する$\abs{A}, \abs{B}$を得る。
\end{Proof*}

\exam{
$n$次行列$A = (a_{ij})$が

$$
A = \begin{pmatrix}
A_{11} & A_{12} \\
0 & A_{22}
\end{pmatrix},
$$

$$
A_{11} \in M_{n_1}(\mathbb{R}), \quad A_{12} \in M_{n_1,n_2}(\mathbb{R}), 
\quad A_{11} \in M_{n_2}(\mathbb{R}), \quad n_1 + n_2 = n
$$
の形ならば、


\begin{equation}
\abs{A} = \abs{A_{11}}\abs{A_{22}}
\label{equation:Det3}
\end{equation}

\begin{Proof*}
$A$の形状より、$i > n_1, \hspace{5pt} j \leq n_1$のとき$a_{ij} = 0$となるから、式(\ref{equation:Det2})において

$$
j \leq n_1 \Longrightarrow \sigma(j) \leq n_1
$$

でるような$\sigma$に対する項だけが残る。この条件を満たす$\sigma$は$\{1, \cdots, n_1\}$の置換$\sigma_1$と$\{n_1 + 1, \cdots, n\}$の置換$\sigma_2$の積として表される。$\{1, \cdots, n_1\}$の置換全体の集合を$S_{n_1}, {n_1 + 1, \cdots, n}$の置換全体の集合を$S_{n-{n_1}}$とおけば、$\sigma_1, \sigma_2$は独立にそれぞれ$S_{n_1}, S_{n-{n}_1}$の上を動く。また、$\sigma = \sigma_1\sigma_2$であるとき、明らかに

$$
sign(\sigma) = sign(\sigma_1)sign(\sigma_2)
$$

である。よって、

\begin{equation}
\begin{split}
  \abs{A}
  ={}&
  \sum_{\sigma_1 \in S_{n_1}} sign(\sigma_1) \hspace{2pt} a_{\sigma_1(1)1} \cdots a_{\sigma_1({n_1}){n_1}}
  \\&\times 
  \sum_{\sigma_1 \in S_{n_2}} sign(\sigma_2) \hspace{2pt} a_{\sigma_2({n_1}+1){n_1+1}} \cdots a_{\sigma_2(n)n}
  \\={}&
  \abs{A_{11}}\abs{A_{22}}
\end{split}
\end{equation}
\end{Proof*}
}\label{exam:det2}

\exercise{
$n_1 = 2, n_2 = 3$として、例\ref{exam:det2}を確かめよ。

{\bf 解答}

$\{1, 2\}$の置換全体の集合を$S_2$、$\{3, 4, 5\}$の置換全体の集合を$S_3$とする。$\sigma_1 \in S_2, \sigma_2 \in S_3$とする。
$\sigma_1$と$\sigma_2$はそれぞれ$S_2, S_3$の上を独立に動くから、行列式は以下のようになる。

\begin{equation*}
\begin{split}
  \abs{A}
  ={}&
  a_{11}a_{22} - a_{21}a_{12}
  \\&\times 
    a_{33}a_{44}a_{55} + a_{43}a_{54}a_{35} + a_{53}a_{34}a_{45}
  \\&- a_{33}a_{54}a_{45} - a_{43}a_{34}a_{55} - a_{53}a_{44}a_{35}
  \\={}&
  \abs{A_{11}}\abs{A_{22}}
\end{split}
\end{equation*}

したがって、例\ref{exam:det2}と同様の行列式が得られた。
}


\subsubsection{余因子}

$n$次行列$A = (a_{ij}) = (\bm{a}_1, \cdots, \bm{a}_n)$の第$i$行、第$j$列を取り去ってできる$(n - 1)$次行列を$A^{(ij)}$とかくことにする。そのとき、$A$の第$j$列を第$i$基本ベクトル$\bm{e}_i$でおきかえて得られる行列の行列式は次の式で与えられる。($\abs{\bm{a}_1, \cdots, \bm{a}_j, \cdots, \bm{a}_n}$の$\bm{a}_j$が$\bm{e}_i$に入れ替えらている。)

\begin{equation}
\abs{\bm{a}_1, \cdots, \bm{e}_i, \cdots, \bm{a}_n} = (-1)^{i+j}\abs{A^{ij}}
\label{defi:cofactor}
\end{equation}


この式の右辺を$A$の$(i,j)${\bf 余因子}(cofactor)という。

実際、余因子は次のように求められる。

$\bm{a}_1, \cdots, \bm{a}_n$の$i$行、$j$列を消し去り、第$j$列を基本ベクトル$\bm{e}_i$で置き換えた行列は以下のように表示できる。


$$
A^{(ij)} = \begin{pmatrix*}
a_{11} & \cdots & a_{1{j-1}} & 0 & a_{1{j+1}} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
a_{{i-1}1} & \cdots & a_{{i-1}{j-1}} & 0 & a_{{i-1}{j+1}} & \cdots & a_{{i-1}n} \\
0 & \cdots & 0 & 1 & 0 & \cdots & 0 \\
a_{{i+1}1} & \cdots & a_{{i+1}{j-1}} & 0 & a_{{i+1}{j+1}} & \cdots & a_{{i+1}n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
a_{n1} & \cdots & a_{n{j-1}} & 0 & a_{n{j+1}} & \cdots & a_{nn}
\end{pmatrix*}
$$


行列式の性質より、一組の列と列（あるいは行と行）を入れ替えると、符号が反転するところに着目すると、$A^{(ij)}$の第$j-1$列と第$j$列(つまり、$\bm{e}_i$)を入れ替えた行列$A^{(ij)'}$は

$$
A^{(ij)} = -A^{(ij)'}
$$

と表すことができる。この入れ替えを列に関して$j-1$回行うと

$$
\begin{pmatrix*}
0 & a_{11} & \cdots & a_{1{j-1}} & a_{1{j+1}} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & a_{{i-1}1} & \cdots & a_{{i-1}{j-1}} & a_{{i-1}{j+1}} & \cdots & a_{{i-1}n} \\
1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
0 & a_{{i+1}1} & \cdots & a_{{i+1}{j-1}} & a_{{i+1}{j+1}} & \cdots & a_{{i+1}n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & a_{n1} & \cdots & a_{n{j-1}} & a_{n{j+1}} & \cdots & a_{nn}
\end{pmatrix*}
$$

の行列が得られるから、この行列の行列式は

$$
(-1)^{j-1}\abs{\bm{e}_i, \bm{a}_1, \cdots, \bm{a}_{j-1}, \bm{a}_{j+1}, \cdots}
$$

の形になる。さらに、行に関して$i-1$回入れ替えを行うと最終的に

$$
\begin{pmatrix*}
1 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
0 & a_{11} & \cdots & a_{1{j-1}} & a_{1{j+1}} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & a_{{i-1}1} & \cdots & a_{{i-1}{j-1}} & a_{{i-1}{j+1}} & \cdots & a_{{i-1}n} \\
0 & a_{{i+1}1} & \cdots & a_{{i+1}{j-1}} &a_{{i+1}{j+1}} & \cdots & a_{{i+1}n} \\
\vdots & \vdots & \vdots & \vdots &\vdots & \vdots & \vdots \\
0 & a_{n1} & \cdots & a_{n{j-1}} &a_{n{j+1}} & \cdots & a_{nn}
\end{pmatrix*}
$$

の行列が得られる。この行列の行列式は

$$
(-1)^{(i-1)+(j-1)}\begin{vmatrix}
a_{11} & \cdots & a_{1{j-1}} & a_{1{j+1}} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
a_{{i-1}1} & \cdots & a_{{i-1}{j-1}} & a_{{i-1}{j+1}} & \cdots & a_{{i-1}n} \\
a_{{i+1}1} & \cdots & a_{{i+1}{j-1}} &a_{{i+1}{j+1}} & \cdots & a_{{i+1}n} \\
\vdots & \vdots & \vdots & \vdots &\vdots & \vdots & \vdots \\
a_{n1} & \cdots & a_{n{j-1}} &a_{n{j+1}} & \cdots & a_{nn}
\end{vmatrix}
$$

に等しい。よって、$(-1)^{(i-1)+(j-1)} = (-1)^{i+j}$から

$$
\abs{A^{(ij)}} = \abs{\bm{a}_1, \cdots, \bm{e}_i, \cdots, \bm{a}_n} = (-1)^{i+j}\abs{A^{(ij)}}
$$

が成立する。

\subsubsection{余因子展開}

この余因子を使ったn次行列式を求めるテクニックを紹介する。

$A = (a_{ij}) \in M_n(\mathbb{R})$の第$(i,j)$余因子を$\Delta_{ij} = (-1)^{i+j}\abs{A^{(ij)}}$とすれば、

第$j$列に関する展開を

$$
\abs{A} = \sum_{i=1}^{n} a_{ij}\Delta_{ij}
$$

第$i$行に関する展開を

$$
\abs{A} = \sum_{j=1}^{n} a_{ij}\Delta_{ij}
$$

とできる。

\begin{Proof*}
列に関する展開式を証明すれば十分である。$A$の列ベクトルを$\bm{a}_1, \cdots, \bm{a}_n$とすれば、

$$
\bm{a}_j = \sum_{i=1}^{n} a_{ij}\bm{e}_i
$$

であるから、行列式の線形性$D(c\bm{a}_1, \bm{a}_2) = cD(\bm{a}_1, \bm{a}_2)$により

$$
\abs{A} = D(\bm{a}_1, \cdots, \bm{a}_j, \cdots, \bm{a}_n) = \sum_{i=1}^{n} a_{ij}D(\bm{a}_1, \cdots, \bm{e}_i, \cdots, \bm{a}_n)
$$

となることより、

$$
\abs{A} = \sum_{i=1}^{n} a_{ij}\Delta_{ij}
$$

を得る。
\end{Proof*}

\exam{
$n = 3$のとき、第1列に関する余因子展開は次となる。

$$
\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{vmatrix} = a_{11}\begin{vmatrix}
a_{22} & a_{23} \\
a_{32} & a_{33}
\end{vmatrix} - a_{21}\begin{vmatrix}
a_{12} & a_{13} \\
a_{32} & a_{33}
\end{vmatrix} + a_{31}\begin{vmatrix}
a_{12} & a_{13} \\
a_{22} & a_{23}
\end{vmatrix}
$$
}

\rem{

$A, B \in M_n(\mathbb{R})$に対して

$$
\abs{AB} = \abs{A}\abs{B}.
$$

\begin{Proof*}
Bの列ベクトルを$\bm{b}_1, \cdots, \bm{b}_n$とすれば、
$$
\abs{AB} = D(A\bm{b}_1, \cdots, A\bm{b}_n)
$$

右辺を$F(\bm{b}_1, \cdots, \bm{b}_n)$とおけば、明らかに行列式の性質(D1), (D2)をもっている。


\begin{equation*}
\begin{split}
  F(\bm{b}_1, \cdots, \bm{b}_n)
  ={}&
  F(\bm{e}_1, \cdots, \bm{e}_n)D(\bm{b}_1, \cdots, \bm{b}_n)
  \\={}&
  F(\bm{e}_1, \cdots, \bm{e}_n)\abs{B}
\end{split}
\end{equation*}

$A$の列ベクトルを$\bm{a}_1, \cdots, \bm{a}_n$とすれば、$A\bm{e}_j = \bm{a}_j$であるから

$$
F(\bm{e}_1, \cdots, \bm{e}_n) = D(\bm{a}_1, \cdots, \bm{a}_n) = \abs{A}
$$
\end{Proof*}
}

\rem{
$A \in M_n(\mathbb{R})$が可逆であるためには、$\abs{A} \neq 0$が必要十分である。このとき、$\abs{A}^{-1} = \abs{A^{-1}}$となる。

\begin{Proof*}
$A$が可逆ならば

$$
AA^{-1} = E
$$

であるから、両辺の行列式をとり

$$
\abs{A}\abs{A^{-1}} = \abs{E} = 1
$$

よって$\abs{A} \neq 0, \quad \abs{A}^{-1} = \abs{A^{-1}}$
\end{Proof*}

}

\section{計量ベクトル空間}

線形空間では長さ、角、面積、体積といった計量を行うことは出来なかったが、ここに内積という構造を導入することでそのような計量を行える線形空間を{\bf 計量ベクトル空間}（内積空間）と呼ぶ。この章では、計量ベクトル空間における基本的な概念や性質について説明する。

\subsection{内積の性質}

$V = \vecSpace{n}$における内積は\ref{subsubsection:innerProduct}節において定義した。簡単に復習すると$\bm{x} = (x_i), \bm{y} = (y_i) \in \vecSpace{n}$の{\bf 内積}(inner product)は

$$
\innerProduct{x}{y} = \transposeVec{x}\bm{y} = \sum_{i = 1}^n x_iy_i
$$

であった。なお、$\vecSpace{n}$における内積を標準内積という。（$\mathbb{C}^n$などの空間にも内積は定義されることから、このように標準と呼称している。）

この定義より、内積は次の性質を持つ。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\innerProduct{x}{y}$は$\bm{x}, \bm{y}$について線形である。$\bm{x}$についていえば、任意の$\bm{x}, \bm{x}', \bm{y} \in V, c \in \mathbb{R}$に対して
\begin{eqnarray*}
\langle \bm{x} + \bm{x}', \bm{y} \rangle &= &\innerProduct{x}{y} + \innerProduct{x'}{y}, \\
\langle c\bm{x}, \bm{y} \rangle &= &c\innerProduct{x}{y}
\end{eqnarray*}
\item $\innerProduct{x}{y}$は$\bm{x}, \bm{y}$について対称である。
$$
\innerProduct{x}{y} = \innerProduct{y}{x}
$$
\item $\bm{x} \neq 0$ならば
$$
\innerProduct{x}{x} > 0
$$
実際、$\bm{x} = (x_i) \neq 0$ならば、
$$
\innerProduct{x}{x} = \sum_{i = 1}^n x_i^2 > 0
$$
\end{enumerate}

一般に、(1), (2)を満たす$V$上の2変数関数を$V \times V$上の対称双1次形式(symmetric bilinear form)という。特に(3)の性質を持つものを正値定符号であるという。（正定値性を持つという。）したがって、$\innerProduct{x}{y}$は正値定符号対称双1次形式である。

\subsubsection{ベクトルのノルム}

内積の正定値性より

$$
\|\bm{x}\| = \sqrt{\innerProduct{x}{x}} > 0
$$

が定義される。これをベクトル$\bm{x}$の{\bf 長さ}あるいは{\bf ノルム}という。ノルムの定義から明らかに

$$
\|\bm{x}\| = 0 \Longleftrightarrow \bm{x} = \bm{0}
$$

である。また任意の$\bm{x} \in V, c \in \mathbb{R}$に対して

$$
\|c\bm{x}\| = \abs{c}\|\bm{x}\|
$$

が成立する。また同様のベクトルの2乗は、内積を利用し
$$
\bm{x}^2  = \nrm{x}^2 = \innerProduct{x}{x}
$$

と表すことができる。

\exercise{次の等式を証明せよ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\innerProduct{x}{y} = \frac{1}{2}\{ \|\bm{x} + \bm{y}\|^2 - \|\bm{x}\|^2 + \|\bm{y}\|^2 \}$
\item $\|\bm{x} + \bm{y}\|^2 + \|\bm{x} - \bm{y}\|^2 = 2(\|\bm{x}\|^2 + \|\bm{y}\|^2)$
\end{enumerate}

{\bf 解答}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}

\item \begin{eqnarray*}
t = \|\bm{x} + \bm{y}\|^2 - \|\bm{x}\|^2 + \|\bm{y}\|^2 &= &\langle \bm{x} + \bm{y}, \bm{x} + \bm{y} \rangle - \innerProduct{x}{x} +\innerProduct{y}{y} \\
&= &\langle \bm{x}, \bm{x} + \bm{y} \rangle + \langle \bm{y}, \bm{x} + \bm{y} \rangle - \innerProduct{x}{x} +\innerProduct{y}{y} \\
&= &\innerProduct{x}{x} +\innerProduct{x}{y} + \innerProduct{y}{x} +\innerProduct{y}{y} - \innerProduct{x}{x} +\innerProduct{y}{y} \\
&= & 2 \innerProduct{x}{y}
\end{eqnarray*}
$$
\frac{1}{2} \cdot t = \frac{1}{2} \cdot 2\innerProduct{x}{y} = \innerProduct{x}{y}
$$

\item 
\begin{eqnarray*}
\|\bm{x} + \bm{y}\|^2 + \|\bm{x} - \bm{y}\|^2 &= &\langle \bm{x} + \bm{y}, \bm{x} + \bm{y} \rangle + \langle \bm{x} - \bm{y}, \bm{x} - \bm{y} \rangle \\
&= &\langle \bm{x}, \bm{x} + \bm{y} \rangle + \langle \bm{y}, \bm{x} + \bm{y} \rangle + \langle \bm{x}, \bm{x} - \bm{y} \rangle + \langle -\bm{y}, \bm{x} - \bm{y} \rangle \\
&= &\innerProduct{x}{x} + \innerProduct{x}{y} + \innerProduct{y}{x} + \innerProduct{y}{y} + \innerProduct{x}{x} + \innerProduct{x}{-y} + \innerProduct{x}{-y} + \innerProduct{-y}{x} + \innerProduct{-y}{-y} \\
&= &\innerProduct{x}{x} + \innerProduct{x}{y} + \innerProduct{y}{x} + \innerProduct{y}{y} + \innerProduct{x}{x} - \innerProduct{x}{y} - \innerProduct{x}{y} - \innerProduct{y}{x} + \innerProduct{y}{y} \\
&=& 2\innerProduct{x}{x} + 2 \innerProduct{y}{y} \\
&=& 2(\|\bm{x}\|^2 + \|\bm{y}\|^2)
\end{eqnarray*}

\end{enumerate}

\rem{
任意の$\bm{x}, \bm{y} \in \vecSpace{n}$に対して
$$
\abs{\innerProduct{x}{y}} \leq \nrm{x}\nrm{y} 
$$

シュバルツの不等式が成立する。

\begin{Proof*}
$\bm{x} = 0 \quad or \quad \bm{y} = 0$ならば、この不等式は$0 = 0$として成立する。したがって、$\bm{x}, \bm{y} \neq 0$とする。

$$
f(\lambda) = \|\bm{x} + \bm{y}\|^2 \quad (\lambda \in \mathbb{R})
$$

とおくと

\begin{eqnarray*}
f(\lambda) &= &\langle \lambda\bm{x} + \bm{y},\lambda\bm{x} + \bm{y} \rangle \\
&= &\lambda^2 \innerProduct{x}{x} + 2\lambda \innerProduct{x}{y} + \innerProduct{y}{y}
\end{eqnarray*}

と展開できる。よって、$f$は$\lambda$に関する2次式で、すべての$\lambda \in \mathbb{R}$に対して、$f(\lambda) \geq 0$であるためには、2次式の判別式($\frac{D}{4} = b^2 - ac$)が負にならなければならない。したがって、

$$
\innerProduct{x}{y}^2 - \innerProduct{x}{x}\innerProduct{y}{y} \leq 0
$$

すなわち

$$
\innerProduct{x}{y}^2 \leq \innerProduct{x}{x}\innerProduct{y}{y}
$$

ここで両辺とも$\geq 0$であるから、平方根をとると

$$
\abs{\innerProduct{x}{y}} \leq \nrm{x}\nrm{y} 
$$

が成立する。
\end{Proof*}
}\label{rem:schwarz}

\cor{
$\bm{x}, \bm{y} \in \vecSpace{n}$に対して三角不等式

$$
\nrm{x} - \nrm{y} \leq \|\bm{x} + \bm{y} \| \leq \nrm{x} + \nrm{y}
$$

が成立する。

\begin{Proof*}
定理\ref{rem:schwarz}の証明で$\lambda = 1$とおけばシュバルツの不等式により

\begin{eqnarray*}
f(1) &= &\|\bm{x} + \bm{y} \|^2 = \nrm{x}^2 + 2 \innerProduct{x}{y} + \nrm{y}^2 \\
& \leq &\nrm{x}^2 + 2\nrm{x}\nrm{y} + \nrm{y}^2 \\
& = &(\nrm{x}^2 + \nrm{y}^2)
\end{eqnarray*}
\end{Proof*}

両辺ともに$0$以上であるから、平方根をとって主張の三角不等式のうちの2つめの不等式を得る。
また、ひとつめの不等式は明らかに成立する。
}

この三角不等式の成立により計量ベクトル空間においてノルムが距離と呼べるようになる。

\subsubsection{ベクトルのなす角}

シュバルツの不等式の内積の絶対値を外して式を変形することで、$\bm{x}, \bm{y} \neq 0$のとき、

\begin{equation}
\label{cosThetaDefFormulaByVector}
-1 \leq \frac{\innerProduct{x}{y}}{\nrm{x}\nrm{y}} \leq 1
\end{equation}

が成立する。もうひとつの内積の定義

$$
\innerProduct{x}{y} = \nrm{x}\nrm{y}\cos\theta
$$

を変形すると

$$
\cos\theta = \frac{\innerProduct{x}{y}}{\nrm{x}\nrm{y}}
$$

を得るから、式(\ref{cosThetaDefFormulaByVector})は

$$
-1 \leq \cos\theta \leq 1
$$

に1対1で対応する。つまり、$0 \leq \theta \leq \pi$が一意的に定まる。この$\theta$を２つのベクトル$\bm{x}, \bm{y}$のなす角という。

\exam{
$n = 2$として、ベクトルのなす角$\theta$を求める。$\bm{x} = (x_i), \bm{y} = (y_i) \in \vecSpace{n}$, 

\begin{eqnarray*}
  \left\{
    \begin{array}{l}
      x_1 = r_1 \cos \theta_1 \\
      x_2 = r_1 \sin \theta_1
    \end{array}
  \right. \quad
  \left\{
    \begin{array}{l}
      y_1 = r_1 \cos \theta_2 \\
      y_2 = r_1 \sin \theta_2
    \end{array}
  \right.
\end{eqnarray*}
とおく。

\begin{eqnarray*}
\innerProduct{x}{y} &= &x_1y_1 + x_2y_2 \\
&= &r_1r_2(\cos\theta_1 \cos\theta_2 + \sin\theta_1 \sin\theta_2) \\
& =&r_1r_2\cos(\theta_2 - \theta_1) \\
\end{eqnarray*}
}

したがって
$$
\innerProduct{x}{y} = \nrm{x}\nrm{y}\cos(\theta_2 - \theta_1)
$$

であるから、ある整数$m$があって
$$
\theta = \abs{\theta_2 - \theta_1 + 2 \pi m} \leq \pi
$$
となる。よって、この$\theta$は幾何学的な意味での$\bm{x}, \bm{y}$のなす角に他ならない。

\exam{
２つのベクトルが張る平行四辺形の面積と行列式

$\bm{x} = (x_i), \bm{y} = (y_i) \in \vecSpace{2}$の２つのベクトルが張る平行四辺形の面積と$A = (\bm{a}, \bm{b})$の行列式($\det(A)$)の絶対値をとった値は一致することを示す。

ベクトル$\bm{x}, \bm{y}$が張る平行四辺形に$\bm{y}$の先端から$\bm{x}$の先端に対角線つまり、ベクトル$\bm{y} - \bm{x}$を引くと平行四辺形を2分する三角形ができる。

この三角形の面積$S_1$を

$$
S_1 = \frac{1}{2} \cdot \nrm{x}\nrm{y}\sin\theta
$$

とすると。高さは$\nrm{y}\sin\theta$であるから、平行四辺形の面積$S_2$は

$$
S_2 = \nrm{x}\nrm{y}\sin\theta
$$

この式より

\begin{eqnarray*}
\nrm{x}^2\nrm{y}^2\sin^2\theta &= &\innerProduct{x}{x}\innerProduct{y}{y}(1 - \cos^2 \theta) \\
&=&\innerProduct{x}{x}\innerProduct{y}{y} - \innerProduct{x}{x}\innerProduct{y}{y}\cos^2\theta
\end{eqnarray*}

が得られるから、両辺の平方根を取ると

\begin{eqnarray*}
\nrm{x}\nrm{y}\sin\theta &= &\nrm{x}\nrm{y} - \nrm{x}\nrm{y}\cos\theta \\
&= &\nrm{x}\nrm{y} - \innerProduct{x}{y}
\end{eqnarray*}

のような関係式が得られる。

これを展開すると

\begin{eqnarray*}
\nrm{x}\nrm{y} - \innerProduct{x}{y} &=& \sqrt{\innerProduct{x}{x}\innerProduct{y}{y} - \innerProduct{x}{y}^2} \\
&=& \sqrt{(x_1^2 + x_2^2)(y_1^2 + y_2^2) - (x_1y_1 + x_2y_2)^2} \\
&=& \sqrt{x_1^2y_1^2 + x_1^2y_2^2 + x_2^2y_1^2 + x_2^2y_2^2 - (x_1^2y_1^2 + x_1y_1x_2y_2 + x_2y_2x_1y_1 +  x_2^2y_2^2)} \\
&=&\sqrt{x_1^2y_2^2 + x_2^2y_1^2 - x_1y_1x_2y_2 + x_2y_2x_1y_1} \\
&=&\sqrt{(x_1y_2 - x_2y_2)^2} \\
&=&\abs{x_1y_2 - x_2y_1}
\end{eqnarray*}

より $S_2 = x_1y_2 - x_2y_1$である。

一方

$$
det(A) = \begin{vmatrix}
x_1 & y_1 \\
x_2 & y_2
\end{vmatrix} = x_1y_2 - x_2y_1
$$

であるから

$$
\nrm{x}\nrm{y} - \innerProduct{x}{y} = \det(A)
$$

である。
}

\exercise{
$$
\bm{x} = \begin{pmatrix}
\sqrt{3} \\
1
\end{pmatrix},\bm{y} = \begin{pmatrix}
1 \\
1
\end{pmatrix}
$$に対し、$\nrm{x}, \nrm{y}, \theta = \angle(\bm{x}, \bm{y})$を求めよ。
$\newline$

ノルムを求める。
$$
\nrm{x} = 2, \quad \nrm{y} = \sqrt{2}
$$

角を求める。
\begin{eqnarray*}
\angle(\bm{x}, \bm{y}) &=& \frac{\innerProduct{x}{y}}{\nrm{x}\nrm{y}} \\
&=&\frac{\sqrt{3} + 1}{2\sqrt{2}} \\
&=&\frac{1}{2} \cdot \frac{1}{\sqrt{2}} \frac{\sqrt{3}}{2} \cdot \frac{1}{\sqrt{2}} \\
&=&\cos\frac{\pi}{3} \cdot \cos\frac{\pi}{4} + \sin\frac{\pi}{3} \cdot \sin\frac{1}{4} \\
&=&\cos(\frac{\pi}{3} - \frac{\pi}{4})
\end{eqnarray*}
}

\subsubsection{ベクトルの直行条件}

核の定義から、特別な場合として、ベクトル$\bm{x}, \bm{y}  \neq 0$に対し
$$
\innerProduct{x}{y} = 0 \Longleftrightarrow \theta = \frac{\pi}{2}
$$
となるようなとき、$\bm{x}, \bm{y}$は{\bf 直行する}(orthogonal)という。逆に、シュバルツの不等式で統合が成立する場合、つまり
$$
\innerProduct{x}{y} = \pm\nrm{x}\nrm{y}
$$
となるためには、$\theta = 0 \hspace{5pt} or \hspace{5pt} \pi$であることが必要十分である。
この場合、２つのベクトルは平行となる。

$\newline$

この節で述べた定理やその系により、内積の定義された計量ベクトル空間では、様々な計量を定義することが可能になる。

\subsection{正規直交系}

ベクトル$\bm{e}_1, \cdots, \bm{e}_r$が

$$
\langle \bm{e}_i, \bm{e}_j \rangle = \delta_{ij} \quad (1 \leq i, j \leq r)
$$

を満たすとき、{\bf 正規直交系}(orthonormal system)であるという。この条件から、各$i$に対して
$\|\bm{e}_i\| = 1$. また任意の相違なる$i, j$に対して、$\langle \bm{e}_i, \bm{e}_j \rangle = 0$、すなわち$\bm{e}_i, \bm{e}_j$は直行する。


\exam{
$V = \vecSpace{n}$の基本ベクトル$\vecSet{e}$は標準内積に関して{\bf 正規直交基底}をなす。これまで$\vecSet{e}$は基本ベクトルを表してきたが、以降、一般の正規直交基底に関しても$\vecSet{e}$で表すことにする。（正規直交基底は基底と名がつく通り、それを構成するベクトルはすべて線形独立である。一方、直交でない通常の基底は線形独立ではあるが、内積は0とはならいので、「線形独立である $\Longleftrightarrow$ 内積が0」ではないことに注意したい。）
}

\lemm{
$\bm{e}_1, \cdots, \bm{e}_r$を正規直交系

\begin{eqnarray*}
\bm{x} = \sum_{i=1}^r x_i\bm{e}_i, \quad \bm{y} = \sum_{i=1}^r y_i\bm{e}_i
\end{eqnarray*} 
}\label{lemm:orthonormalSystem}

とすれば

$$
\innerProduct{x}{y} = \sum_{i=1}^r x_iy_i
$$

である。また

$$
\langle \bm{x}, \bm{e}_i \rangle = x_i
$$

\begin{Proof*}
これは計算過程により証明できる。

\begin{eqnarray*}
\innerProduct{x}{y} &= &\langle \sum_{i=1}^r x_i\bm{e}_i, \sum_{i=1}^r y_i\bm{e}_i \rangle \\
&= &\sum_{i,j=1}^r x_iy_i \langle \bm{e}_i, \bm{e}_j \rangle \\
&= &\sum_{i,j=1}^r x_iy_i\delta_{ij} = \sum_{i,j=1}^r x_iy_i
\end{eqnarray*}.

この計算過程より、$\bm{y} = \bm{e}_i$とすれば

$$
\langle \bm{x}, \bm{e}_i \rangle = x_i
$$

となる。（つまり、$\bm{x}$のi番目以外はすべて0になる）
\end{Proof*}


\subsubsection{ノルムが1のベクトルを作る方法}

$\bm{a} \in \vecSpace{n}$をノルムが1ではないベクトルとする。$\bm{a}$のノルムを1（つまり単位ベクトル）とするには

$$
\frac{\bm{a}}{\|\bm{a}\|}
$$

のように、ベクトルをそのベクトルのノルムで割ればよい。

\rem{
$V = \mathbb{R}^n$の任意の部分空間$W$は正規直交基底をもつ。

{\bf 証明}

$\dim W = r$とし、$\bm{a}_1, \cdots, \bm{a}_r$を$W$の任意の底とする。このとき、

$$
\langle \bm{e}_1, \cdots, \bm{e}_k \rangle = \langle \bm{a}_1, \cdots, \bm{a}_k \rangle \quad (1 \leq k \leq r)
$$
となるよう正規直交系$\langle \bm{e}_1, \cdots, \bm{e}_k \rangle$が取れる（作れる）ことを$k$に関する帰納法で示す。（$k = n$の場合に定理を与える）
}\label{rem:schmidtOrthonormalization}

$k = 1$のとき

\begin{equation}
\bm{e}_1 = \frac{\bm{a}_1}{\|\bm{a}_1\|}
\label{rem:subspaceShouldHaveOrthonormalBasis}
\end{equation}

とおけば、$\{\bm{e}_1\}$はただ一つのベクトルからなる正規直交系となる。

$k - 1$のとき式(\ref{rem:subspaceShouldHaveOrthonormalBasis})が成立するような$\bm{e}_1, \cdots, \bm{e}_{k-1}$が存在したとする。このとき、

$$
\bm{a}_k' = \sum_{1 \leq i \leq k -1} \langle \bm{e}_i, \bm{a}_k \rangle \bm{e}_i, \quad \bm{a}_k'' = \bm{a}_k - \bm{a}_k'
$$

とおけば、各$k$における式(\ref{rem:subspaceShouldHaveOrthonormalBasis})は

$$
\bm{e}_i = \frac{\bm{a}_k''}{\| \bm{a}_k'' \|} \quad (2 \leq i \leq k - 1)
$$
である。$\bm{a}_k'$は$\bm{e}_i$方向のベクトルより、(ベクトルどうしの引き算の定義から)$\bm{a}_k''$は$\bm{e}_i$に必ず直行する。つまり、

$$
\langle \bm{e}_i, \bm{a}_k'' \rangle = \langle \bm{e}_i, \bm{a}_k \rangle - \langle \bm{e}_i, \bm{a}_k' \rangle = 0 \quad (1 \leq i \leq k - 1)
$$

である。ここで、$\bm{a}_1, \cdots, \bm{a}_k$は線形独立であるから

$$
\bm{a}_k \notin \langle \bm{a}_1, \cdots, \bm{a}_{k-1} \rangle = \langle \bm{e}_1, \cdots, \bm{e}_{k-1} \rangle
$$

となるはずであるから、$\bm{a}_k'' \neq 0$. したがって、

$$
\bm{e}_k = \frac{\bm{a}_k''}{\|\bm{a}_k''\|}
$$

とおけば、

$$
\langle \bm{e}_i, \bm{e}_k \rangle = \delta_{ik} \quad (1 \leq i \leq k)
$$

が成立する。したがって、定理の主張が示せた。

また、このような正規直交基底の構成方法を{\bf（グラム）シュミットの正規直交化法 }という。
{\bf \newline}



\cor{
部分空間$W$の任意の正規直交基底は$V$の正規直交基底に延長される。

これは基底の延長を行い、シュミットの正規直交化法を利用することで証明できる。
}



\exercise{
$$
\bm{a}_1 = \begin{pmatrix}
1 \\
1 \\
0
\end{pmatrix},
\bm{a}_2 = \begin{pmatrix}
1 \\
0 \\
1
\end{pmatrix}, 
\bm{a}_3 = \begin{pmatrix}
0 \\
1 \\
1
\end{pmatrix}
$$
とする。

$$
\langle \bm{e}_1, \bm{e}_2, \bm{e}_3 \rangle = \langle \bm{a}_1, \bm{a}_2, \bm{a}_3 \rangle \quad (i = 1, 2, 3)
$$

となるような正規直交基底$\bm{e}_1, \bm{e}_2, \bm{e}_3$を求めよ。
}

シュミットの正規直交化法より

$$
\|\bm{a}_1\| = \sqrt{2}, \quad \bm{e}_1 = \begin{pmatrix}
\frac{1}{\sqrt{2}} \\[1.5ex]
\frac{1}{\sqrt{2}} \\[1.5ex]
0
\end{pmatrix}
$$

\begin{eqnarray*}
\bm{a}_2' &= &\langle \bm{e}_1, \bm{a}_2 \rangle \bm{e}_1 \\[1.5ex]
&=& \frac{1}{\sqrt{2}} \begin{pmatrix}
\frac{1}{\sqrt{2}} \\[1.5ex]
\frac{1}{\sqrt{2}} \\[1.5ex]
0
\end{pmatrix} = \begin{pmatrix}
\frac{1}{2}  \\[1.5ex]
\frac{1}{2}  \\[1.5ex]
0
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\bm{a}_2'' &= &\bm{a}_2 - \bm{a}_2' = \begin{pmatrix}
\frac{1}{2} \\[1.5ex]
-\frac{1}{2} \\[1.5ex]
1
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\bm{e}_2 &= &\frac{\bm{a}_2''}{\|\bm{a}_2''\|} \\
&=&\left(\frac{\sqrt{6}}{2}\right)^{-1} \cdot \begin{pmatrix}
\frac{1}{2} \\[1.5ex]
-\frac{1}{2} \\[1.5ex]
1
\end{pmatrix} \\
&=&\begin{pmatrix}
\frac{1}{\sqrt{6}} \\[1.5ex]
-\frac{1}{\sqrt{6}} \\[1.5ex]
\frac{2}{\sqrt{6}}
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
\bm{a}_3' &= &\langle \bm{e}_1, \bm{a}_3 \rangle\bm{e}_1 + \langle \bm{e}_2, \bm{a}_3 \rangle\bm{e}_2 \\
&= & \frac{1}{\sqrt{2}} \begin{pmatrix}
\frac{1}{\sqrt{2}} \\[1.5ex]
-\frac{1}{\sqrt{2}} \\[1.5ex]
0
\end{pmatrix} + \frac{1}{\sqrt{6}}\begin{pmatrix}
\frac{1}{\sqrt{6}} \\[1.5ex]
-\frac{1}{\sqrt{6}} \\[1.5ex]
\frac{2}{\sqrt{6}}
\end{pmatrix} = \begin{pmatrix}
\frac{2}{3} \\[1.5ex]
\frac{1}{3} \\[1.5ex]
\frac{1}{3}
\end{pmatrix} \\
\bm{a}_3'' &=& \bm{a}_3 - \bm{a}_3' \\
&=& \begin{pmatrix}
-\frac{2}{3} \\[1.5ex]
\frac{2}{3} \\[1.5ex]
\frac{2}{3}
\end{pmatrix} \\
\end{eqnarray*}

\begin{eqnarray*}
\bm{e}_3 &= &\frac{\bm{a}_3''}{\|\bm{a}_3''\|} = \begin{pmatrix}
-\frac{1}{\sqrt{3}} \\[1.5ex]
\frac{1}{\sqrt{3}} \\[1.5ex]
\frac{1}{\sqrt{3}}
\end{pmatrix}
\end{eqnarray*}

\subsection{直交行列}

一般に

$$
A \in M_{m,n}(\mathbb{R}), \bm{x} \in \vecSpace{n}, y \in \vecSpace{n}
$$

に対し

\begin{eqnarray*}
\langle \bm{x}, A\bm{y} \rangle &=& \transposeMat{\bm{x}}A\bm{y} = \transposeMat{\bm{x}}\transposeMat{\transposeMat{A}}\bm{y} \\
&=& \transposeMat{(\transposeMat{A}\bm{x})}\bm{y} = \langle \transposeMat{A}\bm{x
}, \bm{y} \rangle 
\end{eqnarray*}

が成立することに注意して、次の定理を見ていくことにする。

\rem{
$\bm{T} \in M_n(\mathbb{R})$に対して、次の４条件は同値である。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $T$は内積を不変にする。すなわち
$$
\langle T\bm{x}, T\bm{y} \rangle = \innerProduct{x}{y} \quad (\forall \bm{x}, \bm{y} \in \vecSpace{n})
$$
\item $T$はベクトルの長さを不変にする。すなわち
$$
\|T\bm{x}| = \nrm{x}
$$
\item $T$は{\bf 直交行列}(orthogonal matrix)である。すなわち
$$
\transposeMat{T}T = T\transposeMat{T} = E
$$

これをいいかえると、$T$は可逆で$T^{-1} = \transposeMat{T}$
\item $\{\bm{e}_i\}$を$V$の正規直交基底とすれば、$\{T\bm{e}_i\}$も正規直交基底である。
\end{enumerate}
}\label{rem:orthogonalMatrix}

\begin{Proof*}
(1) $\Rightarrow$ (2), (2) $\Rightarrow$ (3), (3) $\Rightarrow$ (4), (4) $\Rightarrow$ (1)の順で証明する。

{\bf (1) $\Rightarrow$ (2)}

$$
\langle T\bm{x}, T\bm{x} \rangle = \langle \bm{x}, \bm{x} \rangle.
$$

であるから、

$$
\|T\bm{x}\| = \sqrt{\langle T\bm{x}, T\bm{x} \rangle} = \sqrt{\langle \bm{x}, \bm{x} \rangle} =\|\bm{x}\|
$$

{\bf (2) $\Rightarrow$ (3)}

$T = (t_{ij}), \bm{x} = (x_i)$とおく。


\begin{eqnarray*}
(左辺) &=& \langle T\bm{x}, T\bm{x} \rangle = \sum_{i=1}^n \left(\sum_{j=1}^n t_{ij}x_j\right) \left(\sum_{k=1}^n t_{ik}x_k\right) \\
&=& \sum_{j,k=1}^n \left(\sum_{i=1}^n t_{ij}t_{ik}\right)x_jx_k \\
\end{eqnarray*}

\begin{eqnarray*}
(右辺) &=& \langle \bm{x}, \bm{x} \rangle = \sum_{j=1}^n x_j^2
\end{eqnarray*}

両辺の$x_jx_k$の係数を比べれば

$$
\sum_{i=1}^n t_{ij}t_{ik} = \delta_{jk}
$$

を得る. よって

$$
\transposeMat{T}T = E
$$

がいえ、$T$は可逆で$T^{-1} = \transposeMat{T}$.

{\bf (3) $\Rightarrow$ (4)}

(3)を仮定すれば

$$
\langle T\bm{e}_i, T\bm{e}_j \rangle = \langle \transposeMat{T}T\bm{e}_i, \bm{e}_j \rangle = \langle \bm{e}_i, \bm{e}_j \rangle = \delta_{ij}
$$.

よって、$\{T\bm{e}_i\}$も正規直交基底である.

{\bf (4) $\Rightarrow$ (1)}

$\{T\bm{e}_i\} = \{\bm{e}_i\}$を正規直交基底とし $\bm{x}, \bm{y} \in V$に対し、
$$
\bm{x} = \sum_{i=1}^n x_i\bm{e}_i, \quad \bm{y} = \sum_{i=1}^n y_i\bm{e}_i.
$$

とすれば、補題\ref{lemm:orthonormalSystem}を使い

\begin{eqnarray*}
\langle T\bm{x}, T\bm{y} \rangle &=& \langle \sum_{i=1}^n x_iT\bm{e}_i, \sum_{i=1}^n y_iT\bm{e}_i \rangle \\
&=& \sum_{i=1}^n x_iy_i = \innerProduct{x}{y}
\end{eqnarray*}
\end{Proof*}

$\Box$

直交行列$T$に対して

$$
\det T^2 = \det \transposeMat{T}T = \det E = 1
$$

であるから、$\det T = \pm1$.

なお、$n$次直交行列の全体の集合を$O(n)$で表す。また、$T \in O(n)$で$\det T = 1$であるようなもの全体の集合を$SO(n)$で表す。

\exercise{
$T \in M_n(\mathbb{R})$の各列ベクトルを$\bm{t}_1, \cdots, \bm{t}_n$とすれば、


$$
Tが直交行列 \Longleftrightarrow {t_i}が正規直交基底
$$
であることを示せ.
}

\begin{Proof*}
（方針: Tが直交行列であるとは、$\transposeMat{T}T = E$となることを示し、$\bm{t}_1, \cdots, \bm{t}_n$が正規直行基底となるには$\langle \bm{t}_i\bm{t}_j \rangle = \delta_{ij}$を示せば良い。）

$T = (t_1, \cdots, t_n)$を直交行列とするとき、$T \in O(n) \Longleftrightarrow \transposeMat{T}T = E \Longleftrightarrow \transposeMat{\bm{t}_i}\bm{t}_j = \langle \bm{t}_i, \bm{t}_j \rangle =  \delta_{ij} \Longleftrightarrow \bm{t}_1, \cdots, \bm{t}_nは正規直交基底$
\end{Proof*}

\exam{
次の行列はいずれも直交行列である

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item 
$
\begin{pmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix}
$
$\newline$

導出
$\newline$

$\begin{pmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix} \begin{pmatrix}
\cos\theta & \sin\theta \\
-\sin\theta & \cos\theta
\end{pmatrix} = \begin{pmatrix}
\cos^2\theta + \sin^2\theta & \cos\theta\sin\theta-\sin\theta\cos\theta \\
\sin\cos\theta - \cos\theta\sin\theta & \sin^2\theta + \cos^2\theta
\end{pmatrix} = E$

\item $\begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{pmatrix}$
$\newline$

導出
$\newline$

$\begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{pmatrix}\begin{pmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{pmatrix} = E
$
\end{enumerate}
}

\rem{
$\{\bm{e}_i\}, \{\bm{a}_i\}$を$\vecSpace{n}$の二つの基底、$T$を基底の変換$\{\bm{e}_i\} \rightarrow \{\bm{a}_i\}$の表現行列とする。$\{\bm{e}_i\}$が正規直交基底である時

$$
\{\bm{a}_i\}が正規直交基底 \Longleftrightarrow Tが直交行列
$$
であることを示せ.

\begin{Proof*}
$T = (t_{ij})$とおけば、$\bm{a}_i = \sum_{k=1}^n t_{ki}\bm{e}_i$であるから、

\begin{eqnarray*}
\langle \bm{a}_i, \bm{a}_j \rangle &=& \langle \sum_{k=1}^n t_{ki}\bm{e}_k, \sum_{l=1}^n t_{li}\bm{e}_l \rangle \\
&=& \sum_{k=1}^n t_{ki}t_{kj} = \delta_{ij}
\end{eqnarray*}
\end{Proof*}
}

よって、

$$
\langle \bm{a}_i, \bm{a}_j \rangle \Longleftrightarrow \transposeMat{T}T = E \Longleftrightarrow \transposeMat{t}_it_j = \delta_{ij}
$$.

$\newline$

この定理により、$T$を直交行列, $\{\bm{e}_i\}$を任意の正規直交基底とすれば、線形変換$f_{T}: \bm{x} \rightarrow T\bm{x}$の基底$\{\bm{e}_i\}$（基底を取り替えた）に関する新座標系での表現行列を$T'$としてもこれも同様に直交行列である。この仮定のもとに、$\{\bm{e}_i\}$, $\{T\bm{e}_i\}$はともに正規直交基底であり、$T'$は基底の変換$\{\bm{e}_i\} \rightarrow \{T\bm{e}_i\}$の表現行列に他ならない。実際、基底の変換行列を$T_1 = (\bm{e}_1, \cdots, \bm{e}_n)$とおけば、$T' = T_1^{-1}TT_1$である。このように、直交行列$T$に対応する線形変換$f_T$を直交変換という。$f_T$は内積を不変にする線形変換として特徴づけられる。

\subsection{直交補空間}

$V = \vecSpace{n}$の部分空間$W$に対し

$$
W^\bot = \{\bm{x} \in V | \innerProduct{x}{y} = 0 \hspace{3pt} for \hspace{3pt} \forall \bm{y} \in W \}
$$

は明らかに$V$の部分空間になる。これを$W$の{\bf 直交補空間}(orgothonal complement)という。

\rem{
$V = \vecSpace{n}$の部分空間$W$の直交補空間を$W^\bot$とすれば、
$$
V = W \oplus W^\bot
$$

である。（$\oplus$は直和を表す論理記号で $W_1, W_2 \subset V$に対して、$W_1 \cap W_2 = \{0\}, W_1 \cup W_2 = V$となるもの）

また

$$
\dim W^\bot = n - \dim W
$$

である。

\begin{Proof*}
$W$の正規直交基底$\{\bm{e}_1, \cdots, \bm{e}_r\} (r = \dim W)$をとり、それを$V$の正規直交基底$\{\bm{e}_1, \cdots, \bm{e}_n\}$に延長する。そのとき、$W = \langle \bm{e}_1, \cdots, \bm{e}_r \rangle$であるから、任意の

$$
\bm{x} \in V, \bm{x} = \sum_{1 \leq i \leq n} x_i\bm{e}_i
$$
\end{Proof*}
に対して、補題\ref{lemm:orthonormalSystem}より

\begin{eqnarray*}
\innerProduct{x}{y} = 0 \quad (\forall \bm{y} \in W) &\Longleftrightarrow& \langle \bm{x}, \bm{e}_i \rangle = 0 \quad (1 \leq i \leq r) \\
&\Longleftrightarrow& x_i = 0 \quad (1 \leq i \leq r) \\
&\Longleftrightarrow& \bm{x} \in \langle \bm{e}_{r+1}, \cdots, \bm{e}_n \rangle
\end{eqnarray*}

よって、

$$
W^\bot = \langle \bm{e}_{r + 1}, \cdots, \bm{e}_n \rangle, \quad \dim W^\bot = n - r
$$

である。よって

$$
V = \langle \bm{e}_1, \cdots, \bm{e}_r \rangle \oplus \langle \bm{e}_{r+1}, \cdots, \bm{e}_n \rangle
$$

は明らか。
}\label{rem:orthogonalComplementOplus}

\subsubsection{正射影}

$\{\bm{e}_1, \cdots \bm{e}_r, \}$を部分空間$W$の正規直交基底とし、$\bm{x} \in W$に対し

$$
\bm{x}' = \sum_{i=1}^r \langle \bm{e}_i, \bm{x} \rangle\bm{e}_i, \quad \bm{x}'' = \bm{x} - \bm{x}'
$$

とおけば、

$$
\bm{x} = \bm{x}' + \bm{x}'',\quad \bm{x}' \in W, \bm{x}'' \in W^\bot
$$

である。このとき写像$p: \bm{x} \mapsto \bm{x}'$を$W$への{\bf 正射影}(orthogonal projection)という。（$\bm{x}'' \in W^\bot$となるのは、（シュミットの直交化を思い出すと）$ベクトルの引き算\bm{x} - \bm{x}'$が$\bm{x'}$に直交するベクトルを求める行為に他ならないからである。）

$p$は$V$の線形変換で、$W$によって一意的に定まる。$\{\bm{e}_1, \cdots, \bm{e}_r\}$を$V$の正規直交基底$\{\bm{e}_1, \cdots, \bm{e}_n\}$に延長すれば、この基底に関する$p$の行列は

$$
\begin{pmatrix}
E_r & 0 \\
0 & 0
\end{pmatrix}, \quad r = \rank p
$$

である。

\cor{
$W$を$V = \vecSpace{n}$の部分空間とすれば

$$
W^{\bot\bot} = W
$$

\begin{Proof*}
$W \subset W^{\bot\bot}$は定義から明らか。$\dim W^\bot = n - \dim W$を使えば
$$
\dim W^{\bot\bot} = n - \dim W^\bot = n - (n - \dim W) = \dim W
$$
であるから、定理の主張が成立する。
\end{Proof*}
}

\rem{
$W_1, W_2$を$V = \bm{R}^n$の部分空間とすれば

\begin{eqnarray}
\label{expression6-1}
(W_1 + W_2)^\bot &=& W_1^\bot \cap W_2^\bot, \\
\label{expression6-2}
(W_1 \cap W_2)^\bot &=& W_1^\bot + W_2^\bot
\end{eqnarray}
\begin{Proof*}

式(\ref{expression6-1})を示す。$\bm{x} \in (W_1 + W_2)^\bot$とする。

任意の$\bm{y} \in W_1 + W_2$に対して$\innerProduct{x}{y} = 0$

$\hspace{5pt} \Longleftrightarrow$ ある$\bm{w}_1 \in W_1, \bm{w}_2 \in W_2$に対し、$\bm{y} = \bm{w}_1 + \bm{w}_2$とおけるから、$\langle \bm{x}, \bm{w}_1 + \bm{w}_2 \rangle = \langle \bm{x}, \bm{w}_1 \rangle + \langle \bm{x}, \bm{w}_2 \rangle = 0$

$\hspace{5pt} \Longleftrightarrow \langle \bm{x}, \bm{w}_1 \rangle = \langle \bm{x}, \bm{w}_2 \rangle = 0$

$\hspace{5pt} \Longleftrightarrow$ 任意の$\bm{w}_1 \in W_1, \bm{w}_2 \in W_2$に対し、$\langle \bm{x}, \bm{w}_1 \rangle = \langle \bm{x}, \bm{w}_2 \rangle = 0$

$\hspace{5pt} \Longleftrightarrow$ $\bm{x} \in W_1^\bot \cap W_2^\bot$

つまり、$(W_1 + W_2)^\bot = W_1^\bot \cap W_2^\bot = \{0\}$
\newline

式(\ref{expression6-2})を示す。$\bm{x} \in (W_1 \cap W_2)^\bot$とする。

任意の$\bm{y} = W_1 \cap W_2$に対して、$\langle \bm{x}, \bm{y} \rangle = 0$

$\hspace{5pt} \Longleftrightarrow$ ある$\bm{w}_1 \in W_1, \bm{w}_2 \in W_2$に対し、$\langle \bm{x}, \bm{w}_1 \rangle = \langle \bm{x}, \bm{w}_2 \rangle = 0$. 

$\hspace{5pt} \Longleftrightarrow$ $\langle \bm{x}, \bm{w}_1 \rangle + \langle \bm{x}, \bm{w}_2 \rangle = \langle \bm{x}, \bm{w}_1 + \bm{w}_2  \rangle = \langle \bm{x}, \bm{y} \rangle = 0$

$\hspace{5pt} \Longleftrightarrow$ 任意の$\bm{w}_1 \in W_1, \bm{w}_2 \in W_2$に対し、$\langle \bm{x}, \bm{w}_1 \rangle + \langle \bm{x}, \bm{w}_2 \rangle = 0$

$\hspace{5pt} \Longleftrightarrow \bm{x} \in W_1^\bot + W_2^\bot$ 

つまり、$(W_1 \cap W_2)^\bot = W_1^\bot + W_2^\bot = {0}$
\end{Proof*}
}

\section{行列の標準化}

\subsection{不変部分空間}

$V = \vecSpace{n}, A = (a_{ij}) \in M_n(\mathbb{R})$とし、$A$によって定義される$V$の線形変換

$$
f = f_A: \bm{x} \mapsto A\bm{x}
$$

があるとする。$A$は$f$の標準基底$\{\bm{e}_i\}$に関する表現行列であるが、$V$の他の基底$\{\bm{v}_i\}$をとり

\begin{eqnarray}
\label{expression7-1}
(\bm{v}_1, \cdots, \bm{v}_n) = (\bm{e}_1, \cdots, \bm{e}_n)P, \quad P \in GL_n(\mathbb{R})
\end{eqnarray}

とすれば（Pは基底の変換行列）、基底$\{\bm{v}_i\}$に関する$f$の表現行列は$A' = P^{-1}AP$である。式(\ref{expression7-1})を使えば
\begin{eqnarray*}
(f(\bm{v}_1), \cdots, f(\bm{v}_n)) &=& (f(\bm{e}_1), \cdots, f(\bm{e}_n))P \\
&=& (\bm{e}_1, \cdots, \bm{e}_n)AP \\
&=& (\bm{v}_1, \cdots, \bm{v}_n)P^{-1}AP
\end{eqnarray*}.

このように、一般にある$P \in GL_n(\mathbb{R})$があって、$A' = P^{-1}AP$となるとき、$A$と$A'$とは{\bf 相似}(similar)または共益(conjugate)であるという。相似な行列は同様の線形変換を別の基底によって表す行列と考えられる。線形代数の一つの基本的な利用用途として、与えられた行列（線形変換）に対し、適当な基底をとり、対応する行列をできるだけ簡単な形にすることである。これを行列の{\bf 標準化}という。
この章では行列の標準化に関連した基本的な定義や概念に関して説明する。

まず、部分空間$W \subset V$は$AW \subset W$であるとき、$A-{\bf 不変}(A-invariant)$, または単に不変な部分空間であるという。

\rem{
$W$を$A-不変$な部分空間とし、$V$の基底$\{\bm{v}_1, \cdots, \bm{v}_n\}$を$\{\bm{v}_1, \cdots, \bm{v}_r\}$が$W$の基底になるようにとる。そのとき基底$\{\bm{v}_i\}$に関して$f_A$の表現行列は

$$
A' = \begin{pmatrix}
A_1' & A_{12}' \\
0 & A_2'
\end{pmatrix}
$$
の形になる。ここで、$A_1$'は$f_A$が引き起こす線形変換の基底$\bm{v}_1, \cdots, \bm{v}_r$に関する表現行列である。

\begin{Proof*}
$A'=(a'_{ij})$とする。$W$が$A-不変$より
$$
(f_A(v_1), \cdots, f_A(v_r)) = (v_1, \cdots, v_r)A'_1
$$

とできるから、基底を延長すれば

$$
(f_A(v_1), \cdots, f_A(v_n)) = (v_1, \cdots, v_r, v_{r+1}, v_n) \begin{pmatrix}
A'_1 \\
0
\end{pmatrix}
$$

が得られる。つまり、$r + 1 \leq i \leq n, \quad 1 \leq j \leq r$において、$a'_{ij} = 0$である。また、$(f_A(v_{r+1}), \cdots, f_A(v_n))$においても

$$
(f_A(v_{r+1}), \cdots, f_A(v_n)) = (v_1, \cdots, v_r, v_{r+1}, \cdots, v_n) \begin{pmatrix}
A'_{12} \\
A'_2
\end{pmatrix}
$$

が成り立つ。よって、$\begin{pmatrix}
A'_1 \\
0
\end{pmatrix}, \begin{pmatrix}
A'_{12} \\
A'_2
\end{pmatrix}$を並べれば$A'$の形になる。
\end{Proof*}
}\label{rem:shapeOfAinvariantRepresentationMatrix}

この証明より、$A'_1$は$f_A$が引き起こす線形変換の基底$\bm{v}_1, \cdots, \bm{v}_r$に関する行列であり、$\rank A'_1 = r$であることが分かる。

\rem{
$V$が

$$
V = W_1 \oplus W2, \quad W_i: A-invariant
$$
の形に直和分解されるとする。そのとき、$V$の基底$\{\bm{v}_i\}$を$\{\bm{v}_1, \cdots, \bm{v}_r\}, \{\bm{v}_r+1, \cdots, \bm{v}_n\}$がそれぞれ$W_1, W_2$の基底になるようにとれば、対応する$f_A$の行列は

$$
A' = \begin{pmatrix}
A_1' & 0 \\
0 & A'_2
\end{pmatrix}
$$

$A_1', A_2'$は$f_A$が$W_1, W_2$に引き起こす線形変換の行列である。

\begin{Proof*}
\begin{eqnarray}
\label{equation7-2-1}
(f_A(\bm{v}_1), \cdots, f_A(\bm{v}_r)) &=& (\bm{v}_1, \cdots, \bm{v}_r)A'_1 \\
\label{equation7-2-2}
(f_A(\bm{v}_{r + 1}), \cdots, f_A(\bm{v}_n)) &=& (\bm{v}_{r + 1}, \cdots, \bm{v}_n)A'_2
\end{eqnarray}
とおけば、$W_1, W_2$は$A-$不変であるから$(\ref{equation7-2-1}) \subset W_1, (\ref{equation7-2-2}) \subset W_2$でなければならない。よって、

$$
(\bm{v}_1, \cdots, \bm{v}_r, \bm{v}_{r + 1}, \cdots, \bm{v}_n)\begin{pmatrix}
A'_1 \\
0
\end{pmatrix} \subset W_1
$$

$$
(\bm{v}_1, \cdots, \bm{v}_r, \bm{v}_{r + 1}, \cdots, \bm{v}_n)\begin{pmatrix}
0 \\
A'_2
\end{pmatrix} \subset W_2
$$
より、$f_A$の行列は上記の形になる。
\end{Proof*}
}

これらの定理により、$V$を不変部分空間の直和に分解できればそれに対応して行列$A$が簡約された形になることがわかる。

\subsection{固有値、固有空間}

固有値、固有空間に入る前にまずは特性多項式、消去法の原理に関して概説する。

\subsubsection{特性多項式}
$A \in M_n(\mathbb{R})$に対し

$$
\abs{xE - A} = \begin{vmatrix}
x - a_{11} & -a_{12} & \cdots & - a_{1n} \\
x - a_{21} & -a_{22} & \cdots & - a_{2n} \\
\vdots     & \vdots  & \ddots & \vdots \\
x - a_{n1} & -a_{n2} & \cdots & - a_{nn}
\end{vmatrix}
$$

は$x$に関するn次多項式になる。これを$A$の{\bf 特性多項式}(characteristic polyynominal)あるいは{\bf 固有方程式}といい、$f(x;A)$のように書くことにする。これが多項式になることを確認するにはその行列式を展開すれば良い。例えば

$$
A = \begin{pmatrix}
a_{11} & a_{12} & a_{13} \\ 
a_{21} & a_{22} & a_{23} \\ 
a_{31} & a_{32} & a_{33}
\end{pmatrix}
$$としたとき、

\begin{equation}
\begin{split}
    f(x;A) 
    ={}&
    \begin{vmatrix}
    x - a_{11} & - a_{12} & -a_{13} \\ 
    -a_{21} & x - a_{22} & -a_{23} \\ 
    -a_{31} & -a_{32} & x - a_{33}
    \end{vmatrix} \\
    \\={}&
    (x - a_{11})(x - a_{22})(x - a_{33}) + (-a_{12} \cdot -a_{23} \cdot -a_{31})
    \\&+
    (-a_{13} \cdot -a_{21} \cdot x - a_{32}) - (-a_{12} \cdot -a_{21} \cdot x - a_{33})
    \\&-(x - a_{11} \cdot  -a_{23} \cdot -a_{32}) - (- a_{13} \cdot x - a_{22} \cdot - a_{31})
    \\={}&x^3 - x^2(a_{33} + a_{22} + a_{11}) + x(a_{22}a_{33} + a_{11}a_{22}) - a_{11}a_{22}a_{33}, \cdots
\end{split}
\end{equation}

のような3次の多項式の形になる。これを一般化すると

$$
f(x;A) = a_0x^n + a_1x^{n-1} + \cdots + a_n
$$

のようなn次多項式になる。このとき、この多項式の次数は行列式を求める際にxがある対角成分どうしが掛けられるのがただ一回のみであることから分かるように、行列の$\rank$と一致する。

\subsubsection{消去法の原理}

$A \in M_n(\mathbb{R})$に対し、１次方程式

$$
A\bm{x} = 0
$$
が自明でない解$\bm{x} \neq 0$をもつためには、$\abs{A} = 0$であることが必要十分である。
\begin{Proof*}
$\abs{A} \neq 0$ならば、$A$は可逆であるから、$\bm{x} = 0$が唯一の解である。したがって、自明でない解は存在しない。逆に$\abs{A} = 0$ならば、次元定理により、解の空間は次元が$n - \rank A > 0$の部分空間である。よって自明でない解が確かに存在する。
\end{Proof*}
 

\defi{固有値、固有空間}

まず一番簡単な例として、1次元部分空間$W = \langle \bm{v_1} \rangle$が$A-不変$になっているとする。定理\ref{rem:shapeOfAinvariantRepresentationMatrix}によれば、$\{\bm{v}_1\}$を延長して得られる基底$\{\bm{v}_1, \cdots, \bm{v}_n\}$に関する$f_A$の行列は

$$
A' = \begin{pmatrix}
\alpha_1 & \ast \\
0 & A_2'
\end{pmatrix}
$$
の形になる。特に

$$
A\bm{v}_1 = \alpha_1\bm{v}_1
$$
が成立する。一般に$\bm{v}_1 \neq 0$に対して$A\bm{v}_1 = \alpha_1\bm{v}_1$が成立する時、$\alpha_1$を$A$あるいは$f_A$の{\bf 固有値}(eigen value), $\bm{v}_1$をそれに属する{\bf 固有ベクトル}(eigen vector)という。

また
$$
V(A;\alpha_1) = \{\bm{v} \in V | A\bm{v} = \alpha_1\bm{v}\}
$$
とけば、これは明らかに$V$の部分空間になる。$V(A;\alpha_1)$を$\alpha_1$に対応する{\bf 固有空間}(eigen space)という。

\rem{
$\alpha \in \mathbb{R}$が$A \in M_n(\mathbb{R})$の固有値になるためには、$\alpha$が特性多項式$f(x;A)$の根になることが必要十分である。（根は特性方程式における解のこと）

\begin{Proof*}
定義によれば$\alpha$が$A$の固有値になるためには、一次方程式

$$
(A - \alpha E)\bm{x} = 0
$$

が自明でない解($\bm{x} \neq 0$)を持つことが必要十分である。よって、消去法の原理により

$$
\alpha: eigen \hspace{5pt} value \hspace{5pt} of \hspace{5pt} A \Longleftrightarrow \abs{A -\alpha E} = 0
$$
である。
\end{Proof*}
これを言い換えると次の定理を得る。
}

\rem{
$\alpha$に対応する固有空間$V(A;\alpha)$は一次方程式$(A - \alpha E)\bm{x} = 0$の解空間となりその次元は

$$
\dim V(A;\alpha) = n - \rank(A - \alpha E)
$$
である。
}

複素数の範囲ではn次方程式$f(x;A) = 0$は重解まで考慮すると、n個の根を持つ。したがって、$A$を$\mathbb{C}^n$の線形変換と考えれば、これも重解を考慮して$n$個の固有値を持つ。したがって、固有値について考えるときには$\vecSpace{n}$ではなく、$\mathbb{C}^n$で考えるほうがより自然である。しかし、もし固有値がすべて実数ならば、$\vecSpace{n}$で考えても十分である。（これは代数学の基本定理により証明されている。）

\rem{
$\alpha_1, \cdots, \alpha_r$を$A$の相違なる固有値$\bm{v}_1, \cdots, \bm{v}_r$をそれぞれに属する固有ベクトルとすれば$\bm{v}_1, \cdots, \bm{v}_r$は線形独立である。

\begin{Proof*}
数学的帰納法で示す。$r = 1$の時は明らか。($c_1\bm{v}_1 = 0$となるのは、$c_1 = 0$であるから)

$r - 1$の時
\begin{equation}
\label{rem:matrixNormalization-1}
c_1\bm{v}_1 + \cdots + c_{r-1}\bm{v}_{r-1} + c_r\bm{v}_r = 0, \quad c_i = 0
\end{equation}
と仮定する。式(\ref{rem:matrixNormalization-1})に左から$A$を掛けて

\begin{equation}
\label{rem:matrixNormalization-2}
c_1A\bm{v}_1 + \cdots + c_{r-1}A\bm{v}_{r-1} + c_rA\bm{v}_1 = c_1\alpha_1\bm{v}_1 + \cdots + c_{r-1}\alpha_{r-1}\bm{v}_{r-1} + c_r\alpha_r\bm{v}_1 = 0
\end{equation}

さらに式(\ref{rem:matrixNormalization-1})に$a_r$を掛けると

\begin{equation}
\label{rem:matrixNormalization-3}
c_1\alpha_r\bm{v}_1 + \cdots + c_{r-1}\alpha_r\bm{v}_{r-1} + c_r\alpha_r\bm{v}_1 = 0
\end{equation}

ここで

\begin{equation*}
\begin{split}
    (\ref{rem:matrixNormalization-2}) - (\ref{rem:matrixNormalization-3})
    ={}&
    c_1(\alpha_1 - \alpha_r)\bm{v}_1 + \cdots c_{r-1}(\alpha_{r-1} - \alpha_r)\bm{v}_{r-1} + c_r(\alpha_r - \alpha_r)\bm{v}_r
    \\={}&
    c_1(\alpha_1 - \alpha_r)\bm{v}_1 + \cdots + c_{r-1}(\alpha_{r-1} - \alpha_r)\bm{v}_{r-1}
    \\={}&0
\end{split}
\end{equation*}

とおくと、仮定より相違なる固有値に属する$r - 1$個のベクトル$\bm{v}_1, \cdots, \bm{v}_{r-1}$は線型独立より係数$c_i$が$0$になる必要がある。すなわち

$$
c_1(\alpha_1 - \alpha_r) + \cdots + c_{r-1}(\alpha_{r-1} - \alpha_r) = 0
$$

しかし、固有値はすべて異なるという仮定より

$$
c_1 = \cdots = c_{r-1} = 0
$$

である。これを式(\ref{rem:matrixNormalization-1})に代入すれば$c_r = 0$が得られる。よて、$r = 1, r - 1$で成り立てば、$r$でも同様に成り立つ。したがって、$A$の相違なる固有値に属する固有ベクトル$\bm{v}_1, \cdots, \bm{v}_r$は線型独立である。

この結果より（帰納法を$r = 1, 2, 3, \cdots$のように進めていけば）

$$
V = \sum_{i=1}^{r} V(A;\alpha_i) = V(A;\alpha_1) \oplus \cdots \oplus V(A;\alpha_r) \quad (直和)
$$

であることが分かる。
\end{Proof*}
}

\exam{
$$
A = \begin{pmatrix}
6 & -3 & -7 \\
-1 & 2 & 1 \\
5 & -3 & -6
\end{pmatrix}
$$の固有値、固有ベクトルは以下の手順で求まる。

Aの特性多項式

\begin{eqnarray*}
\abs{\lambda E - A} &=& \begin{vmatrix}
\lambda - 6 & 3 & 7 \\
1 & \lambda -2 & -1 \\
-5 & 3 & \lambda + 6
\end{vmatrix} = 0
\end{eqnarray*}

を解くと

$$
\lambda^3 - 2\lambda^2 -\lambda + 2 = (\lambda - 1)(\lambda -2 )(\lambda + 1) = 0
$$

より、固有値は$\lambda = 1, 2, -1$である。以降、各固有ベクトルを求めるには、各固有値に関して

$$
(\lambda E - A)\begin{pmatrix}
x \\
y \\
z
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
$$

の連立方程式を解けば良い。（一般的には$(\lambda E - A)$に基本変形を施して、行列を単純な形にすると連立方程式が簡単に解けるようになる。）

$\lambda = 1$のとき

$$
\begin{pmatrix}
-5 & 3 & 7 \\
1 & -1 & -1 \\
-5 & 3 & 7
\end{pmatrix}\begin{pmatrix}
x \\
y \\
z
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
$$

を解くと、$y = 1, z = 1, x = 2$より、$z = \alpha_1$とすれば

$$
\bm{x}_1 = \alpha_1 \begin{pmatrix}
2 \\
1 \\
1
\end{pmatrix}
$$.

したがって、固有空間は$\langle \begin{pmatrix}
2 \\
1 \\
1
\end{pmatrix} \rangle$ である。これより、固有値$0$に属する固有ベクトルは$\begin{pmatrix}
2 \\
1 \\
1
\end{pmatrix}$である。

$\lambda = 2$のとき

$$
\begin{pmatrix}
-4 & 3 & 7 \\
1 & 0 & -1 \\
-5 & 3 & 8
\end{pmatrix}\begin{pmatrix}
x \\
y \\
z
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
$$を解くと、$x = z$これを使って$-4x + 3y + 7x = 3x + 3y = 0$を解けば、$y = -x$である。

$x = \alpha_2$とすれば

$$
\bm{x}_2 = \alpha_2 \begin{pmatrix}
1　\\
-1 \\
1
\end{pmatrix}
$$

であるから、固有値$2$に属する固有ベクトルは
$$\begin{pmatrix}
1　\\
-1 \\
1
\end{pmatrix}$$

$\lambda = -1$のとき

$$
\begin{pmatrix}
-7 & 3 & 7 \\
1 & -3 & -1 \\
-5 & 3 & 5
\end{pmatrix}\begin{pmatrix}
x \\
y \\
z
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
$$

これは、連立方程式が複雑なので簡単のため基本変形を施すと

$$
\begin{pmatrix}
1 & 0 & -1 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{pmatrix}\begin{pmatrix}
x \\
y \\
z
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
$$

であるから、$y = 0, x = z$より$x = \alpha_3$とおけば

$$
\bm{x}_3 = \alpha_3 \begin{pmatrix}
1 \\
0 \\
1
\end{pmatrix}
$$
である。よって、固有値$-1$に属する固有ベクトルは$$
\begin{pmatrix}
1 \\
0 \\
1
\end{pmatrix}.
$$

なお、$\bm{x}_1, \bm{x}_2, \bm{x}_3$は線形独立である。
}

\subsection{実対称行列の対角化}

$A \in M_n(\mathbb{R})$を対称とする。そのとき、$\langle \bm{x}, A\bm{y} \rangle = \langle {\transposeMat{A}}\bm{x}, \bm{y} \rangle$により任意の$\bm{x}, \bm{y} \in \vecSpace{n}$に対して

\begin{equation}
\label{symmetricMatrixEquation-1}
\langle A\bm{x}, \bm{y} \rangle = \langle \bm{x}, A\bm{y} \rangle = 0.
\end{equation}
が成立する。

\defi{実対称行列}

$A \in M_n(\mathbb{R})$が実対称行列であるとは、$A$が

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\transposeMat{A} = A$ \ (対称性)
\item $\bar{A} = A$ \ (複素共役)
\end{enumerate}

を満たすときを言う。特に(1)を満たすものを対称行列、(2)を満たすものを実行列という。

\lemm{
実対称行列$A$の固有値は実数である。
\begin{Proof*}
複素数の範囲で$A$の固有値$\alpha$をとり、$\bm{v} \neq 0$をそれに属する一つの固有ベクトルとする。

$A = (a_{ij}), \bm{v} = (v_i)$とし、$\bar{\bm{v}} = (v_i)$とすれば、実対称行列の定義より$A = \transposeMat{\bar{A}}$となるから 

\begin{eqnarray*}
\transposeMat{\bm{v}}\bar{A}\bm{v} &=& \transposeMat{\bar{\bm{v}}}(A\bm{v}) = \transposeMat{\bar{\bm{v}}}(\alpha\bm{v}) \\
&=& \alpha \transposeMat{\bar{\bm{v}}}\bm{v} = \transposeMat{\bar{\bm{v}}} \transposeMat{\bar{A}}\bm{v} = \transposeMat{(\bar{A}\bar{\bm{v}})}\bm{v} \\
&=& \transposeMat{(\alpha \bar{\bm{v}})}\bm{v} = \bar{\alpha} \transposeMat{\bar{\bm{v}}}\bm{v}
\end{eqnarray*}
よって
$$
\alpha - \bar{\alpha}\transposeMat{\bm{v}}\bm{v} = 0
$$

ここで、
$$
\transposeMat{\bar{\bm{v}}}\bm{v} = \sum_{i = 1}^n \bar{v}_iv_i = \sum_{i=1}^n \abs{v_i}^2 > 0
$$

であるから、$\alpha - \bar{\alpha} = 0$. よって、$\alpha = \bar{\alpha}$. つまり$\alpha$は実数である。
\end{Proof*}
}

\lemm{
$A$を実対称行列とする。$W \in V$が$A-不変$ならば、$W^\bot$も$A-不変である$。

\begin{Proof*}
$\bm{x} \in W, y \in W^\bot$を任意に取ってくる。$\langle \bm{x}, A\bm{y} \rangle = 0$を言えば十分である。$W$は$A-不変$であるから、$A\bm{x} \in W$.よって、式(\ref{symmetricMatrixEquation-1})により
$$
\langle A\bm{x}, \bm{y} \rangle = \langle \bm{x}, A\bm{y} \rangle = 0.
$$
\end{Proof*}
}

\rem{
実対称行列$A \in M_n(\mathbb{R})$に対して、適当な直交行列$T \in O(n)$をとれば、

$$
A' = T^{-1}AT = \transposeMat{T}AT = \begin{pmatrix}
\alpha_1 & 0 & \cdots & 0 \\
0 & \alpha_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \alpha_n
\end{pmatrix}
$$
と対角化される。このことを$A$の基底の直交変換によって対角化されると言い表す。

\begin{Proof*}
$A$の固有値$\alpha_i (1 \leq i \leq n)$に対応する固有ベクトルからなる正規直交基底$\{\bm{t}_1, \cdots, \bm{t}_n\}$が存在することをいえばよい。これをnに関する帰納法で証明する。$n = 1$のとき明らかに成立する。よって、$n \geq 2$とし、$\alpha_1$を$A$の固有値とする。
実対称行列の固有値は実数であることより、$\alpha_1 \in \mathbb{R}$であるから、それに属する固有ベクトル$\bm{v}_1 \in \vecSpace{n}$をとることができる。これを正規化して

$$
\bm{t}_1 = \frac{\bm{v}_1}{\|\bm{v}_1\|}
$$

とおけば、$\bm{t}_1$も$\alpha_1$に属する固有ベクトルで$\|\bm{t}_1\| = 1$である。

$$
W_1 = \langle \bm{t}_1 \rangle, \quad W_2 = W_1^\bot
$$

とおく。$W_1$は$A-不変$であるから、$W_2$も$A-不変$で

$$
V = W_1 \oplus W_2
$$

と直和分解される。（定理(\ref{rem:orthogonalComplementOplus})より） 次に、$W_2$から適当に基底を取り、それらにシュミットの正規直交化法を適用して$\{\bm{t}_2, \cdots, \bm{t}_n\}$とする。

$$
T = (\bm{t}_1, \bm{t}_2, \cdots, \bm{t}_n)
$$

とおけば、$T$は直交行列になる。直交行列に関する定理(\ref{rem:orthogonalMatrix})により、$\transposeMat{T}AT = (\transposeMat{\bm{t}}_iA\bm{t}_j)$とおけば、その$ij$成分は$A \langle \bm{t}_i, \bm{t}_j \rangle = \alpha_i \langle \bm{t}_i, \bm{t}_j \rangle $のように固有値と内積の積（実数）で表される。ここで、$\transposeMat{T} = T^{-1}, \transposeMat{T}T = E$より、$\transposeMat{T}AT$の$ij$成分はさらに$\alpha_i\delta_{ij}$と書ける。特に$i =  1$のときの固有値を$\alpha_1$とすれば、$j = 1$のとき$\alpha_1 \cdot 1 = \alpha_1$それ以外で、$\alpha_1 \cdot 0 = 0$になる。よって


$$
A' = T^{-1}AT = \begin{pmatrix}
\alpha_1 & 0 \\
0 & A'_2
\end{pmatrix}
$$

の形になる。ここで、\begin{eqnarray*}
\transposeMat{A}' = \transposeMat{(\transposeMat{T}AT)} = \transposeMat{T}\transposeMat{A}\transposeMat{\transposeMat{T}} = \transposeMat{T}AT = A'
\end{eqnarray*}

より、$A'_2$は$(n-1)$次対称行列である。ゆえに帰納法の仮定により

$$
T_2^{-1}A'_2T_2 = \begin{pmatrix}
\alpha_2 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \alpha_n
\end{pmatrix}
$$

となるような直交行列$T_2$が存在し

$$
A' = \begin{pmatrix}
1 & 0 \\
0 & T_2
\end{pmatrix}
$$

とすると、$A'$は直交行列で

\begin{eqnarray*}
A' = T^{-1}AT &=& \begin{pmatrix}
1 & 0 \\
0 & T_2^{-1}
\end{pmatrix}\begin{pmatrix}
\alpha_1 & 0 \\
0 & A'_2
\end{pmatrix}\begin{pmatrix}
1 & 0 \\
0 & T_2
\end{pmatrix} \\[1.5ex]
&=& \begin{pmatrix}
\alpha_1 & 0 \\
0 & T_2^{-1}A'_2T_2
\end{pmatrix} \\[1.5ex]
&=& \begin{pmatrix}
\alpha_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \alpha_2
\end{pmatrix}
\end{eqnarray*}

よって、nの場合にも定理は成立する。

\end{Proof*}
}

\exercise{
$A = \begin{pmatrix}
2 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 2
\end{pmatrix}$を対角化せよ。

\begin{Answer*}
まずは$A$の固有値、固有ベクトルを求める。$A$の特性多項式は
\begin{eqnarray*}
\begin{vmatrix}
x - 2 & -1 & -1 \\
-1 & x - 2 & -1 \\
-1 & -1 & x - 2
\end{vmatrix} &=& (x - 2)^3 -2 -3(x - 2) \\
&=& x^3 - 6x^2 + 9x -4 \\
&=& (x - 4)(x - 1)^2
\end{eqnarray*}

より、解は$x = 4, x = 1$(重解)が得られる。よって、$A$の固有値は$4, 1$である。まず、固有値$4$に対応する固有ベクトルを求める。

$$
\begin{pmatrix}
2 & -1 & -1 \\
-1 & 2 & -1 \\
-1 & -1 & 2
\end{pmatrix}\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
$$

を解けば、$x_1 = x_2 = x_3 = 1$であるから、固有値$4$に対応する固有ベクトルは$\begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}$である。ここで、このベクトルが張る固有空間は

$$
W_1 = \langle\begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}\rangle
$$

となる。$\rank A = 3, \rank W_1 = 1$であるから、$W_1^\bot = \dim A - \dim W_1 = 2$にならなければならい。（つまり、このことはn次方程式が重解を含むときもかならずランクに対応した数の固有ベクトルがあるということ。）

このことを考慮して、$\lambda = 1$のときの固有ベクトルを求める。

$$
\begin{pmatrix}
-1 & -1 & -1 \\
-1 & -1 & -1 \\
-1 & -1 & -1
\end{pmatrix}\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} = \begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix} \Longleftrightarrow x_1 + x_2 + x_3 = 0
$$ 

これを解くと、$x_1 = - x_2 - x_3$より、$x_2 = -\alpha_1, x_3 = -\alpha_2$とおけば

$$
\begin{pmatrix}
\alpha_1 + \alpha_2 \\
-\alpha_1 \\
-\alpha_2
\end{pmatrix} = \alpha_1 \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix} + \alpha_2 \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}
$$

である。したがって、固有値$1$に属する固有ベクトルは

$$
\begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix}, \quad \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}
$$

である。（この二つは線形独立であることは容易に確かめられる。）

ここで得られた3つの基底（固有ベクトル）

$$
\bm{x}_1 = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}, \quad \bm{x}_2 = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix}, \quad \bm{x}_3 = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}
$$

は$\langle \bm{x}_2, \bm{x}_3 \rangle \neq 0$であるから、直交していない。これをシュミットの正規直交化法で直交化すると

$$
\begin{pmatrix}
\frac{1}{\sqrt{3}} \\[1.5ex]
\frac{1}{\sqrt{3}} \\[1.5ex]
\frac{1}{\sqrt{3}}
\end{pmatrix}, \quad \begin{pmatrix}
\frac{1}{\sqrt{2}} \\[1.5ex]
-\frac{1}{\sqrt{2}} \\[1.5ex]
0
\end{pmatrix}, \quad \begin{pmatrix}
\frac{1}{\sqrt{6}} \\[1.5ex]
\frac{1}{\sqrt{6}} \\[1.5ex]
- \frac{2}{\sqrt{6}}
\end{pmatrix}
$$

が得られる。これを並べて

$$
T = \begin{pmatrix}
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} \\[1.5ex]
\frac{1}{\sqrt{3}} & - \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} \\[1.5ex]
\frac{1}{\sqrt{3}} & 0 & - \frac{2}{\sqrt{6}}
\end{pmatrix}
$$

とおけば、$T$は直交行列で

$$
T^{-1}AT = \begin{pmatrix}
4 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
$$

と対角化される。

\end{Answer*}
}

% TODO 吉田さん
% 線形写像 4.6行列の階数からレビュー再開

\begin{thebibliography}{n}
\bibitem[1]{key2} 佐武一郎 [線形代数] 共立出版株式会社
\end{thebibliography}

\end{document}