\documentclass[dvipdfmx,autodetect-engine]{jsarticle}
\usepackage{tikz}
\usepackage{graphicx,fancybox,ascmac, amsmath, amssymb}
\usepackage[all]{xy}
\usepackage{bm}
\usepackage{cases}
\usepackage{mathtools}

\newtheorem{theo}{定理}[section]
\newtheorem{defi}[theo]{定義}
\newtheorem{rem}[theo]{定理}
\newtheorem{exam}[theo]{例}
\newtheorem{exercise}[theo]{例題}
\newtheorem{prop}[theo]{命題}
\newtheorem{ques}[theo]{問}
\newtheorem{lemm}[theo]{補題}

% functions %
\newcommand{\innerProduct}[2]{\langle \bm{#1}, \bm{#2} \rangle}
\newcommand{\tensorProduct}[2]{\bm{#1} \otimes \bm{#2}}
\newcommand{\transposeMat}[1]{{}^t\!{#1}}
\newcommand{\transposeVec}[1]{{}^t\!{\bm{#1}}}
\newcommand{\vecSpace}[1]{\mathbb{R}^{#1}}
\newcommand{\polynomialVecSet}[1]{\mathbb{R}[x]_{#1}}
\newcommand{\vecSet}[1]{\{\bm{#1}_1, \cdots, \bm{#1}_n\}}
\newcommand{\linearCombination}[2]{\langle #1, \cdots, #2\rangle}
\newcommand{\rank}[1]{{\rm rank}\,#1}
\newcommand{\img}[0]{{\rm Im}}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\setcounter{tocdepth}{3}

\title{線形代数}

\author{武井優己}
\date{\today}
\begin{document}
\maketitle

\tableofcontents

\section{2次行列の演算}

線形代数では、数の拡張としてベクトル、行列という高次元の数を考える。いきなりn次元に目を向けるのは難しいので、ウォーミングアップとして2次元の行列に対して、通常の数と同様の加法と乗法を天下り的に定義する。ここでは、行列は我々の知る実数に似た代数的構造を持つ（実数のような演算ができる）ということが単に分かればよい。なぜそうなるかはこのレポートを読み進めていくうちに自ずと理解できるようになってくる。

$A, A'$をそれぞれ以下の2次（正方）行列とし、行列の演算を進めていくことにする。2次正方行列とは行と列がそれぞれ$2 \times 2$の形をした行列のことである。

$$
A = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix},
A' = \begin{pmatrix}
a' & b' \\
c' & d' \\
\end{pmatrix}
$$

$A$であれば、$(a, b), (c, d)$の横の並びが行、$\begin{pmatrix}
a \\
c \\
\end{pmatrix}, \begin{pmatrix}
b \\
d \\
\end{pmatrix}$の縦の並びが列に該当する。


\subsection{2次行列の加法}

$A, A'$に対し、その和を

$$
A + A' = \begin{pmatrix}
a + a' & b + b' \\
c + c' & d + d'\\
\end{pmatrix}
$$

によって定義する。

$A,A'$の他に、

$$
A'' = \begin{pmatrix}
a'' & b'' \\
c'' & d'' \\
\end{pmatrix}
$$

があるとすれば、これらの和は

$$
(A + A') + A'' = \begin{pmatrix}
(a + a') + a'' & (b + b') + b'' \\
(c + c') + c'' & (d + d') + d'' \\
\end{pmatrix}
$$

$$
A + (A' + A'') = \begin{pmatrix}
a + (a' + a'') & b + (b' + b'') \\
c + (c' + c'') & d + (d' + d'') \\
\end{pmatrix}
$$

と書くことができる。

ここで、和で表される各成分は実数における加法の形になっているため、

$$
(A + A') + A'' = A + (A' + A'')
$$

と結合法則が成立する。同様に、
$$
A + A' = A' + A
$$

と交換法則も成立する。

\subsubsection{零行列}

すべての成分が0である行列を{\bf 零行列}といい

$$
0 = \begin{pmatrix}
0 & 0 \\
0 & 0 \\
\end{pmatrix}
$$

と書く。演算において零行列は単に$0$として表し、$A + 0 = 0 + A = A$が成立する。

\subsubsection{2次行列の減法}

加法の逆演算として減法も可能である。$X + A = A'$となる2次行列

$$
X = \begin{pmatrix}
x & y \\
z & w \\
\end{pmatrix}
$$

が存在する。このような$X$があったとすれば、$A'$のそれぞれの成分は

\begin{eqnarray*}
x + a = a', \quad y + b = b' \\
z + c = c', \quad w + d = d' \\
\end{eqnarray*}

であるから、$X$の成分はそれぞれ

\begin{eqnarray*}
x = a' - a, \quad y = b' - b \\
z = c' - c, \quad w = d' - d \\
\end{eqnarray*}

でなければならない。これより、$X$の成分は$A, A'$の差の唯一の解


$$
X = \begin{pmatrix}
a' - a & b' - b \\
c' - c & d' - d \\
\end{pmatrix}
$$

をもつ。この$X$を$A'-A$とするのが行列の減法である。

\subsection{2次行列の乗法}\label{subsubsection:matrixMultiple}

\subsubsection{2次行列のスカラー倍}

$k \in \mathbb{R}$に対して

$$
kA = \begin{pmatrix}
ka & kb \\
kc & kd
\end{pmatrix}
$$

となるような演算をスカラー倍(乗法)という。また、

$$
A0 = 0A = 0
$$

$-A = -1 \times A$より、

$$
A(-A') = (-A)A' = -AA'
$$

である。

\subsubsection{2次行列の積}

二つの2次行列$A, A'$の積を次のように定義する。

$$
AA' = \begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\begin{pmatrix}
a' & b' \\
c' & d'
\end{pmatrix} = 
\begin{pmatrix}
aa' + bc' & ab' + bd' \\
ca' + dc' & cb' + dd' \\
\end{pmatrix}
$$

$A$の$i$行と$A'$の$j$列の対応する成分の積の和を$AA'$の$(i, j)$成分と定義するのが行列の積である。なぜ加法のような対応ではないのかと疑問に思うが、それは本レポートを読み進めるうちに理解できるようになる。

積についても、結合法則が成立する。

$$
A'' = \begin{pmatrix}
a'' & b'' \\
c'' & d'' \\
\end{pmatrix}
$$

とすると、

$$
(AA')A'' = A(A'A'')
$$

である。しかし、2次行列の積に関しては交換法則

$$
AA' = A'A
$$

は一般には成立しない。実際、$AA'$の$(1, 1)$成分は$aa' + bc'$であるが、$A'A$の$(1, 1)$成分は、$a'a + b'c$となり、$AA' \neq A'A$である。

加法と乗法を組み合わせると、

\begin{eqnarray*}
A(A' + A'') = AA' + AA'' \\
(A' + A'')A = A'A + A''A
\end{eqnarray*}

のような分配法則が成立する。

このように、2次正方行列は一部の例外を除きほぼ実数と同じような振る舞いをすることが分かる。（これは、群・環・体といった代数的構造の賜である）

\section{一般の行列とベクトル}

\subsection{行列の成分表示}

$m \times n$個の数を矩形に並べ括弧で囲んだものを{\bf $m$行$n$列の行列}あるいは、{\bf $m \times n$行列}という。$m \times n$行列の全体の集合を$M_{m,n}(\mathbb{R})$と書き、$m = n$であるとき、{\bf $n$次正方行列}といい、$M_n(\mathbb{R})$と書く。行列を構成するそれぞれの数を{\bf 成分}と呼ぶ。

\defi {一般の行列の成分表示}

上述の例より、一般の $m \times n$行列$A$を成分表示すると次のようになる。 $A$の $(i, j)$成分を $(a_{ij})$とするとき

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix}
$$

である。

多くの場合これを省略して、$A = (a_{ij})$と表記する。ただし、
$(1 \leq i \leq m), (1 \leq j \leq n)$

\exam {$2 \times 2$行列の表示}

$m = 2, n = 2$のとき、2次正方行列Aは以下のように表示される

$$
A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{pmatrix}
$$

\exam {$4 \times 5$行列の表示}

$m = 4, n = 5$のとき、$4 \times 5$行列Aは以下のように表示される

$$
A = \begin{pmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
\end{pmatrix}
$$

\subsection{一般の行列の加法、スカラー倍}\label{subsection:generalMatrixAdditionAndScalarMultiple}

\defi{一般の行列の加法、スカラー倍}\label{defi:additionAndScalarMultiple}

$A, B \in M_{m,n}(\mathbb{R}), A = (a_{ij}), B = (b_{ij})$とし、加法、スカラー倍を以下のように定義する。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A + B = (a_{ij}) + (b_{ij})$
\item $cA = (ca_{ij}) \quad (c \in \mathbb{R})$
\item $(-1)A = -A, \quad A + (-B) = A - B$
\end{enumerate}

\prop{定義\ref{defi:additionAndScalarMultiple}より、以下の法則を満たす}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $(A + B) + C = A + (B + C)$ \quad (結合法則)
\item $A + B = B + A$ \quad (交換法則)
\item $c(A+B) = cA + cB$ \quad (分配法則)
\end{enumerate}

である。

{\bf 証明}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $a_{ij}とb_{ij}の各成分どうしに結合法則が成り立つことより明らか$
\item $a_{ij}とb_{ij}の各成分どうしに交換法則が成り立つことより明らか$
\item $A + B = a_{ij} + b_ij$より、$c(A + B) = c(a_{ij} + b_ij)$である。
$cA = ca_{ij}, cB = cb_{ij}$であるから、$cA + cB = ca_{ij} + cb_{ij}$となる。
ここで実数における分配法則により、$c(a_{ij} + b_{ij}) = ca_{ij} + cb_{ij}$より、すべての(i, j)について分配法則が適用されるから、$c(A+B) = cA + cB$である。
\end{enumerate}

\defi{零行列との加法、スカラー倍}

$A \in M_{m,n}(\mathbb{R})$とするとき、零行列$0$との加法、スカラー倍で以下が成立する。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A + 0 = 0 + A = A$
\item $A + (-A) = (-A) + A = 0$
\item $1A = A$
\item $0A = 0$
\end{enumerate}


\subsection{一般の行列の積}

\defi{$l, m, n \in \mathbb{Z}$とし、$A \in M_{l,m}(R), B \in M_{m,n}(\mathbb{R})$とするとき、積ABを以下のように定義する。}\label{defi:matrixMultiple}

$$
AB = \sum_{1 \leq k \leq m} a_{ik}b_{kj} \in M_{l, n}(\mathbb{R})
\quad (ただし、1 \leq i \leq l, 1 \leq j \leq n)
$$

\ref{subsubsection:matrixMultiple}で紹介した$2 \times 2$行列の積がまさにこのようになっている。このように行列の積を定義する理由は、線形写像の章で説明する。

\prop{一般の行列の積について以下の法則が成立する}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A(BC) = (AB)C$
\item $A(B+C) = AB + AC$
\item $(B+C)A = BA + CA$
\end{enumerate}

これらも$AB$の各成分どうしが実数の積と和の形をしていることから、実数の演算における法則が適用できる。これにより、実際に演算することでこれらの証明は容易である。
(2)と(3)が同値でない理由として、行列の積は非可換なことが挙げられる。
実際、$AB$と$BA$を行列の積の定義に従い演算すると、それぞれ異なる行列が得られ、$AB \neq BA$となる。そのため、$A(B+C) \neq (B+C)A$となる。

\subsection{単位行列}

\defi

$A \in M_n(\mathbb{R})$に対して$AE = EA = A$となるような行列を{\bf $n$次単位行列}という。単に、単位行列ということもある。

単位行列は、

$$
E = \begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & 1 \\
\end{pmatrix}
$$

のように対角成分が$1$でそれ以外が$0$から構成される行列である。


\subsection{逆行列}

n次行列Aに対し$AX = E, XA = E$を満たすようなn次行列$X$が存在するとき、$A$は{\bf 可逆}である、または{\bf 正則行列}という。

\rem{n次行列Aが可逆であれば、$AX=E$および$YA=E$はそれぞれ唯一の解を持ち、それらの解は一致する。そのような$X, Y$を$A^{-1}$と書き、{\bf 逆行列}という。}

{\bf 証明}

$AX = XA = E, AY = YA = E$となる行列$X, Y$があるとする。このとき、結合法則により

\begin{eqnarray*}
X &= &XE \\
  &= &X(AY) \\
  &= &(XA)Y \\
  &= &EY \\
  &= &Y
\end{eqnarray*}

となり、$X = Y$である。仮に、$AB = BA = E, AC = CA = E$となる行列$B, C$があったとすれば、同様の理由により$X = Y = B = C$とこれらの方程式の解は一意に定まる。

\exam{単位行列の逆行列}

単位行列Eは$EE = E$であるから$E^{-1} = E$であり可逆である。$A, B \in M_n(\mathbb{R})$が可逆ならば、$AB$も可逆で$(AB)^{-1} = B^{-1}A^{-1}$である。
これが言える理由として、$(AB)(B^{-1}A^{-1}) = (B^{-1}A^{-1})(AB) = E$だからである。また、$A$が可逆であれば$A^{-1}$も可逆であり、$(A^{-1})^{-1} = A$。つまり、$(A^{-1})A = A(A^{-1}) = E$である。

これは、$M_n(\mathbb{R})$の可逆元全体が群の性質を持つことを言っており、この行列全体の集合を一般線形群といい、$GL_n(\mathbb{R})$と表される。

\subsection{列ベクトルと行ベクトル}

\subsubsection{(n, 1)型行列}

$$
B = \begin{pmatrix}
b_{11} \\
\vdots \\
b_{n1}
\end{pmatrix}
$$

を{\bf n-dim列ベクトル}という。

本レポートでは、n-dim列ベクトルを

$$
\bm{b} = \begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

のようにボールドのアルファベットで表記する。

\subsubsection{(1, n)型行列}

$(b_{11}, \cdots, b_{n1})$を{\bf n-dim行ベクトル}という。

\subsubsection{ベクトルの演算}

n-dim列ベクトルは(n,1)型の行列であるから、やはり加法と乗法が成立する。以降の節の理解を円滑に進めるに当たり、ここではベクトルの演算に関する細かな定義や性質の説明は飛ばす。(3章のベクトル空間にて、この辺りの細かい議論を行う) よって、予めベクトルの演算はできるものとして気楽に考えてほしい。

2次元ベクトルにおいて、

$$
\bm{a} = \begin{pmatrix}
x \\
y
\end{pmatrix},
\bm{b} = \begin{pmatrix}
x' \\
y'
\end{pmatrix}
$$
を取ってくる。

このとき、$a + b$は

$$
\begin{pmatrix}
x \\
y
\end{pmatrix} + 
\begin{pmatrix}
x' \\
y'
\end{pmatrix}
= \begin{pmatrix}
x + x' \\
y + y'
\end{pmatrix}
$$

である。

また、$c \in \mathbb{R}$に対して、

$$
c\bm{a} =
c\begin{pmatrix}
x \\
y
\end{pmatrix}
= \begin{pmatrix}
cx \\
cy
\end{pmatrix}
$$

である。

\subsection{連立一次方程式と行列}

$$
A = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix} \in M_2(\mathbb{R}),
\bm{x} = c\begin{pmatrix}
x \\
y
\end{pmatrix}
$$
とすれば、

$$
A\bm{x} = \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
= \begin{pmatrix}
ax + by \\
cx + dy \\
\end{pmatrix}
$$

のように、2次元ベクトル$\bm{x}$に2次行列$A$を差乗することができる。この行列算の応用として$A\bm{x}$を連立一次方程式として表示することを考える。$ax + by$と$cx + dy$の和をそれぞれ$z, w$とすると

\begin{numcases}
  {}
  ax + by = z & \\
  cx + dy = w &
\end{numcases}

のような連立方程式で表すことができる。これより、もし$A$が逆行列を持つなら、

$$
\begin{pmatrix}
x \\
y
\end{pmatrix}
= A^{-1}
\begin{pmatrix}
z \\
w
\end{pmatrix}
$$
と計算することができる。

\subsection{転置行列}

(m, n)行列

$$
A = \begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots \\
a_{m1} & \cdots & a_{mn} \\
\end{pmatrix}
$$

に対し対角線$a_{aa} a_{22} \cdots$に対して、折返してできる(n, m)行列を${}^t\!A$と書き、$A$の転置行列という。

${}^t\!A$を表示すると以下の通りである。

$$
{}^t\!A = \begin{pmatrix}
a_{11} & \cdots & a_{m1} \\
\vdots & \vdots & \vdots \\
a_{1n} & \cdots & a_{mn} \\
\end{pmatrix}
$$

\prop{$A, B \in M_{m, n}(R)$に対して、明らかに${}^t(A + B) = \transposeMat{A} + \transposeMat{B}, {}^t{}^t\!A = A$が成立する。}

\subsubsection{対称と交代}

$A \in M_n(\mathbb{R})$とする。$A = {}^t\!A$ つまり、$a_{ij} = a_{ji} \quad (1 \leq i, j \leq n)$であるとき、$A$を対称または対称行列という。

$A = -{}^t\!A$ つまり、$a_{ij} = -a_{ji} \quad (1 \leq i, j \leq n)$であるとき、$A$を交代(的)または交代行列という。

\subsubsection{内積とテンソル積}

後の計量ベクトル空間の章で内積について詳しく説明するが、ここでは内積の定義とその意味についてだけ軽く触れておく。

二つのn次元ベクトル

$$
\bm{a} = \begin{pmatrix}
a_1 \\
\vdots \\
a_n
\end{pmatrix}, 
\bm{b} = \begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

に対し、


$$
{}^t\!\bm{a}\bm{b} = \sum_{1 \leq i \leq n} a_ib_i = (a_1, \cdots, a_n)\begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}
$$

を{\bf 内積}といい$\langle \bm{a}, \bm{b} \rangle$と書く。

また、
$$
\bm{a}{}^t\!\bm{b} = \begin{pmatrix}
a_1b_1 & \cdots & a_1b_n \\
\vdots & \vdots & \vdots \\
a_bb1 & \cdots & a_nb_n \\
\end{pmatrix}
$$

を$\bm{a}, \bm{b}$の{\bf テンソル積}といい、 $\bm{a} \otimes \bm{b}$と書く。

\exercise{$\bm{a}, \bm{b}, \bm{c}, \bm{d}$をn次元ベクトルとするとき、次の等式を証明せよ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item ${}^t(\tensorProduct{a}{b}) = \tensorProduct{b}{a}$
\item $(\tensorProduct{a}{b})\bm{c} = \innerProduct{b}{c}\bm{a}$
\item $\innerProduct{c}{(\tensorProduct{a}{b})\bm{d}} = \innerProduct{a}{c}\innerProduct{b}{d}$
\end{enumerate}

{\bf 解答}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item ${}^t(\tensorProduct{a}{b}) = {}^t(\bm{a}{}^t\bm{b}) = {}^t{}^t\bm{b}{}^t\bm{a} = \bm{b}{}^t\bm{a} = \tensorProduct{b}{a}$
\item $(\tensorProduct{a}{b})\bm{c} = (\bm{a}{}^t\bm{b})\bm{c} = \innerProduct{b}{c}\bm{a}$
\item $\innerProduct{c}{(\tensorProduct{a}{b})\bm{d}} = {}^t\bm{c}(\bm{a}{}^t\bm{b})\bm{d} = \innerProduct{c}{a}\innerProduct{b}{d} = \innerProduct{a}{c}\innerProduct{b}{d}$
\end{enumerate}


\section{ベクトル空間(線形空間)}

n次元ベクトル全体の集合を$\vecSpace{n}$とかく。2章の時点ですでに2次元のベクトルやベクトルと行列間に演算を導入していたが、一般的な$\vecSpace{n}$に対して、演算を改めて定義する。

\defi{
$$
\bm{x} = \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix},
\bm{y} = \begin{pmatrix}
y_1 \\
\vdots \\
y_n
\end{pmatrix} \in \vecSpace{n}, a \in \mathbb{R}
$$
とするとき、これらの加法(減法)、スカラー倍はそれぞれ次のように定義される。
}

$$
\bm{x} + \bm{y} = \begin{pmatrix}
x_1 + y_1 \\
\vdots \\
x_n + y_n
\end{pmatrix},
\bm{x} - \bm{y} = \bm{x} + -(\bm{y}) \begin{pmatrix}
x_1 + -(y_1) \\
\vdots \\
x_n + -(y_n)
\end{pmatrix},
a\bm{x} = \begin{pmatrix}
ax_1 \\
\vdots \\
ax_n
\end{pmatrix}
$$

$\vecSpace{n}$が次の条件を満たしたとき、単なる集合ではなく、{\bf n次元ベクトル空間}となる。単にベクトル空間ともいう。

\defi{$V = \vecSpace{n}$とするとき、$V$は以下の条件のもと、ベクトル空間となる}\label{defi:VectorSpace}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item 任意の$\bm{x}, \bm{y}, \bm{z} \in V$に対して、$(\bm{x}+\bm{y}) + \bm{z} = \bm{x}+ (\bm{y} + \bm{z}) $ \quad (結合法則)
\item 任意の$\bm{x}, \bm{y}\in V$に対して、$\bm{x} + \bm{y} = \bm{y} + \bm{x}$ \quad (交換法則)
\item 任意の$\bm{x} \in V$に対して、$\bm{0} + \bm{x} = \bm{x} + \bm{0} = \bm{x}$ \quad $(零ベクトルの存在)$
\item 任意の$\bm{x} \in V$に対して、逆ベクトル$-\bm{x}$が存在して、$\bm{x} + (-\bm{x}) = (-\bm{x}) + \bm{x} = \bm{0}$ \quad (逆ベクトルの存在)
\item{
    任意の$\bm{x}, \bm{y} \in V, a, b \in \mathbb{R}$に対して
    \begin{enumerate}
    \item $a(\bm{x}+\bm{y}) = a\bm{x}+a\bm{y}$
    \item $(a+b)\bm{x} = a\bm{x} + b\bm{x}$
    \item $a(b\bm{x}) = ab(\bm{x})$
    \end{enumerate}
}
\item 任意の$\bm{x} \in V$に対して$1 \cdot \bm{x} = \bm{x} \quad (1 \in \mathbb{R})$
\end{enumerate}

\prop{これらをベクトル空間の公理と認めると、以下が成り立つ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{0}\bm{x} = \bm{0}$
\item $(-1)\bm{x} = -\bm{x}$
\end{enumerate}

{\bf 証明}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item (5)より、$\bm{0}\bm{x} = (\bm{0} + \bm{0})\bm{x} = \bm{0}\bm{x} + \bm{0}\bm{x}$となり、両辺から$\bm{0}\bm{x}$を引くと、$\bm{0} = \bm{0}\bm{x}$
\item $\bm{0} = \bm{0}\bm{x} = (1 + -(1))\bm{x} = 1 \cdot \bm{x} + (-1) \cdot \bm{x} = \bm{x} + (-1)\bm{x}$となり、両辺から$-\bm{x}$を引くと、$-\bm{x} = (-1)\bm{x}$
\end{enumerate}

\subsubsection{単位ベクトル}\label{subsection:unitVector}

特殊なベクトルとして$\bm{e}_1, \cdots \bm{e}_n \in V$があり、それぞれが

$$
\bm{e}_1 = \begin{pmatrix}
1 \\
0 \\
\vdots \\
0
\end{pmatrix},
\bm{e}_2 = \begin{pmatrix}
0 \\
1 \\
\vdots \\
0
\end{pmatrix}, 
\cdots, 
\bm{e}_n = \begin{pmatrix}
0 \\
0 \\
\vdots \\
1
\end{pmatrix}
$$

のように大きさが1になるようなベクトルを、{\bf 単位ベクトル}という。この単位ベクトル$\bm{e}_1, \cdots \bm{e}_n$を使えば、任意の$V$に対して

$$
\bm{x} = \sum_{1 \leq i \leq n} x_ie_i
$$

と一意に表現できる。

\subsection{部分空間}

\defi{$V$をn次元ベクトル空間とする。$V \supset W \neq \phi$とする。$W$が以下の条件を満たすとき、$W$を$V$の部分空間という}\label{defi:subspace}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{x}, \bm{y} \in W \Rightarrow \bm{x} + \bm{y} \in W$
\item $\bm{x} \in W, a \in \mathbb{R} \Rightarrow a\bm{x} \in W$
\end{enumerate}

これらの条件は、Wの中でベクトルとしての演算が閉じていることを示している。

\lemm{$W$が$V$の部分空間であれば、$W$もベクトル空間である}

{\bf 証明}

仮定により$\bm{x} \in W$であるから、定義\ref{defi:subspace}(2)を使うと、$0 \in \mathbb{R}, 0 \cdot \bm{x} = \bm{0} \in W$と$-1 \in \mathbb{R}, (-1)\bm{x} = -\bm{x} \in W$である。したがって、任意の$\bm{x}, \bm{y}$に対して、$\bm{x} - \bm{y} = \bm{x} + (-\bm{y}) \in W$である。ベクトルの演算の公理が$V$で成立しているので、$W$においても成立することは明らか。また零ベクトルと任意のベクトル$\bm{x}の$逆ベクトルも$W$に存在することは分かっているので、$W$もまたベクトル空間である。

つまり、部分空間はベクトル空間として扱うことができるということである。

\prop{$\{\bm{0}\}$、$V(=\vecSpace{n})$は部分空間である}

{\bf 証明}

$\bm{0} + \bm{0} = \bm{0}, \quad a\bm{0} = \bm{0} \quad (\forall a \in \mathbb{R}, \bm{0} \in \{\bm{0}\})$より、$\{\bm{0}\}$は部分空間である。また、$V$はベクトル空間なので、部分空間の条件を満たすことは明らか。これらの部分空間を{\bf 自明な部分空間}と呼ぶ。

\exercise{$V = \vecSpace{3}$において、以下は部分空間か判定せよ}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $$
W_1 = \left\{ 
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} \mid x_1 + x_2 + x_3 = 0
\right\}
$$
\item $$
W_2 = \left\{ 
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} \mid 2x_1 + x_2 = 5
\right\}
$$
\end{enumerate}

{\bf 解答}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item 部分空間である。実際、
$$
\bm{x} = \begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix},
\bm{y} = \begin{pmatrix}
y_1 \\
y_2 \\
y_3
\end{pmatrix} \in W_1
$$

をとってきて、$\bm{x} + \bm{y}$を計算すると、$(x_1 + x_2 + x_3) = 0, (y_1 + y_2 + y_3) = 0$より、$(\bm{x}_1 + \bm{y}_1) + (\bm{x}_2 + \bm{y}_2) + (\bm{x}_3 + \bm{y}_3) = \bm{0} \in W_1$である。同様に、$\forall a \in \mathbb{R}$を取ってきて、$a\bm{x} = \bm{0} \in W_1$である。

\item 部分空間ではない。凡例をあげる。

$$
\bm{x} = \begin{pmatrix}
1 \\
3 \\
0
\end{pmatrix},
\bm{y} = \begin{pmatrix}
\dfrac{1}{2} \\[1.5ex]
4 \\
0
\end{pmatrix} \in W_2
$$をとってくる。両方とも上の条件を満たすが、

$$
\bm{x} + \bm{y} = \begin{pmatrix}
\dfrac{3}{2} \\[1.5ex]
7 \\
0
\end{pmatrix}
$$

$2 \cdot \dfrac{3}{2} + 7 \neq 5$であるため、$\bm{x} + \bm{y} \notin W_2$となり、部分空間の必要十分条件に反する。
\end{enumerate}

\defi{
$V = \vecSpace{n}$とする。任意の$\bm{x} \in V$に対し、そのスカラー倍全体の集合$\{a\bm{x}\mid a \in \mathbb{R}\}$は明らかに$V$の部分空間になる。さらに一般的に、$\bm{x_1}, \cdots, \bm{x_r} \in V$が与えられたとき、

$$
\bm{x} = \sum_{1 \leq i \leq r} a_{i}\bm{x}_i \quad (a_1, \cdots, a_r \in \mathbb{R})
$$

のような形で$\bm{x}$が表されるとき、これを$\bm{x_1}, \cdots, \bm{x_r}$の{\bf 線形結合}(linear combination)または{\bf 1次結合}という。
}\label{defi:linearCombination}

$\bm{x_1}, \cdots, \bm{x_r}$の線形結合全体の集合を$W = \{a_1\bm{x_1} + \cdots + a_r\bm{x_r} \mid a_1, \cdots, a_2 \in \mathbb{R} \}$とすると、$W$は$V$の一組の部分空間になる。これを$\bm{x_1}, \cdots, \bm{x}_r$によって張られる部分空間といい、$\langle \bm{x_1}, \cdots, \bm{x_r} \rangle$のように書く。(内積と混同しないように注意したい。)
$\newline$

{\bf 部分空間にならないことの証明}

$$
\bm{0} = \sum_{1 \leq i \leq r} 0\bm{x}_i \in W
$$

より$W \neq \phi$である。$\bm{x}, \bm{y} \in W$とすれば、

$$
\bm{x} = \sum_{1 \leq i \leq r} a_i\bm{x}_i, \quad \bm{y} = \sum_{1 \leq i \leq r} b_i\bm{x}_i \quad (a_i, b_i \in \mathbb{R})
$$

と書くことができる。これより、

$$
\bm{x} + \bm{y} = \sum_{1 \leq i \leq r} (a_i + b_i)\bm{x}_i
$$

とそれぞれのベクトルを$\bm{x}_i$の線形結合で表すことができる。

また、$c \in \mathbb{R}$に対し、

$$
c\bm{x} = c \left( \sum_{1 \leq i \leq r} a_i\bm{x}_i \right) = \sum_{1 \leq i \leq r} (ca_i)\bm{x}_i
$$とこれも$\bm{x}_i$の線形結合で表すことができる。

したがって、$\bm{x} + \bm{y}, \quad c\bm{x} \in W$となることから、Wは部分空間である。

\subsubsection{部分空間の共通部分}

\prop{$W_1, W_2$を$V$の二つの部分空間とするとき、その共通部分$W_1 \cap W_2$は部分空間になる。}

{\bf 証明}

$\bm{0} \in W_1 \cap W_2$であるから、$W_1 \cap W_2 \neq \phi$である。$\bm{x}, \bm{y} \in W_1 \cap W_2$とすれば、$\bm{x} + \bm{y} \in W_1$であり、$\bm{x} + \bm{y} \in W_2$である。（$\bm{x}, \bm{y} \in W_1, \bm{x}, \bm{y} \in W_2$だから。）したがって、$\bm{x} + \bm{y} \in W_1 \cap W_2$。同様に、$a \in \mathbb{R}$に対して、$a\bm{x} \in W_1 \cap W_2$である。


\subsection{部分空間の和}

部分空間の共通部分を考えたときに、今度は和集合（ベクトル空間どうしの和）も部分空間になるかを考えたくなるものである。$W_1, W_2$を$V = \vecSpace{3}$の部分空間とし、$W_1$と$W_2$の和集合$W_1 \cup W_2$を考えてみる。

$W_1 = \langle \bm{e}_1 \rangle, \quad W_2 = \langle \bm{e}_2 \rangle$とおくと、
$$
\begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}, 
\begin{pmatrix}
0 \\
1 \\
0
\end{pmatrix}
\in W_1 \cup W_2
$$

であるが、すべての$a, b \in \mathbb{R}$に対して、

$$
a\bm{e}_1 + b\bm{e}_2 = 
\begin{pmatrix}
a \\
b \\
0
\end{pmatrix}
\notin W_1 \cup W_2
$$

となる。（厳密に言うと、和集合の構成次第で含まれているところもあるかもしれないが、含まれていないところもある。）

そこで、$a\bm{e}_1 + b\bm{e}_2$をすべて含むようにすることを考えるために、$\langle \bm{e}_1, \bm{e}_2 \rangle$で張られる空間を考えると良さそうである。

\defi{一般に二つの部分空間$W_1, W_2$の和を以下のように定義する。}\label{defi:VecSpaceUnion}

$$
W_1 + W_2 = \{ \bm{x} + \bm{y} \mid \bm{x} \in W_1, \bm{y} \in W_2 \}
$$

この演算で定義される部分空間は$V$の最小の部分空間となる。それを次の定理で示す。

\rem{$V = \vecSpace{n}$とし、$W_1, W_2$を$V$の部分空間とする。$W_1 + W_2$は$W_1$と$W_2$を含む最小の部分空間となる。}

{\bf 証明}

まず、$W_1 + W_2$が部分空間であることを示す。

$\bm{0} =\bm{0} + \bm{0} \in W_1 + W_2$より、$W_1 + W_2 \neq \phi$。$\bm{x}, \bm{y} \in W_1 + W_2$とすれば、定義\ref{defi:VecSpaceUnion}により、$\bm{x}',\bm{y}' \in W_1, \bm{x}'', \bm{y}'' \in W_2$と書くことができる。（定義\ref{defi:linearCombination}で示した形式を思い出すと良い。）よって、$\bm{x} + \bm{y} = (\bm{x}' + \bm{x}'') + (\bm{y}' + \bm{y}'') = (\bm{x}' + \bm{y}') + (\bm{x}'' + \bm{y}'')$であり、$a \in \mathbb{R}$に対して、$\bm{x}' \in W_1$より$a\bm{x}' \in W_1$また、$\bm{x}'' \in W_2$より$a\bm{x}'' \in W_2$である。

これらより、$\bm{x}' + \bm{y}' \in W_1, \bm{x}'' + \bm{y}'' \in W_2$であるから、$\bm{x} + \bm{y} \in W_1 \cup W_2$であり、$a\bm{x} \in W_1 \cup W_2$である。($a\bm{x} \in W_1$であるから、和集合である$W_1 \cup W_2$に属するのは当然) よって、$W_1 + W_2$は$V$の部分空間である。

次に、$W_1 + W_2$が最小の部分空間であることを示す。

$\bm{x} \in W_1$とすれば、$\bm{x} = \bm{x} + \bm{0} \quad (\bm{x} \in W_1, \bm{0} \in W_2)$より、$\bm{x} \in W_1 + W_2$である。よって、$W_1 \subset W_1 + W_2$。同様に$W_2 \subset W_1 + W_2$が言え、$W_1 + W_2$は$W_1$と$W_2$を含むことが分かる。ここで、$W$を$W1, W_2$を含む部分空間とすれば、任意の$\bm{x} \in W_1, \bm{y} \in W_2$に対し、$\bm{x}, \bm{y} \in W$であるから、$\bm{x} + \bm{y} \in W$。よって、$W_1 + W_2 \subset W$となり、$W_1 + W_2$は$W_1$と$W_2$を含む最小の部分空間である。

\exercise{
一般に二つの部分空間$W_1, W_2$に対し、$W_1 \subset W_2 \Longleftrightarrow W_1 + W_2 = W_2$であることを示せ。
}

$W_1 \subset W_2$とすれば、$W_1 + W_2 \subset W_2 + W_2 = W_2$。$W_1 + W_2 \supset W_2$は自明であるから、$W_1 + W_2 = W_2$。逆に、$W_1 + W_2 = W_2$とすると、$W_1 \subset W_1 + W_2$であるから、$W_1 \subset W_2$である。

\exercise{
$W_1, W_2, W_3$を$V = \vecSpace{n}$の部分空間とするとき、$(W_1 \cap W_3) + (W_2 \cap W_3) \subset (W_1 + W_2) \cap W_3$を示せ。
}

$\bm{x} \in (W_1 \cap W_3) + (W_2 \cap W_3)$とする。このとき、$\bm{x} = \bm{x}' + \bm{x}''$と分解でき、$\bm{x}' \in W_1 \cap W_3 \hspace{3pt} \& \hspace{3pt} \bm{x}'' \in W_2 \cap W_3$. $\bm{x}', \bm{x}''$が両方とも$W_3$の元より、$\bm{x} \in W_3$また、$W_1, W_2$の定義より、$\bm{x} \in W_1 + W_2$。したがって、$\bm{x} \in (W_1 + W_2) \cap W_3$.

\subsection{線形独立と線形従属}

\defi{r個のベクトル$\bm{x}_1, \cdots \bm{x}_r$は、次の条件が満たされるとき{\bf 線形独立}(linearly independent)または{\bf 一次独立}であるという。}\label{defi:linearlyIndependent}

$$
\sum_{1 \leq i \leq r} a_i\bm{x}_i = 0 \quad (a_i \in \mathbb{R}) \Longrightarrow a_1 = \cdots = a_i = 0
$$

一方、線形独立ではない$\bm{x}, \cdots \bm{x}_r$を{\bf 線形従属}(linearly dependent)または{\bf 1次従属}であるという。$\bm{x}, \cdots \bm{x}_r$が線形従属であるとき以下の1次関係式が成立する。

$$
\sum_{1 \leq i \leq r} a_i\bm{x}_i = 0, \quad a_i \in \mathbb{R} \hspace{3pt} \& \hspace{3pt} \exists a_i \neq 0
$$

\subsubsection{線形独立と線形従属の定義の成り立ち}

線形独立と線形従属の定義は初見では理解が難しいので、この定義の成り立ちを考えてみることにする。

$V = \vecSpace{3}$において3つの基本ベクトル

$$
\bm{e}_1 = \begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}, 
\bm{e}_2 = \begin{pmatrix}
0 \\
1 \\
0
\end{pmatrix}, 
\bm{e}_3 = \begin{pmatrix}
0 \\
0 \\
1
\end{pmatrix}
$$

はそれぞれ独立した方向を向いていると考えられる。つまり、$\langle \bm{e}_1, \bm{e}_2 \rangle$を$xy$平面とすると、$\bm{e}_3$はそれに属さない、つまり$z$軸と考えることができる。これは$\bm{e}_3$は$\bm{e_1}, \bm{e_2}$の線形結合で表されないということである。同様に$\bm{e}_1, \bm{e}_2$のいずれかと$\bm{e}_3$が張る部分空間を考えたときに、残りの単位ベクトルはその部分空間に属さない。つまりこれは

\begin{equation}
    a\bm{e}_1 + b\bm{e}_2 + c\bm{e}_3 = 0 \quad (a, b, c, \in \mathbb{R}) 
\end{equation}

ならば$a = b = c = 0$と言い換えることができる。これは定義\ref{defi:linearlyIndependent}そのものである。なぜ$a = b = c = 0$であれば線形独立なのかを考えたいのであれば、逆に$c \neq 0$とすれば式(2)から

$$
\bm{e}_3 = \left(-\frac{a}{c}\right)\bm{e}_1 + \left(-\frac{b}{c}\right)\bm{e}_2
$$

のように$\bm{e}_3$を$\bm{e}_1$と$\bm{e}_2$の線形結合で表すことが出来てしまう。つまり、$\langle \bm{e}_1, \bm{e}_2 \rangle$に$\bm{e}_3$が含まれていることを意味する。$a \neq 0, b \neq 0$のときも同様である。

\prop{$\bm{x}_1, \bm{x}_2, \cdots \bm{x}_r$が線形独立ならばその1部分も線形独立である。}
線形独立の定義よりこれは明らかなので証明は割愛する。

\exercise{
$V = \vecSpace{3}$において

$$
\bm{x} = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}, 
\bm{z} = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix}
$$

が線形独立であることを示せ。
}

$a\bm{x} + b\bm{y} + c\bm{z} = 0 \quad (a, b, c \in \mathbb{R})$であれば、$a = b = c = 0$を示す。

\begin{numcases}
  {}
  a + (-b) = 0 & \\
  a + (-c) = 0 & \\
  a + b + c = 0 &
\end{numcases}

という連立1自方程式を解けば、$a = b = c = 0$が唯一の解となるので、$\bm{x}, \bm{y}, \bm{z}$は線形独立である。

\exercise{
$V = \vecSpace{3}$において

$$
\bm{x} = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
0 \\
1 \\
-1
\end{pmatrix}, 
\bm{z} = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix}
$$
は線形独立であるか調べよ。
}

$a\bm{x} + b\bm{y} + c\bm{z} = 0 \quad (a, b, c \in \mathbb{R})$とすると、$a, b, c$は

\begin{numcases}
  {}
  a + c = 0 \\
 -a + b = 0 \\
 -b + c = 0 
\end{numcases}

を満たす。このときこの連立一次方程式は$a = 1, b = 1, c = -1$の解を持つため、$\bm{x}, \bm{y}, \bm{z}$は線形従属である。

\lemm{$\bm{x}_1, \bm{x}_2, \cdots \bm{x}_r \in V$に関して次の2つの条件は同値である。}\label{lemm:linearlyIndependent}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\bm{x}_1, \cdots \bm{x}_r$は線形独立である
\item $\bm{x}_1, \cdots \bm{x}_{r-1}$は線形独立で、$\bm{x}_r \notin \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$
\end{enumerate}

{\bf 証明}

(1) $\Rightarrow$ (2)を示す。(1)を仮定すると、$a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} + a_r\bm{x}_r = 0$ならば、$a_i = 0 \quad (1 \leq i \leq r)$である。(2)が成り立たないとすれば、$\bm{x}_r \in \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$であるから$\bm{x}_r = a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} = 0$と表せる。よって、$a_1\bm{x}_1 + \cdots + a_{r-1}\bm{x}_{r-1} + (-1)\bm{x}_r = 0$が成立し$a_r \neq 0$であるから(1)に反する。よって、$\bm{x}_r \notin \langle \bm{x}_1, \cdots \bm{x}_{r-1} \rangle$

(2) $\Rightarrow$ (1)を示す。$\bm{x}_1, \cdots \bm{x}_r$が線形独立でないとすると、ある$a_i \neq 0 \quad (1 \leq i \leq r)$が存在し、$a_1\bm{x}_1 + \cdots + a_{r}\bm{x}_{r} = 0$が成立する。もし$a_r = 0$とすると$a_1, \cdots, a_{r-1}$のいずれかが$0$ではなくなる。これは、(2)の$\bm{x}_1, \cdots \bm{x}_{r-1}$が線形独立であるという仮定に反する。よって、$a_r \neq 0$でなければならない。しかしこのとき、

$$
 \left(-\frac{a_1}{a_r}\right)\bm{x}_1 + \cdots +  \left(-\frac{a_{r-1}}{a_r}\right)\bm{x}_{r-1} = \bm{x}_r \in \langle \bm{x}_1 + \cdots + \bm{x}_{r-1} \rangle
$$
となり(2)の仮定に反する。$\bm{x}_1, \cdots, \bm{x}_{r}$は線形独立である。

\rem{
$\bm{x}_1, \cdots \bm{x}_r$が線形独立であるための必要十分条件は$\bm{x}_1 \neq 0$かつ、任意の$i$に対して$\bm{x}_i \notin \langle \bm{x}_1, \cdots \bm{x}_{i-1} \rangle \quad (2 \leq i \leq r)$である。
}\label{rem:independent}

{\bf 証明}

($\Rightarrow$): 補題\ref{lemm:linearlyIndependent}により、$\bm{x}_1, \cdots, \bm{x}_r$が線形独立であれば、$\bm{x}_1, \cdots \bm{x}_i$も線形独立であるから、この主張が成立する。

($\Leftarrow$): この主張が成立すると仮定する。$\bm{x}_1, \cdots, \bm{x}_i$が線形独立であることは、補題\ref{lemm:linearlyIndependent}を再帰的に行うことで判断できる。まず$\bm{x}_1 \neq 0$から$\bm{x}_1$は線形独立である。次に、$\langle \bm{x}_1 \rangle$に$\bm{x}_2$が含まれていないか、さらにその次に$\langle \bm{x}_1, \bm{x}_2 \rangle$に$\bm{x}_3$が含まていないかといった判定を$\bm{x}_{i-1}$まで再帰的に行っていく。このとき、$\bm{x}_1, \cdots \bm{x}_{i-1}$が線形独立であるとすれば、補題\ref{lemm:linearlyIndependent}により、$\bm{x}_1, \cdots \bm{x}_i$も線形独立である。この判定方法は$i = r$まで成立する。(この操作は高々r回で終わる。)

\subsection{部分空間の基底}

基底の定義をする前に、以下の補題にてベクトル空間を張る線形独立なベクトルは別の線形独立なベクトルに置き換えることができることを示す。

\lemm{$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$とする。$\bm{y}_1, \cdots, \bm{y}_s \in W$が線形独立であるとすれば、$s \leq r$である。}\label{lemm:basisReplacing}

{\bf 証明}

$\{i_1, \cdots, i_{s}\} \subset \{1, \cdots, t\}$をとなる$\{i_1, \cdots, i_{s}\}$とってくる。($\{i_1, \cdots, i_s\}$は、この時点では順不同である。) Wを張る$\bm{x}_1, \cdots, \bm{x}_t$のうち、$\bm{x}_{i_1}, \cdots, \bm{x}_{i_s}$を$\bm{y}_1, \cdots, \bm{y}_s$に置き換える、つまり各$\bm{y}_i$は

$$
\bm{y}_i = \sum_{1 \leq j \leq s} a_j\bm{x}_{i_j}
$$

とかけることを帰納的に示す。まず、$s = 0$のときは自明である。$s \geq 1$として、$\bm{y}_1, \cdots, \bm{y}_{s-1}$に対してこの主張が言えたとする。つまり$\{i_1, \cdots, i_{s-1}\} \subset \{1, \cdots, t\}$をとってきて、$\bm{x}_1, \cdots, \bm{x}_t$の中の$\bm{x}_{i_1}, \cdots, \bm{x}_{i_{s-1}}$が$\bm{y}_1, \cdots, \bm{y}_{s-1}$に替えられたとする。これを、

\begin{eqnarray*}
  x_i' = \left\{
    \begin{array}{l}
      \bm{y}_k \quad i = i_k (1 \leq k \leq s -1) \\
      \bm{x}_i \quad otherwise
    \end{array}
  \right.
\end{eqnarray*}

とおく。このとき、$W = \langle \bm{x}_1', \cdots, \bm{x}_t' \rangle, \bm{y}_s \in W$であるから、$\bm{y}_s = a_1\bm{x}_1' + \cdots + a_t\bm{x}_t'$と表される。しかし、補題\ref{lemm:linearlyIndependent}により$\bm{y}_s \notin \langle \bm{y}_1, \cdots, \bm{y}_{s-1} \rangle = \langle \bm{x}_{i_1}', \cdots, \bm{x}_{i_{s-1}}' \rangle$であるから、ある$i \notin \{i_1, \cdots, i_{s-1} \}$に対して$a_i = 0$である。そのような$i$の1つを$i_s$とする。そのとき、$\bm{x}_{i_s}' = \bm{x}_{i_s}$は、$\bm{x}_1', \cdots, \bm{x}_{i_{s-1}}', \bm{y}_s, \bm{x}_{i_{s + 1}}', \cdots, \bm{x}_t'$の線形結合で表される。よって、$x_{i_s}'$を$\bm{y}_s$で置き換えることができる。

この補題により$1$から$r = s$まで順番にこの置き換えを行うことで、次の定理が得られる。

\rem{$W$を$V = \vecSpace{n}$の部分空間とする。$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle = \langle \bm{y}_1, \cdots, \bm{y}_s \rangle$}で、$\bm{x}_1, \cdots, \bm{x}_r$と$\bm{y}_1, \cdots, \bm{y}_s$がともに線形独立であるとすれば、$r = s$である。

\defi{$W$を$V = \vecSpace{n}$の部分空間とする。$W$に対し、次の条件を満たす順序付けられたベクトルの集合をWの{\bf 基底}(basis)という。}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$
\item $\bm{x}_1, \cdots, \bm{x}_r$は線形独立である。
\end{enumerate}

\rem{$V = \vecSpace{n}$の任意の部分空間$W$に対して基底${x_1, \cdots, x_r}$が存在する。基底をなすベクトルの個数は一定で、$r \leq n$となる}

{\bf 証明}

$W \subset V = \langle \bm{e}_1, \cdots, \bm{e}_n \rangle$であるから、補題\ref{lemm:basisReplacing}により$W$に含まれる線形独立なベクトルの個数は高々nである。$\bm{x}_1, \cdots, \bm{x}_r$をWに含まれる最大個数の線形独立なベクトルの集合とする。そのとき、$r \leq n$で任意の$\bm{x} \in W$に対し、$\bm{x} \in \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$が成立する。よって、$W \subset \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$である。$\langle \bm{x}_1, \cdots, \bm{x}_r \rangle \subset W$は明らかであるから、$W = \langle \bm{x}_1, \cdots, \bm{x}_r \rangle$。よって、$\{\bm{x}_1, \cdots, \bm{x}_r \}$は$W$の基底である。補題\ref{lemm:basisReplacing}により、rは一意的に定まる。

この定理により部分空間Wに対して定まる($r \in \mathbb{R}$)をWの{\bf 次元}(dimension)といい、$\dim W$と書く。

\rem{
$\{\bm{x}_1, \cdots, \bm{x}_n\}$を線形独立なベクトルの組とする。このとき、$\{\bm{x}_1, \cdots, \bm{x}_n\}$の線形結合としての表し方は一意的である。すなわち、$\bm{x} = a_1\bm{x}_1 + \cdots + a_n\bm{x}_n \quad (a_i \in \mathbb{R})$において、$a_i$は$\bm{x}$に対して一意的に定まる。
}\label{rem:VectorExpressionUniquness}

{\bf 証明}

$\bm{x}$の線形結合を$a_i$と$a_i'$を使って以下のように表せたとする。
\begin{eqnarray*}
\bm{x} = a_1\bm{x}_1 + \cdots + a_n\bm{x}_n \\
\bm{x} = a_1'\bm{x}_1 + \cdots + a_n'\bm{x}_n
\end{eqnarray*}

これらについて辺々引くと、

$$
0 = (a_1 - a_1')\bm{x}_1 + \cdots + (a_n - a_n')\bm{x}_n
$$

である。$\{\bm{x}_1, \cdots, \bm{x}_n\}$は線形独立なベクトルの組であるから、$a_1 - a_1' = \cdots = a_n - a_n' = 0$より、$a_1 = a_1', \cdots, a_n = a_n'$である。

\exam{
基本ベクトルの集合, $\langle \bm{e}_1, \cdots, \bm{e}_n \rangle$は明らかに$V = \vecSpace{n}$の基底であるから、$\dim V = n$である。これを$\vecSpace{n}$の{\bf 標準基底}という。
}

\exam{
$\{0\}$は線形独立なベクトルを含まないから、$\dim \{0\} = 0$である
}

\exam{  
$V = \vecSpace{n}$とする。$W, W' \subset V$のとき、$W \subset W', \dim W \leq \dim W'$である。
}

\rem{
$V = \vecSpace{n}$とする。$W, W' \subset V$とする。$W \subset W'$のとき、$\dim W = \dim W' \Longrightarrow W = W'$である。

{\bf 証明}

$r \leq n$とする。$W = \linearCombination{\bm{x}_1}{\bm{x}_n}, W' = \linearCombination{\bm{y}_1}{\bm{y}_r}$とする。$W \subset W', \dim W = \dim W'$であれば、$W$の基底$\vecSet{x}$と$W'$の基底$\{\bm{y}_1, \cdots, \bm{y}_r\}$の個数は一致するから、補題\ref{lemm:basisReplacing}により、

$$
\bm{y}_i = \sum_{1 \leq j \leq n} a_j\bm{x}_{j} \quad (a \in \mathbb{R})
$$

と表すことができる。これより、$W = \linearCombination{\bm{x}_1}{\bm{x}_n} = W' = \linearCombination{\bm{y}_1}{\bm{y}_r}$とすることができる。
}\label{rem:dimMatching}

\exercise{
$V = \vecSpace{3}$において、$V$の部分空間
$$
W = \left\{
\begin{pmatrix}
x \\
y \\
z
\end{pmatrix} \mid x + 2y + z = 0
\right\}
$$
の一組の基底を求めよ。
}

$\newline$
$x + 2y + z = 0$を$x$について解くと、$x = -2y - z$であるから、Wの元は

$$
\begin{pmatrix}
-2y -z \\
y \\
z
\end{pmatrix} = 
y \begin{pmatrix}
-2 \\
1 \\
0
\end{pmatrix} + 
z \begin{pmatrix}
-1 \\
0 \\
1
\end{pmatrix} 
$$

と表すことができる。
ここで、

$$
\begin{pmatrix}
-2 \\
1 \\
0
\end{pmatrix}, \begin{pmatrix}
-1 \\
0 \\
1
\end{pmatrix} 
$$

は線形独立であり、$\vecSpace{2}$を張ることから、基底である。

\subsection{基底の変換}

$V = \vecSpace{n}$の基底として、$\{\bm{a}_1, \cdots, \bm{a}_2\}$と$\{\bm{b}_1, \cdots, \bm{b}_n\}$があったとする。これら2組の基底が、

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} =
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}P
$$

というような関係式であれば、行列$P = (p_{ij}) \in M_n(\mathbb{R})$を$\{\bm{a}_1, \cdots, \bm{a}_2\}$から$\{\bm{b}_1, \cdots, \bm{b}_n\}$への{\bf 基底の変換行列}という。基底の変換行列は次のように求められる。

まず、それぞれの基底は$V$に属しているので、$\{\bm{a}_1, \cdots, \bm{a}_2\}$の各ベクトルは、$\{\bm{a}_1, \cdots, \bm{a}_2\}$の線形結合で表すことができる。

$$
\bm{b}_j = \sum_{1 \leq i \leq n} p_{ij}\bm{a}_j \quad (1 \leq j \leq n)
$$

である。これを書き直すと、

\begin{eqnarray*}
&\bm{b}_1 = P_{11}\bm{a}_1 + \cdots + P_{n1}\bm{a}_n = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{11} \\
\vdots \\
P_{n1}
\end{pmatrix} \\
&\vdots \\
&\bm{b}_n = P_{1n}\bm{a}_1 + \cdots + P_{nn}\bm{a}_n = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{1n} \\
\vdots \\
P_{nn}
\end{pmatrix}
\end{eqnarray*}

となり、これらをまとめると

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
\begin{pmatrix}
P_{11} & \cdots & P_{1n} \\
\vdots & \vdots & \vdots \\
P_{n1} & \cdots & P_{nn} \\
\end{pmatrix}
$$

だから、

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}P
$$

が得られた。この関係式から

$$
\begin{pmatrix}
\bm{b}_1 & \cdots & \bm{b}_n
\end{pmatrix}P^{-1} = 
\begin{pmatrix}
\bm{a}_1 & \cdots & \bm{a}_n
\end{pmatrix}
$$

も容易に導ける。

\exercise{$\vecSpace{3}$の２つの底を

\begin{eqnarray}
\left\{ 
\bm{a}_1 = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix},
\bm{a}_2 = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix},
\bm{a}_3 = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix} 
\right\} \\
\left\{
\bm{b}_1 = \begin{pmatrix}
1 \\
-1 \\
0
\end{pmatrix},
\bm{b}_2 = \begin{pmatrix}
1 \\
0 \\
-1
\end{pmatrix},
\bm{b}_3 = \begin{pmatrix}
1 \\
1 \\
1
\end{pmatrix} 
\right\}
\end{eqnarray}

とするとき、(10)から(11)への基底の変換行列を求めよ。
\newline
}\label{exercise:basisTransformation1}

基底の変換行列を$P$とすると、$\bm{b}_i$は$\bm{x}_i$によって以下のように表される。

\begin{eqnarray*}
\bm{b}_1 = P_{11}\bm{a}_1 + P_{21}\bm{a}_2 + P_{31}\bm{a}_3 \\
\bm{b}_2 = P_{12}\bm{a}_1 + P_{22}\bm{a}_2 + P_{32}\bm{a}_3 \\
\bm{b}_2 = P_{13}\bm{a}_1 + P_{23}\bm{a}_2 + P_{33}\bm{a}_3
\end{eqnarray*}

これらをそれぞれ連立一次方程式の形式で表し、それらを解くと

$$
P = \begin{pmatrix}
-\frac{2}{3} & \frac{4}{3} & -\frac{2}{3} \\[1.5ex]
-\frac{2}{3} & -\frac{2}{3} & \frac{4}{3} \\[1.5ex]
\frac{1}{3} & \frac{1}{3} & \frac{1}{3}
\end{pmatrix} 
$$

が得られる。

\section{線形写像}

線形写像とはベクトル空間からベクトル空間への（準同型）写像である。

\subsection{線形写像の定義}

\defi{
$V = \vecSpace{n}, V' = \vecSpace{m}$とする。$V$から$V$'への写像を$f$とし、以下の2つの性質を満たす時$f$は{\bf 線形}(一次)であるという。
}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $f(\bm{x} + \bm{y}) = f(\bm{x}) + f(\bm{y}) \quad (\bm{x}, \bm{y} \in V)$
\item $f(c\bm{x}) = cf(\bm{x}) \quad (\bm{x} \in V, c \in \mathbb{R})$
\end{enumerate}

これらの性質は$f$がベクトル空間上の加法とスカラー倍の2つの演算を保存することを意味する。
（つまり線形性とは、$f$が加法とスカラー倍において準同型であるということである。準同型は群の概念であるため、詳しい話は割愛する。詳しく知りたい方は、群のレポート: https://github.com/noppoMan/math-report/blob/master/group/group.pdfを読まれることを推奨する。)

特に、$V$から$V$自身への線形写像を$V$の{\bf 線形変換}(linear transformation)または${\bf 一次変換}$という

\exam{
$V = \vecSpace{3}, V' = \vecSpace{2}, V'' = \mathbb{R}$とするとき写像

$$
\begin{array}{ccc}
V & \stackrel{f_1}{\longrightarrow}  & V' \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} & \longmapsto & \begin{pmatrix}
x_1 - x_2 \\
x_1 - x_3
\end{pmatrix}
\end{array}
$$

$\newline$

$$
\begin{array}{ccc}
V & \stackrel{f_2}{\longrightarrow}  & V'' \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} & \longmapsto & (x_1 + x_2 + x_3)
\end{array}
$$

$\newline$

などは線形である。$f_2$のように$\mathbb{R}$への線形写像を{\bf 線形汎関数}ともいう。
}

\exam{
$\vecSpace{2}$（平面ベクトル）の原点を中心とした時計、反時計周りに$\theta$だけ回転させるような写像は線形変換である。($\vecSpace{2}$から$\vecSpace{2}$への線形写像)

$$
f_3: \begin{pmatrix}
x \\
y
\end{pmatrix} 
\longmapsto
\begin{pmatrix}
cos\theta - sin\theta \\
sin\theta + cos\theta \\
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
= 
\begin{pmatrix}
x cos\theta - y sin\theta \\
x sin\theta + y cos\theta \\
\end{pmatrix}
$$
}

この線形変換は三角関数の加法定理から導かれる。

\begin{numcases}
  {}
  rcos(\alpha + \beta) = rcos\alpha cos\beta - sin\alpha sin\beta \\
  rsin(\alpha + \beta) = rsin\alpha cos\beta + cos\alpha sin\beta
\end{numcases}

変換前の点P$(x, y)$、変換後の点P'$(rcos(\alpha + \beta), rsin(\alpha, \beta))$とする。

Pの座標$(x, y)$を(9), (10)にそれぞれ代入して、

\begin{eqnarray*}
rcos(\alpha + \beta) = xcos\beta - ysin\beta \\
rsin(\alpha + \beta) = ycos\beta + xsin\beta
\end{eqnarray*}

したがって、これを行列表示すると

$$
\begin{pmatrix}
x cos\beta & -y sin\beta \\
x sin\beta & y cos\beta \\
\end{pmatrix}
$$

となる。

\subsection{線形写像と行列の対応}

一般に$A = (a_{ij}) \in M_{m,n}(\mathbb{R})$が与えられた時、$V = \vecSpace{n}, V' = \vecSpace{m}$への写像$f_A$を

$$
f_A = \bm{x} = \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix} 
\longmapsto 
A\bm{x} = 
\begin{pmatrix}
{\displaystyle \sum_{1 \leq j \leq n} a_{1j}x_j} \\
\vdots \\
{\displaystyle \sum_{1 \leq j \leq n} a_{mj}x_j}
\end{pmatrix}
$$

によって定義すれば、$f_A$は線形である。つまり、

\begin{eqnarray*}
f_A(\bm{x} + \bm{y}) = A(\bm{x} + \bm{y}) = A\bm{x} + A\bm{y} \quad (\bm{x}, \bm{y} \in V) \\
f_A(c\bm{x}) = A(c\bm{x}) = cA\bm{x} \quad (c \in \mathbb{R}, \bm{x} \in V)
\end{eqnarray*}

が成立する。

したがって、上で挙げた例に登場した$f_1, f_2, f_3$はそれぞれ行列

$$
A_1 = \begin{pmatrix}
1 & -1 & 0 \\
0 & 1 & -1
\end{pmatrix} \in M_{2,3}\mathbb{R}, \quad
A_2 = (1, 1, 1) \in M_{1,3}\mathbb{R}, \quad
A_3 = \begin{pmatrix}
cos\theta & -sin\theta \\
sin\theta & cos\theta
\end{pmatrix}
$$

に対応する線形写像になっている。

\subsubsection{表現行列}

$V = \vecSpace{n}, V = \vecSpace{m}$とする。$f: V \to V'$を線形写像とする。$\{\bm{e}_1, \cdots, \bm{e}_n\}, \{\bm{e}_1', \cdots, \bm{e}_n'\}$をそれぞれ$V, V'$の標準基底とし、

$$
f(\bm{e}_j) = \sum_{1 \leq i \leq n} a_{ij}\bm{e}_{i}'
$$

とする。$f(\bm{e}_j) = \bm{a}_j$とすれば、この関係式は以下の写像$f$

$$
\begin{array}{ccc}
V & \stackrel{f}{\longrightarrow}  & V \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\bm{e}_1 & \longmapsto & \bm{a}_1 \\
\bm{e}_2 & \longmapsto & \bm{a}_2 \\
\vdots & & \vdots \\
\bm{e}_n & \longmapsto & \bm{a}_n
\end{array}
$$

を定義する。つまり、

$$
f(\bm{e}_1) = \bm{a}_1 = \begin{pmatrix}
a_{11} \\
\vdots \\
a_{m1}
\end{pmatrix}, 
f(\bm{e}_2) = \bm{a}_2 = \begin{pmatrix}
a_{12} \\
\vdots \\
a_{m2}
\end{pmatrix}, 
\cdots,
f(\bm{e}_n) = \bm{a}_n = \begin{pmatrix}
a_{n1} \\
\vdots \\
a_{mn}
\end{pmatrix}
$$

と$n$個のベクトルが得られる。これを並べると、

$$
A = (\bm{a}_1, \cdots, \bm{a}_n) = 
\begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots \\
a_{m1} & \cdots & a_{mn} \\
\end{pmatrix}
$$

のような行列$A$が得られる。つまり、
$$
f(\bm{e}_1) = A\bm{e}_1, f(\bm{e}_2) = A\bm{e}_2, \cdots, f(\bm{e}_n) = A\bm{e}_n
$$
である。

ここで、$V$の任意のベクトル$\bm{x}$は
$$
\bm{x} = \sum_{1 \leq j \leq n} x_{j}\bm{e}_j = x_1\bm{e}_1 + \cdots x_n\bm{e}_n
$$で表すことができることを思い出してほしい。これらを利用すると以下の関係式が得られる。

\begin{eqnarray*}
f(\bm{x}) &= &f(\sum_{1 \leq j \leq n} x_{j}\bm{e}_j)  \\
&= &f(x_1\bm{e}_1 + \cdots x_n\bm{e}_n) \\
&= &x_1f(\bm{e}_1) + \cdots + x_nf(\bm{e}_n) \\
&= &x_1A\bm{e}_1 + \cdots + x_nA\bm{e}_n \\
&= &Ax_1\bm{e}_1 + \cdots + Ax_n\bm{e}_n \\
&= &A(x_1\bm{e}_1 + \cdots + x_n\bm{e}_n) \\
&= &A\bm{x}
\end{eqnarray*}

つまり、$f$は$A = (a_ij)$に対応する線形写像$f_A$と一致するということである。

\defi{
$V$から$V'$への線形写像を$f$とする。$V$の基底$\{\bm{e}_1, \cdots, \bm{e}_n\}$, V'の基底$\{\bm{e}_1', \cdots, \bm{e}_n'\}$に関して、
$$
(f(\bm{e}_1), f(\bm{e}_2), \cdots, f(\bm{e}_n)) = (\bm{e}_1', \bm{e}_2', \cdots, \bm{e}_n')A
$$

の形に表される時、行列$A$を$f$の{\bf 表現行列}と呼ぶ。
}

\prop{表現行列は一意的である}

{\bf 証明}

定理\ref{rem:VectorExpressionUniquness}より明らか。表現行列を表すベクトル$a_1, \cdots, a_n$はすべて一意的に表せるので、表現行列も一意的である。

\exercise{
$\vecSpace{2}$のベクトル$\bm{x}, \bm{y}$と線形変換$f$の標準基底での表現行列$A$を
$$
\bm{x} = \begin{pmatrix}
2 \\
5
\end{pmatrix}, 
\bm{y} = \begin{pmatrix}
1 \\
3
\end{pmatrix}, 
A = \begin{pmatrix}
-3 & 1 \\
2 & -2
\end{pmatrix}
$$

とする。この線形変換$f$で$f(a\bm{x} + b\bm{y}) = a'\bm{x} + b'\bm{y}$となるとき、

$$
B\begin{pmatrix}
a \\
b
\end{pmatrix} = 
\begin{pmatrix}
a' \\
b'
\end{pmatrix}
$$
となる行列を求めよ。
}

$\bm{x}, \bm{y}$は$\vecSpace{2}$を張る標準基底${\bm{e}_1, \bm{e}_2}$で

$$
\begin{pmatrix} 
\bm{x} & \bm{y} 
\end{pmatrix} = 
\begin{pmatrix} 
\bm{e}_1 & \bm{e}_2
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix}
$$
と表すことができる。

$$
P = \begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix}
$$とする。


与式$f(a\bm{x} + b\bm{y}) = a'\bm{x} + b'\bm{y}$より、

$$
f(a\bm{x} + b\bm{y}) = A\begin{pmatrix} \bm{x} & \bm{y} \end{pmatrix}
\begin{pmatrix}
a \\
b
\end{pmatrix} = 
AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

また、

$$
a'\bm{x} + b'\bm{y} = 
\begin{pmatrix}
\bm{x} & \bm{y}
\end{pmatrix}
\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P\begin{pmatrix}
a' \\
b'
\end{pmatrix}
$$

の関係式が得られる。これより、

$$
P\begin{pmatrix}
a' \\
b'
\end{pmatrix} = 
AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

である。これに左から$P^{-1}$を掛けて（ここでは、$P$が可逆であることを前提に解答を進める。可逆の判定は行列式の章で解説する。）

$$
P^{-1}P\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P^{-1}AP\begin{pmatrix}
a \\
b
\end{pmatrix}
\Longleftrightarrow
\begin{pmatrix}
a' \\
b'
\end{pmatrix} =
P^{-1}AP\begin{pmatrix}
a \\
b
\end{pmatrix}
$$

つまり、

$$
P^{-1}AP = 
\frac{1}{2 \cdot 3 -5 \cdot 1}\begin{pmatrix}
3 & -1 \\
-5 & 2
\end{pmatrix}
\begin{pmatrix}
-3 & -1 \\
2 & -2
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
5 & 3
\end{pmatrix} = 
\begin{pmatrix}
3 & 4 \\
-7 & -8
\end{pmatrix}
$$

この例題より以下の定理が得られる。

\rem{
標準基底での$\vecSpace{n}$上の線形変換$f$の表現行列を$A$とする。標準基底$\{\bm{e}_1, \cdots, \bm{e}_n\}$を、基底$\{\bm{x}_1, \cdots, \bm{x}_n\}$に取り替えたとする。
基底の変換行列をPとおくと、基底$\{\bm{x}_1, \cdots,  \bm{x}_n\}$を座標系とした新座標での線形変換$f$の表現行列は$P^{-1}AP$である。
}

\defi{
実数係数を持つ$x$の多項式全体の集合も、通常の加法・スカラー倍に関してベクトル空間である。これを

$$
\polynomialVecSet{n} = \{ a_nx^n + a_{n-1} + \cdots + a_1x + a_0 \mid a_i \in \mathbb{R} (1 \leq i \leq n)\}
$$
と表記する。
}

\exercise{
$\polynomialVecSet{4} = \{a_4x^4 + a_3x^3 + a_2x^2 + a_1x + a_0 \mid a_i \in \mathbb{R}\}$を実数係数の4次以下の多項式全体とする。線形写像$F: \polynomialVecSet{4} \to \polynomialVecSet{4}$を

$$
F(f) = \frac{df(\bm{x})}{dx}
$$
とするとき、基底$\{1, x, x^2, x^3, x^4\}$に関する$F$の表現行列を求めよ。
}

$\newline$

$$
\bm{a} = \begin{pmatrix}
a_1 \\
a_2 \\
a_3 \\
a_4 \\
a_5
\end{pmatrix}
$$を$\vecSpace{5}$の標準基底とする。写像$g: \vecSpace{5} \to \polynomialVecSet{4}$を

$$
g(\bm{a}) = a_1 + a_2 \cdot x + a_3 \cdot x^2 + a_4 \cdot x^3 + a_5 \cdot x^4
$$

と定めると、これは$\vecSpace{5}$の標準基底に$\polynomialVecSet{4}$の基底$\{1, x, x^2, x^3, x^4\}$を対応させる線形写像である。線形写像$g$に対し、$F$を適用すると

\begin{eqnarray*}
F(g(\bm{a})) &= &F(a_1 + a_2 \cdot x + a_3 \cdot x^2 + a_4 \cdot x^3 + a_5 \cdot x^4) \\
&= &a_1F(1) + a_2F(x) + a_3F(x^2) + a_4F(x^3) + a_5F(x^4) \\
&= &a_1 \cdot 0 + a_2 \cdot 1 + a_3 \cdot 2x + a_4 \cdot 3x^2 + a_5 \cdot 4x^3
\end{eqnarray*}

となる。この結果を書き直すと

$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 0 & 4 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
\begin{pmatrix}
1 & x & x^2 & x^3 & x^4
\end{pmatrix}\bm{a}
$$

となっているので、Fの表現行列

$$
\begin{pmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 0 & 4 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix}
$$

が得られる。この表現行列より、

$$
(F(1), F(x), F(x^2), F(x^3), F(x^4)) = (1, x, x^2, x^3, x^4)F
$$

の関係式が成立する。


\subsection{線形写像の合成と行列の積}

線形写像$f_A$と$f_B$があるとき、$f_A \circ f_B = f_{AB}$が成立する。実際、任意の$\bm{x} \in \vecSpace{n}$に対して

$$
f_A \circ f_B(\bm{x}) = f_A(f_B(\bm{x})) = A(B\bm{x}) = (AB)\bm{x} = f_{AB}(\bm{x})
$$

であるから、$f_A \circ f_B = f_{AB}$が得られた。

この関係式を具体的な線形写像$f_A, f_B$を使って表すと面白いことが分かる。

$$
A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix}, 
B = \begin{pmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{pmatrix}
$$

を表現行列とし、線形写像$f_A$と$f_B$の合成を考える。

\begin{eqnarray*}
f_A \circ f_B(\bm{x}) = f_A(f_B(\bm{x})) &= &f_A(\begin{pmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{pmatrix} \begin{pmatrix}
x \\
y
\end{pmatrix}) \\
&= &f_A(\begin{pmatrix}
xb_{11} + yb_{12} \\
xb_{21} + yb_{22}
\end{pmatrix}) \\
&= &\begin{pmatrix}
a_{11} + a_{12} \\
a_{21} + a_{22}
\end{pmatrix} 
\begin{pmatrix}
xb_{11} + yb_{12} \\
xb_{21} + yb_{22}
\end{pmatrix} \\
& = &\begin{pmatrix}
a_{11}(xb_{11} + yb_{12}) + a_{12}(xb_{21} + yb_{22}) \\
a_{21}(xb_{11} + yb_{12}) + a_{22}(xb_{21} + yb_{22}) \\
\end{pmatrix} \\
& = &\begin{pmatrix}
(a_{11}b_{11} + a_{12}b_{21})x + (a_{11}b_{12} + a_{12}b_{22})y \\
(a_{21}b_{11} + a_{22}b_{21})x + (a_{21}b_{12} + a_{22}b_{22})y
\end{pmatrix} \\
& = &\begin{pmatrix}
a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix} \\
&= &(AB)\bm{x} \\
\end{eqnarray*}

が得られる。つまり、行列の積は線形写像の合成に等しいということが分かったのである。

したがって、行列の積は線形写像の合成に対応するように定義されていたのだ。

\subsection{線形写像の核と像}

$V = \vecSpace{n}, V' = \vecSpace{m}$とし、線形写像$f: V \to V'$が与えられたとする。
$f$によって、$V'$のベクトル$0_{V'}$に移される$V$のベクトル全体の集合

$$
\ker(f) = \{\bm{x} \in V \mid f(\bm{x}) = \bm{0}\}
$$

を$f$の{\bf 核}(Kernel, カーネル)という。また、$V$の$f$による像全体の集合

$$
\img(f) = \{f(\bm{x}) \mid \bm{x} \in V\}
$$

を$f$の{\bf 像}(Image, イメージ)という。

この定義から、$f$が全射$\Longleftrightarrow \img(f) = V'$である。また、$f$が単射であれば、$\ker(f) = \{0\}$である。これを証明する。

$f$が単射であるとする。$\forall \bm{x} \in \ker(f)$をとると、$f(\bm{x}) = \bm{0}$.ここで、$f$は線形写像より、$f(\bm{0}) = \bm{0}$.よって、$f(\bm{x}) = \bm{0}$ .仮定より、$f$は単射なので$\bm{x} = \bm{0}$.よって、$\ker(f) = \{0\}$となる。

逆に、$f(\bm{x}) = f(\bm{y})$となる$\forall \bm{x}, \bm{y} \in V$に対し、$f(\bm{x}) = f(\bm{y}) = 0$。ここで$f$は線形写像なので、$f(\bm{x} - \bm{y}) = \bm{0}$が言え、$\bm{x} - \bm{y} \in \ker(f) = \{0\}$である。したがって、$\bm{x} - \bm{y} = \bm{0}$より、$\bm{x} = \bm{y}$.

\rem{
$\img(f), \ker(f)$はそれぞれ$V', V$の部分空間である。
}

{\bf 証明}

$\img(f) \subset V'$を示す。$\bm{x}, \bm{y} \in \img(f)$とすれば、ある$\bm{x}, \bm{y} \in V$があって、$\bm{x}' = f(\bm{x}), \bm{y}' = f(\bm{y})$と表せる。よって、

\begin{eqnarray*}
\bm{x}' + \bm{y}' = f(\bm{x}) + f(\bm{y}) = f(\bm{x} + \bm{y}) \in \img(f) \\
c\bm{x}' = cf(\bm{x}) = f(c\bm{x}) \in \img(f)
\end{eqnarray*}

より、$\img(f)$は$V'$の部分空間である。

$\ker(f) \subset V$を示す。$\bm{x}, \bm{y} \in Ker(f)$とすれば、$f(\bm{x}) = f(\bm{y}) = 0$であるから

\begin{eqnarray*}
f(\bm{x} + \bm{y}) = f(\bm{x}) + f(\bm{y}) = \bm{0} \in Ker(f) \\
cf(\bm{x}) = f(c\bm{x}) = \bm{0} \in Ker(f)
\end{eqnarray*}

より、$\ker(f)$は$V$の部分空間である。

\subsection{次元定理}

\rem{
$V = \vecSpace{n}, \dim V = n$のとき、次の等式が成立する。

$$
\dim \img(f) = n - \dim \ker(f)
$$

{\bf 証明}

$\dim \ker(f) = s$とし、$\{\bm{x}_1, \cdots, \bm{x}_s\}$を$\ker(f)$の1つの基底とする。$s \leq n$で$V$の元$\bm{x}_{s + 1}, \cdots, \bm{x}_n$を適当に選び、$\vecSet{x}$を$V$の基底とすることができる。このとき、$f(\bm{x}_i) = \bm{0} \quad (1 \leq i \leq s)$であるから、

$$
\img(f) = \linearCombination{f(\bm{x}_1)}{f(\bm{x}_n)} = \linearCombination{f(\bm{x}_{s + 1})}{f(\bm{x}_n)}
$$

である。$f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$が線形独立であれば、このベクトルの組が$\img(f)$の基底となり、$\dim \img(f) = n - \dim \ker(f)$が証明される。$f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$が線形独立であることを示す。

$$
\sum_{s + 1 \leq j \leq n} c_jf(\bm{x}_j) = \bm{0} \quad (c_j \in \mathbb{R})
$$
と仮定すると、$f$は線形より、

$$
f(\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j) = \bm{0}
$$

であるから、

$$
\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j \in \ker(f)
$$

したがって、

$$
\sum_{s + 1 \leq j \leq n} c_j\bm{x}_j = \sum_{1 \leq i \leq n} k_i\bm{x}_i \quad (k_i \in \mathbb{R})
$$

とかける。つまり、$\{\bm{x}_1, \cdots, \bm{x}_s\} \subset \vecSet{x}$であるが、$\{\bm{x}_1, \cdots, \bm{x}_s\}$は$f$により$\bm{0}$に潰れてしまうので、$s \leq n$であっても右辺と左辺の$\bm{0}$に移されないベクトルの個数は
一致するということを言っている。

ここで、$\bm{x}_1, \cdots, \bm{x}_n$は$V$の基底より、線形独立であるから、すべての$j$で$c_j = 0$、すべての$i$で$k_i = 0$でなければならない。よって、$f$によって移される新たな$\img(f)$の基底 $f(\bm{x}_{s + 1}), \cdots, f(\bm{x}_n)$も線形独立である。

また、この定理により次の等式が成立することが直ちに分かる。

$$
\dim V = \dim \ker(f) + \dim \img(f)
$$
}\label{rem:rankNullity}

$\newline$

\exercise{
$$
A = \begin{pmatrix}
-2 & 1 & 1 \\
1 & -2 & 1 \\
1 & 1 & -2 
\end{pmatrix}
$$
によって定義される線形写像$f: \bm{x} \mapsto A\bm{x} \quad (\bm{x} \in \vecSpace{3})$に対する$\ker(f)$と$Im(f_A)$とぞれぞれの次元を求めよ。
}\label{exercise:FindingTheImageAndKernel1}

$\ker(f)$とその次元を求める。$f$のカーネルは$f$に写すと$\bm{0}$になるベクトルの集合であるから、$A\bm{x} = \bm{0}$.
つまり

$$
\begin{cases}
-2x + y + z = 0 \\
x - 2y + z = 0 \\
x + y - 2z = 0
\end{cases}
$$

を意味する。この連立方程式の解は$x = y = z$であるから

$$
\ker(f) = \left\{ \bm{x} = 
\begin{pmatrix} 
x \\
y \\
z
\end{pmatrix}
\mid x = y = z
\right\}
$$

である。ここから基底を取り出すことを考える。$\bm{x}$の成分はすべて同じ値なので、いずれかの成分を固定する（例えば、$x$で$y,z$を置き換える）ことで

$$
\bm{x} = x\begin{pmatrix} 
1 \\
1 \\
1
\end{pmatrix}
$$

と表すことができる。この1つのベクトル$\bm{x}$は線形独立であり、カーネルを張る基底であるから、$\dim \ker(f) = 1$である。

次に、$\img(f)$とその次元を求める。次元定理を使えば、$\dim \img(f) = 2$となるはずであるから、

$$
\img(f) = \left\{ \bm{x} = 
\begin{pmatrix} 
x \\
y \\
z
\end{pmatrix}
\mid x + y + z = 0
\right\}
$$

と仮定する。ここで、$x$を$x = -y + (-)z$とおけば$y$と$z$で$\bm{x}$を表現できるから、
$$
\bm{x} = y\begin{pmatrix} 
-1 \\
1 \\
0
\end{pmatrix} + z\begin{pmatrix} 
-1 \\
0 \\
1
\end{pmatrix}
$$

より、2つの線形独立なベクトル
$$
\left\{
\begin{pmatrix} 
-1 \\
1 \\
0
\end{pmatrix}, 
\begin{pmatrix} 
-1 \\
0 \\
1
\end{pmatrix}
\right\}
$$

が得られる。これらは$\vecSpace{2}$を張る基底となるから、仮定は正しいことが証明された。


\subsection{行列の階数}

\defi{
線形写像$f: V \to V'$に対し、$\dim \img(f)$を$f$の{\bf 階数}（rank, ランク）といい、$\rank f$とかく。$\rank f = \dim \img(f)$である。

また、$A \in M_{m,n}(\mathbb{R})$に対してその階数を$\rank A = \rank f_A = \dim A \vecSpace{n}$と定義する。したがって、$\rank A$は$A$の列ベクトルの中で線形独立なものの最大個数である。
}\label{defi:rankOfMatrix}

なお、$A \vecSpace{n}$の定義は次である。
$V = \vecSpace{n}$とする。$f = A$であることに着目すると
$$
Im(f) = f(V) = \{f(\bm{x}) \mid \bm{x} \in V\} = AV = \{A\bm{x} \mid \bm{x} \in V\} \subset V'
$$

\exam{
例題\ref{exercise:FindingTheImageAndKernel1}の行列$A$のランクは$\rank A = 2$である。
実際、定義\ref{defi:rankOfMatrix}に従えば、$\dim \img(f) = 2$なので、$\rank f = \dim \img(f)$である。
}

\exam{
$A = \bm{x}{}^t\bm{y}, \bm{x} \in \vecSpace{m}, \bm{y} \in \vecSpace{n}, \bm{x}, \bm{y} \neq \bm{0}$とすれば、$\rank A = 1$である。

$\bm{y} = (y_i)$とすれば、$A$の列ベクトルは$y_1\bm{x}, \cdots, y_n\bm{x}$であるから、

$$
\rank A = \dim \langle y_1\bm{x}, \cdots, y_n\bm{x} \rangle = \dim \langle \bm{x} \rangle = 1
$$
である。
}

\rem{
$A \in M_{m,n}(\mathbb{R}), f_A: V \to V'$とすれば、
\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $\rank A \leq \min(m, n)$
\item $\rank A = m \Longleftrightarrow f$は全射 (このとき、$n \geq m$)
\item $\rank A = n \Longleftrightarrow f$は単射 (このとき、$n \leq m$)
\end{enumerate}

{\bf 証明}

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $AV \subset V'$であるから、$\dim AV \leq \dim V' = m$.よって、$\rank A \leq m$である。しかし、$\dim AV = n - \dim \ker(f)$より、$\dim AV \leq n$.よって、$\rank A \leq \min(m, n)$

\item $\rank A = m$ならば、
$$
AV \subset V', \dim AV = \dim V'
$$
であるから、定理\ref{rem:dimMatching}より$AV = V'$となり、$f_A$は全射である。逆に$f_A$が全射であるとすれば、$n \geq m$より、$\rank A = m$は明らか。

\item $\rank A = n$ならば、定理\ref{rem:rankNullity}より

$$
\dim \ker(f) = 0
$$

したがって、$\ker(f) = \{0\}$. つまり、$f_A$は単射である。逆に$f_A$が単射とする。$n \leq m$より、$\rank A = n$は明らか。

\end{enumerate}
}\label{rem:dimentionRule}

\exam{
$\rank A = m$で$f$は全射の例

$f: V \to V'$を以下の写像とすれば、$f$は全射である。

$$
A = \begin{pmatrix} 
1 & 2 & 3 \\
4 & 5 & 6
\end{pmatrix} \in M_{2, 3}(\mathbb{R})
$$

$$
\begin{array}{ccc}
\vecSpace{3} & \stackrel{f = A}{\longrightarrow} & \vecSpace{2} \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix} & \longmapsto & \begin{pmatrix}
x_1  + 2x_2 + 3x_3 \\
4x_1 + 5x_2 + 6x_3
\end{pmatrix}
\end{array}
$$
}

\exam{
$\rank A = n$ならば$f$は単射の例

$f: V \to V'$を以下の写像とすれば、$f$は単射である。

$$
A = \begin{pmatrix} 
1 & 4 \\
2 & 5 \\
3 & 6
\end{pmatrix} \in M_{3, 2}(\mathbb{R})
$$

$$
\begin{array}{ccc}
\vecSpace{2} & \stackrel{f = A}{\longrightarrow} & \vecSpace{3} \\
\rotatebox{90}{$\in$} & & \rotatebox{90}{$\in$} \\
\begin{pmatrix}
x_1 \\
x_2
\end{pmatrix} & \longmapsto & \begin{pmatrix}
x_1  + 4x_1 \\
2x_2 + 5x_2 \\
3(x_1 + x_2) + 6(x_1 + x_2)
\end{pmatrix}
\end{array}
$$
}

\subsection{基本変形}

具体的な行列の階数は基本変形を用いて求めることができる。ここでは基本変形の定義とそれを使った行列の階数の求め方を示す。

$A = (a_{ij}) \in M_{m,n}(\mathbb{R})$とする。$A$に対する{\bf 基本変形}(初等変形)とは以下の3つの操作とその組み合わせのことである。


\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $A$のi行(列)を定数倍する。
$$
A \longrightarrow cA = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
ca_{i1} & ca_{i2} & \cdots & ca_{in} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix} \quad (c \in \mathbb{R})
$$

の変換を指す。

\item $A$の第$i$行(列)と第$j$行(列)を入れ替える。

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{i1} & a_{i2} & \cdots & a_{in} \\
\vdots & \vdots & \vdots & \vdots \\
a_{j1} & a_{j2} & \cdots & a_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix} \\
\longrightarrow \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{j1} & a_{j2} & \cdots & a_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{i1} & a_{i2} & \cdots & a_{in} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix}
$$

の変換を指す。

\item $A$の第$i$行(列)に第$j$行(列)の定数倍を加える。

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{i1} & a_{i2} & \cdots & a_{in} \\
\vdots & \vdots & \vdots & \vdots \\
a_{j1} & a_{j2} & \cdots & a_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix} \\
\longrightarrow \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{i1} + ca_{j1} & a_{i2} + ca_{j2} & \cdots & a_{in} + ca_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{j1} & a_{j2} & \cdots & a_{jn} \\
\vdots & \vdots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{pmatrix} \quad (c \in \mathbb{R})
$$

の変換を指す。
\end{enumerate}

行に関する基本変形を行基本変形、列に関する基本変形を列基本変形という。

この基本変形を適当な組み合わせで繰り返し$A$に適用することで、$A$を標準形

$$
\begin{pmatrix}
E_r & 0 \\
0 & 0
\end{pmatrix}
$$


(ここで、$E_r$は$A$の階数における単位行列である)に変換することができる。$E_r$が階数を表すので、標準形に変換する行為は行列の階数を求めることにほかならない。

% $\newline$

% また、この操作を列に適用することを列基本変形という。

\exam{
行列$A = \begin{pmatrix}
0 & 1 & 2 \\
-1 & 0 & 3 \\
-2 & -3 & 0
\end{pmatrix}$の階数を基本変形を用いて求める方法は以下である。


\begin{eqnarray*}
\begin{pmatrix}
0 & 1 & 2 \\
-1 & 0 & 3 \\
-2 & -3 & 0
\end{pmatrix} &\xrightarrow{第1列と第2列を入れ替え} &\begin{pmatrix}
1 & 0 & 2 \\
0 & -1 & 3 \\
-3 & -2 & 0
\end{pmatrix} \\[1ex]
&\xrightarrow{第1行を3倍して第3行に加える} &\begin{pmatrix}
1 & 0 & 2 \\
0 & -1 & 3 \\
0 & -2 & 6
\end{pmatrix} \xrightarrow{第1列を-2倍して第3列に加える} \begin{pmatrix}
1 & 0 & 0 \\
0 & -1 & 3 \\
0 & -2 & 6
\end{pmatrix} \\[1ex]
&\xrightarrow{第2行に-1倍する} &\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & -3 \\
0 & -2 & 6
\end{pmatrix} \xrightarrow{第2行を-2倍し、第3行に加える} \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & -3 \\
0 & 0 & 0
\end{pmatrix}
\end{eqnarray*}

この基本変形より、$\rank A = 2$である。

この例より、行列の基本変形は連立方程式の変形であることが分かる。よって、連立方程式の変形の際に出来ないような操作（e.g.行基本変形と列基本変形が同時に行われる）は行基本変形においても行うことが出来ない。
}

\section{行列式}

\subsection{2次行列における行列式}

\defi{2次行列における行列式を次のように定義する。}

$$
A = \begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
$$
に対して、

$$
\det(A) = \abs{A} = \begin{vmatrix}
a & b \\
c & d \\
\end{vmatrix} = ad - bc
$$

である。また、Aの列ベクトル

$$
\bm{a}_1 = \begin{pmatrix}
a \\
c
\end{pmatrix},
\bm{a}_2 = \begin{pmatrix}
b \\
d
\end{pmatrix}
$$に対して、

$$
det(A) = D(\bm{a}_1, \bm{a}_2)
$$
とする。

\subsubsection{行列式の性質}

上で定義した行列式$D$を$V = \vecSpace{2}$上の2変数関数と考えると、次の性質を持つ。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item {\bf $\bm{a}_1, \bm{a}_2$について線形(多重線形性)}

$\bm{a}_1$について

\begin{eqnarray*}
&D(\bm{a}_1 + \bm{a}_1', \bm{a}_2) = D(\bm{a}_1, \bm{a}_2) + D(\bm{a}_1',  \bm{a}_2) \\
&D(c\bm{a}_1, \bm{a}_2) = cD(\bm{a}_1, \bm{a}_2) \quad (c \in \mathbb{R})
\end{eqnarray*}

$\bm{a}_2$について

\begin{eqnarray*}
&D(\bm{a}_1, \bm{a}_2 + \bm{a}_2') = D(\bm{a}_1, \bm{a}_2) + D(\bm{a}_1,  \bm{a}_2') \\
&D(\bm{a}_1, c\bm{a}_2) = cD(\bm{a}_1, \bm{a}_2) \quad (c \in \mathbb{R})
\end{eqnarray*}


\item {\bf $\bm{a}_1, \bm{a}_2$について交代的}

$$
D(\bm{a}_2, \bm{a}_1) = -D(\bm{a}_1, \bm{a}_2)
$$

これは以下のように展開できる。

$$
D(\bm{a}_2, \bm{a}_1) = \begin{vmatrix}
b & a \\
d & c
\end{vmatrix} = bc - ad = -D(\bm{a}_1, \bm{a}_2)
$$

\item $\vecSpace{2}$の単位ベクトルを$\bm{e}_1, \bm{e}_2$とすれば、

$$
D(\bm{e}_1, \bm{e}_2) = 1
$$

これは以下のように展開できる。

$$
D(\bm{e}_1, \bm{e}_2) = \begin{vmatrix}
1 & 0 \\
0 & 1
\end{vmatrix} = 1
$$

\end{enumerate}

この性質から2次行列式はこれらの性質によって導くことができる。$\vecSpace{2}$上の2変数関数$D(\bm{a}_1, \bm{a}_2)$が(1), (2), (3)の性質を持つとすると、(1)より

\begin{eqnarray*}
D(\bm{a}_1, \bm{a}_2) &= &D(a\bm{e_1} + c\bm{e_2}, b\bm{e_1} + d\bm{e_2}) \\
&= &aD(\bm{e_1}, b\bm{e_1} + d \bm{e}_2) + cD(\bm{e_2}, b\bm{e_1} + d\bm{e_2}) \\
&= &abD(\bm{e}_1, \bm{e}_1) + adD(\bm{e}_1, b\bm{e}_2) + cbD(\bm{e}_2, \bm{e}_1) + cdD(\bm{e}_2, \bm{e}_2)
\end{eqnarray*}

(2), (3)より

$$
D(\bm{e}_1, \bm{e}_2) = 1, \quad D(\bm{e}_2, \bm{e}_1) = -D(\bm{e}_1, \bm{e}_2) = -1
$$

$D(\bm{e}_1, \bm{e}_1) = -D(\bm{e}_1, \bm{e}_1)$であるから、$D(\bm{e}_1, \bm{e}_1) = 0$.同様に、$D(\bm{e}_2, \bm{e}_2) = 0$となる。

したがって、(1)で得た関係式に代入すると

\begin{equation*}
\begin{split}
D(\bm{a}_1, \bm{a}_2) &= abD(\bm{e}_1, \bm{e}_1) + adD(\bm{e}_1, b\bm{e}_2) + cbD(\bm{e}_2, \bm{e}_1) + cdD(\bm{e}_2, \bm{e}_2) \\
&= ab \cdot 0 + ad \cdot -1 + cb \cdot + 1 + cd \cdot 0 \\
&= ad - bc
\end{split}
\end{equation*}

である。

\subsection{置換}

前節で、2変数の関数$D(\bm{a}_1, \bm{a}_2)$を紹介したが、一般には2次行列式の性質(1),(2),(3)と類似の性質を持つn変数の関数
$$
D(\bm{a}_1, \bm{a}_2, \cdots, \bm{a}_n) \quad (\bm{a}_1, \bm{a}_2, \cdots, \bm{a}_n \in \vecSpace{n})
$$
が存在する。これをn次の行列式という。ここでは、このn次の行列式を導くために必要な置換について概説する。

n個の文字の集合

$$
X = \{1, 2, \cdots, n\}
$$

を考え、$X$のそれ自身への全単射

$$
\sigma: X \to X
$$

をXの{\bf 置換(permutation)}という。($X$は高々加算な有限集合として考える。)
このn文字の置換の集合を$S_n$とかく。任意の置換$\sigma \in S_n$が$1, 2, \cdots, n$から$i_1, i_2, \cdots, i_n$へ1対1で対応しているとき、つまり$\sigma(k) = i_k \quad (1 \leq k \leq n)$のとき

$$
\sigma = \begin{pmatrix}
1 & 2 & \cdots & n \\
i_1 & i_2 & \cdots &i_n
\end{pmatrix}
$$

と書く。特に、$i_1, i_2, \cdots, i_n$が自分自身であるような置換

$$
\sigma = \begin{pmatrix}
1 & 2 & \cdots & n \\
1 & 2 & \cdots & n \\
\end{pmatrix}
$$

を{\bf 恒等置換}という。これを単に1と表す。
$\newline$

置換に対し、$i,j$の2つの文字のみを入れ替え、その他の文字は不変にするような置換

$$
\sigma = \begin{pmatrix}
1 & \cdots & i & \cdots & j \cdots & n \\
1 & \cdots & j & \cdots & i \cdots & n \\
\end{pmatrix}
$$

を{\bf 互換(transposition)}といい、(i, j)とかく。具体的には$S_4$であれば、

$$
\sigma = \begin{pmatrix}
1 & 2 & 3 & 4 \\
1 & 3 & 2 & 4 \\
\end{pmatrix} = (2, 3)
$$
のように書き、不変な置換$\sigma(1) = 1, \sigma(4) = 4$は省略することができる。この書き方の理由として、
$\sigma(i) = j, \sigma(j) = i$と$i \to j, j \to i$のように$i$と$j$の対応のみが交換されており、その他は不変であることから、$i, j$の2つの文字だけで$\sigma$を表現するに十分だからである。
$\newline$

また、２つの置換$\sigma, \gamma \in S_n$に対し、その合成

$$
(\sigma\gamma)(k) = (\sigma(\gamma(k))
$$

によって定義される写像$\sigma\gamma$もまた１つの置換である。これを$\sigma, \gamma$の{\bf 積}という。また$\sigma \in S_n$は全単射であるから、その逆写像$\sigma^{-1}$が存在し、それもまた置換となる。これを$\sigma$の{\bf 逆置換}という。

\exam{置換の積の例}

$$
\gamma = \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix}, \sigma = \begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix}
$$
の置換の積を考えると、

$$
\gamma(\sigma(1)) = 1, \quad \gamma(\sigma(2)) = 3, \quad \gamma(\sigma(3)) = 2
$$

より、

\begin{equation*}
\begin{split}
\gamma\sigma &= \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix} \begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix}\\[1ex]
&= \begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix}\\[1ex]
&= \begin{pmatrix}
2 & 3 \\
3 & 2
\end{pmatrix}\\[1ex]
&= (2, 3)
\end{split}
\end{equation*}

となる。

\exercise{$S_3$のすべての置換を答えよ。}\label{exercise:permutation1}

{\bf 解答}

置換は自身から自身への対応の並び替えであるから、その総数は$n!$個である。よって、$S_3$では、$3!$個の置換がありそれは以下となる。

\begin{eqnarray*}
\begin{pmatrix}
1 & 2 & 3 \\
1 & 2 & 3
\end{pmatrix} = 1, \quad \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix}, \quad \begin{pmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{pmatrix}, \\[1.5ex]
\begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix} = (2, 3), \quad 
\begin{pmatrix}
1 & 2 & 3 \\
2 & 1 & 3
\end{pmatrix} = (1, 2), \quad 
\begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix} = (1, 3)
\end{eqnarray*}

\subsubsection{互換の積}

$\sigma \in S_3$を

$$
\sigma = \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix}
$$

とする。この置換はいくつかの{\bf 互換の積}で

$$
\sigma = (1, 3)(1, 2) = (1,2)(2,3)
$$

のように表すことができる。実際、


$$
(1, 2) = \begin{pmatrix}
1 & 2 & 3\\
2 & 1 & 3
\end{pmatrix}, \quad 
(2, 3) = \begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix}
$$

であるから$\gamma = (1, 2), \rho = (2, 3)$としてその合成$\gamma\rho$を考えると

\begin{eqnarray*}
\gamma(\rho(1)) = \gamma(1) = 2 \\
\gamma(\rho(2)) = \gamma(3) = 3 \\
\gamma(\rho(3)) = \gamma(3) = 1
\end{eqnarray*}

となることから

$$
\gamma\rho = \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix} = \sigma
$$

である。（互換の合成も写像の合成同様に、右側の置換（互換）から評価する。）

\exercise{
$
\begin{pmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{pmatrix}
$を互換の積で表わせ。
}

{\bf 解答}

この置換は

$$
\begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix} = (1, 3),\quad 
\begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix} = (2, 3)
$$
および

$$
\begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix} = (2, 3),\quad 
\begin{pmatrix}
1 & 2 & 3 \\
2 & 1 & 3
\end{pmatrix} = (1, 2)
$$

の互換の積で

$$
\begin{pmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{pmatrix} = (1,3)(2,3) = (2,3)(1,2)
$$のように表すことができる。
$\newline$

実は、任意の$\sigma \in S_n$は表示は一定でないにしろ、互換の個数の偶奇は一定であることが知られている。
それを示すために、次の補題を証明する。

\lemm{
$S_n$上に定義された関数$sign(\sigma)$が次の性質のもと一意的に存在する。

\begin{enumerate}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\item $sign(\sigma) \in \{\pm 1\}$
\item $\sigma$が互換ならば、$sign(\sigma) = -1$
\item $sign(\sigma\gamma) = sign(\sigma)sign(\gamma)$
\end{enumerate}

{\bf 証明}

このような関数は
$$
sign(\sigma) = \prod_{i \neq j} \frac{\sigma(i) - \sigma(j)}{i-j}
$$
によって与えられる。

$\sigma \in S_n$とする。$\sigma$を互換とする。$\sigma$は互換より、$i \neq j$のとき、
$\sigma(k) = k, \sigma(j) = i, \sigma(i) = j$. それ以外、つまり$i = j$ですべての$sign(k) = k$であるから、右辺の積は

$$
\frac{i - j}{i - j} = 1
$$

となる。これより、$i \neq j$のときに

$$
\frac{j - i}{i - j} = -1
$$

となることから、(2)を満たす。

したがって、$\sigma$が$k$個の互換の積$\tau_1, \cdots, \tau_k$の積に等しければ、関数$sign$は次のように展開できる。

$$
sign(\sigma) = sign(\tau_1) \cdots sign(\tau_k) = (-1)^k \quad (k \in \mathbb{N})
$$

これより(1)を満たす。（この関数はwell-definedであることが分かる。）
また、$(\tau_1, \cdots, \tau_k) = \sigma$より、$sign(\tau_1, \cdots, \tau_k) = sign(\tau_1) \cdots sign(\tau_k)$であるから、(3)を満たすことも確かめられる。

$sign(\sigma) = 1$のとき互換の数は偶数、$sign(\sigma) = -1$のとき互換の数は奇数である。kの偶奇により、$\sigma$を{\bf 偶置換}、{\bf 奇置換}という。
}\label{lemm:signFunction}
$\newline$

この補題より、次の定理が証明される。

\rem{
任意の$\sigma \in S_n$は高々$n-1$個の互換の積に表される。このような表示は一意的ではないが、互換の個数の偶奇は一定である。
}

第一の主張はnの帰納法で容易に証明され、第二の主張は補題\ref{lemm:signFunction}により証明される。
$\newline$

\exam{
$\begin{pmatrix}
1 & 2 & 3 \\
1 & 2 & 3
\end{pmatrix}, \begin{pmatrix}
1 & 2 & 3 \\
2 & 3 & 1
\end{pmatrix}, \begin{pmatrix}
1 & 2 & 3 \\
3 & 1 & 2
\end{pmatrix}$
は偶置換、
$\begin{pmatrix}
1 & 2 & 3 \\
1 & 3 & 2
\end{pmatrix}, \begin{pmatrix}
1 & 2 & 3 \\
2 & 1 & 3
\end{pmatrix}, \begin{pmatrix}
1 & 2 & 3 \\
3 & 2 & 1
\end{pmatrix}$は奇置換である。

このように$\sigma \in S_n$の遇奇の個数は一致する。（証明は割愛する）
}

\subsubsection{巡回置換}

相異なる文字$i_1, i_2, \cdots, i_k$を$i_1$を$i_2$に$i_2$を$i_3$に...と順次次の文字に移し、最後の$i_k$を$i_1$に戻すような（ループを描くような）置換を$i_1, i_2, \cdots, i_k$の巡回置換(cycle)といい、$(i_1, i_2, \cdots ,i_k)$とかく。互換はこの特別の場合である。よって、巡回置換は

$$
(i_1, i_2, \cdots ,i_k) = (i_1, i_k)(i_1, i_{k-1})\cdots(i_1, i_2)
$$
のように、互換の積に分解できる。したがって、

$$
sign(i_1, i_2, \cdots ,i_k) = (-1)^{k-1}
$$
である。

\subsection{n次行列式の定義と基本性質}

\rem{$V = \vecSpace{n}$上に次の性質(D1), (D2), (D3)をもつようなn変数の関数$D(\bm{a}_1, \cdots, \bm{a}_n)$が一意的に存在する。

この関数をn次の{\bf 行列式(determinant)}といい、$A = (\bm{a}_1, \cdots, \bm{a}_n)$であるとき、

$$
D(\bm{a}_1, \cdots, \bm{a}_n) = \det(A) = \abs{A} = \abs{\bm{a}_1, \cdots, \bm{a}_n}
$$
などと書く。

この定理の証明の前に、まずは性質(D1), (D2), (D3)を解説する。

\begin{enumerate}
\renewcommand{\labelenumi}{(D\arabic{enumi})}
\item $D(\bm{a}_1, \cdots, \bm{a}_n)$は各変数$\bm{a}_i$に関して線形である。
\begin{eqnarray*}
&D(\bm{a}_1 + \bm{a}_1`, \cdots, \bm{a}_n) = D(\bm{a}_1, \cdots, \bm{a}_n) + D(\bm{a}_1`, \cdots, \bm{a}_n), \\
&D(c\bm{a}_1, \cdots, \bm{a}_n) = cD(\bm{a}_1, \cdots, \bm{a}_n)
\end{eqnarray*}

\item ２つの変数$\bm{a}_i, \bm{a}_j \hspace{2pt} (i \neq j)$を入れ替えると、Dの符号が変わる。つまり、$\bm{a}_1, \cdots, \bm{a}_n$に関して交代的である。

$$
D(\cdots, \bm{a}_j, \cdots, \bm{a}_i, \cdots) = -D(\cdots, \bm{a}_i, \cdots, \bm{a}_j, \cdots)
$$

\item $\vecSpace{n}$の基本ベクトル$\bm{e}_1, \cdots, \bm{e}_n$に対して

$$
D(\bm{e}_1, \cdots, \bm{e}_n) = 1
$$

\end{enumerate}
}\label{rem:detDef}

また、(D2)は(D1)の下に、次の(D2`)と同値である。

(D2') $\quad$ $\bm{a}_i = \bm{a}_j \hspace{1pt} (i \neq j)$のとき、

$$
D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) = 0
$$

実際、(D2)を仮定すると

$$
D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) = -D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots)
$$

であるから、この主張が言える。

逆に(D2')を仮定すると、(D1)により$D(\cdots, \bm{a}_i, \cdots, \bm{a}_j, \cdots)$の第$i, j$番目の変数に$\bm{a}_i + \bm{a}_j$を代入して


\begin{eqnarray*}
D(\cdots, \bm{a}_i + \bm{a}_j, \cdots, \bm{a}_i + \bm{a}_j, \cdots) &= &D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) + D(\cdots, \bm{a}_i, \cdots, \bm{a}_j, \cdots) \\
&+ &D(\cdots, \bm{a}_j, \cdots, \bm{a}_i, \cdots) + D(\cdots, \bm{a}_j, \cdots, \bm{a}_j, \cdots) \\
&= &0 + 1 - 1 + 0 \\
&= &0
\end{eqnarray*}

(D2')により

$$
D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) = D(\cdots, \bm{a}_j, \cdots, \bm{a}_j, \cdots) = 0
$$

であるから、右辺の$\bm{a}_j, \bm{a}_j$を入れ替えても

$$
D(\cdots, \bm{a}_i, \cdots, \bm{a}_i, \cdots) = -D(\cdots, \bm{a}_j, \cdots, \bm{a}_j, \cdots)
$$

とでき、(D2)の交代式を得る。よって、(D2)と(D2')はこの条件のもと同値である。

$\newline$
これらの性質より、定理\ref{rem:detDef}は次のように証明される。
$\newline$

{\bf 定理\ref{rem:detDef}の証明}

(D1), (D2), (D3)の性質をもつ関数$D(\bm{a}_1, \cdots, \bm{a}_n) \quad (\bm{a}_j \in \vecSpace{n})$が存在すると仮定する。
また、$A = (\bm{a}_1, \cdots, \bm{a}_n) \in M_n(\mathbb{R})$とする。

$A$の各ベクトル$\bm{a}_j$は線形結合により

$$
\bm{a}_j = \sum_{1 \leq i \leq n} a_{ij}\bm{e}_i \quad (a_{ij} \in \mathbb{R})
$$

と表せる。(D1)より

\begin{equation}
D(\bm{a}_1, \cdots, \bm{a}_n) = \sum_{i_1, \cdots , i_n = 1}^{n} a_{{i_1}1} \cdots a_{{i_n}n}D(\bm{e}_{i_1}, \cdots, \bm{e}_{i_n})
\label{equation:Det1}
\end{equation}

ここで、$i_1, \cdots, i_n$は独立に1からnまで動く。(D2),(D3)より$D(\bm{e}_1, \cdots, \bm{e}_n)$は、$(i_1, \cdots, i_n)$が$(1, \cdots, n)$の順列になっているときには、置換$\sigma = \begin{pmatrix}
1 & \cdots & n \\
i_1 & \cdots & i_n
\end{pmatrix}$の符号に等しく、その他に$i_1, \cdots, i_n$の中に同じ数が含まれているときは$0$となる。したがって、

$$
D(\bm{e}_1, \cdots, \bm{e}_n) = \pm 1
$$

であるから、式(\ref{equation:Det1})は次のように表せる。

\begin{equation}
D(\bm{a}_1, \cdots, \bm{a}_n) = \sum_{\sigma \in S_n} sign(\sigma) \hspace{1pt} a_{\sigma(1)1} \cdots a_{\sigma(n)n}
\label{equation:Det2}
\end{equation}

このように、$D(\bm{a}_1, \cdots, \bm{a}_n)$は$a_{ij}$の多項式として明らかに一意的に定まる。($A$に対して$\sigma \in S_n$は$n!$個の組み合わせのみである。)

逆に、式(\ref{equation:Det1})によって$D(\bm{a}_1, \cdots, \bm{a}_n)$を定義すれば、それが(D1),(D2),(D3)の性質を持つことは、補題\ref{lemm:signFunction}で示したように$sign(\sigma)$の性質から確認できる。よって、行列式は式(\ref{equation:Det1})によって定義でき、それは一意的に存在する。

\rem{
$V = \vecSpace{n}$上の関数Fが定理\ref{rem:detDef}における$D$の性質(D1),(D2)をもてば、ある定数cがあるとき

$$
F(\bm{a}_1, \cdots, \bm{a}_n) = c\abs{A}
$$

ただし、$c = F(\bm{e}_1, \cdots, \bm{e}_n)$
}

\exam{
$$
A = \begin{pmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{pmatrix} \in M_3(\mathbb{R})
$$の行列式は以下の要領で得られる。

例題\ref{exercise:permutation1}で得られた置換$\sigma \in S_n$を左上から順に、$\sigma_1, \cdots, \sigma_6$とラベリングすると、

\begin{equation*}
\begin{split}
  \det(A) 
  ={}&
  sign(\sigma_1)\hspace{1pt}a_{\sigma_{1}(1)1}a_{\sigma_{1}(2)2}a_{\sigma_{1}(3)3}
  + sign(\sigma_2)\hspace{1pt}a_{\sigma_{2}(1)1}a_{\sigma_{2}(2)2}a_{\sigma_{2}(3)3}
  + sign(\sigma_3)\hspace{1pt}a_{\sigma_{3}(1)1}a_{\sigma_{3}(2)2}a_{\sigma_{3}(3)3}
  \\&+
  sign(\sigma_4) \hspace{1pt}a_{\sigma_{4}(1)1}a_{\sigma_{4}(2)2}a_{\sigma_{4}(3)3}
  + sign(\sigma_5)\hspace{1pt}a_{\sigma_{5}(1)1}a_{\sigma_{5}(2)2}a_{\sigma_{5}(3)3}
  + sign(\sigma_6)\hspace{1pt}a_{\sigma_{6}(1)1}a_{\sigma_{6}(2)2}a_{\sigma_{6}(3)3}
  \\={}&
  a_{11}a_{22}a_{33} + a_{21}a_{32}a_{13} + a_{31}a_{12}a_{23}
  \\&- a_{11}a_{32}a_{23} - a_{21}a_{12}a_{33} - a_{31}a_{22}a_{13}
\end{split}
\end{equation*}

が得られる。
}


\rem{
$A \in M_n(\mathbb{R})$の転置行列を$\transposeMat{A}$とすれば、

$$
\abs{\transposeMat{A}} = \abs{A}
$$

となる。

{\bf 証明}

$\transposeMat{A}$の列ベクトルを$\bm{a}'_1 \cdots, \bm{a}'_n$とすれば、$\transposeMat{\bm{a}'_1} \cdots, \transposeMat{\bm{a}'_n}$は$A$の行ベクトルである。よって、

$$
\bm{a}_j = \sum_{i=1}^{n}a_{ij}\bm{e}_i
$$

とすれば、

$$
\bm{a}'_j = \sum_{j=1}^{n}a_{ij}\bm{e}_i
$$

である。行列式の定義式を$\transposeMat{A}$に適用して

\begin{eqnarray*}
\abs{\transposeMat{A}} &= &D(\bm{a}'_1 \cdots, \bm{a}'_n) \\
&= &\sum_{\sigma \in S_n}sign(\sigma) \hspace{5pt} a_{\sigma(1)1} \cdots a_{\sigma(n)n}
\end{eqnarray*}


$\sigma$の逆置換を$\sigma^{-1}$とすれば、$sign(\sigma) = sign(\sigma^{-1})$は明らかであるから、

$$a_{\sigma(1)1} \cdots a_{\sigma(n)n} = a_{\sigma^{-1}(1)1} \cdots a_{\sigma^{-1}(n)n}
$$
である。これは、$\sigma$が$S_n$上を動くとき、$\sigma^{-1}$も$S_n$の上を動くことから

$$
\abs{\transposeMat{A}} = \abs{A}
$$

が得られる。
}

\subsubsection{様々な行列式の例}

\exam{\bf 上半三角、下半三角行列の行列式}

n次行列$A = (a_{ij})$が上半三角で、n次行列$B = (b_{ij})$が下半三角であるとき、つまり

$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
0 & a_{22} & \cdots & a_{2n} \\
\cdots & \cdots & \ddots & \cdots \\
0 & 0 & \cdots & a_{nn}
\end{pmatrix}
$$

$$
B = \begin{pmatrix}
b_{11} & 0 & \cdots & 0 \\
b_{21} & b_{22} & \cdots & 0 \\
\cdots & \cdots & \ddots & \cdots \\
b_{n1} & b_{n2} & \cdots & b_{nn} \\
\end{pmatrix}
$$

の形である時、AとBの行列式は

\begin{eqnarray*}
\abs{A} &= &a_{11}a_{22} \cdots a_{nn} \\
\abs{B} &= &b_{11}b_{22} \cdots b_{nn}
\end{eqnarray*}

のように対角成分の積に等しい。

{\bf 証明}

$A, B$の形状より

\begin{eqnarray*}
i > j \Longrightarrow a_{ij} = 0 \\
i < j \Longrightarrow b_{ij} = 0
\end{eqnarray*}

したがって、式(\ref{equation:Det2})において

\begin{eqnarray*}
\sigma(i) \leq i \quad (1 \leq i \leq n) \\
\sigma(i) \geq i \quad (1 \leq i \leq n) \\
\end{eqnarray*}

であるような$\sigma$に対する項だけが残るが、この条件を満たす$\sigma$は恒等置換$\sigma(i) = i$のみであるから、ここで主張する$\abs{A}, \abs{B}$を得る。

\exam{
$n$次行列$A = (a_{ij})$が

$$
A = \begin{pmatrix}
A_{11} & A_{12} \\
0 & A_{22}
\end{pmatrix},
$$

$$
A_{11} \in M_{n_1}(\mathbb{R}), \quad A_{12} \in M_{n_1,n_2}(\mathbb{R}), 
\quad A_{11} \in M_{n_2}(\mathbb{R}), \quad n_1 + n_2 = n
$$
の形ならば、


\begin{equation}
\abs{A} = \abs{A_{11}}\abs{A_{22}}
\label{equation:Det3}
\end{equation}


{\bf 証明}

$A$の形状より、$i > n_1, \hspace{5pt} j \leq n_1$のとき$a_{ij} = 0$となるから、式(\ref{equation:Det2})において

$$
j \leq n_1 \Longrightarrow \sigma(j) \leq n_1
$$

でるような$\sigma$に対する項だけが残る。この条件を満たす$\sigma$は$\{1, \cdots, n_1\}$の置換$\sigma_1$と$\{n_1 + 1, \cdots, n\}$の置換$\sigma_2$の積として表される。$\{1, \cdots, n_1\}$の置換全体の集合を$S_{n_1}, {n_1 + 1, \cdots, n}$の置換全体の集合を$S_{n-{n_1}}$とおけば、$\sigma_1, \sigma_2$は独立にそれぞれ$S_{n_1}, S_{n-{n}_1}$の上を動く。また、$\sigma = \sigma_1\sigma_2$であるとき、明らかに

$$
sign(\sigma) = sign(\sigma_1)sign(\sigma_2)
$$

である。よって、

\begin{equation}
\begin{split}
  \abs{A}
  ={}&
  \sum_{\sigma_1 \in S_{n_1}} sign(\sigma_1) \hspace{2pt} a_{\sigma_1(1)1} \cdots a_{\sigma_1({n_1}){n_1}}
  \\&\times 
  \sum_{\sigma_1 \in S_{n_2}} sign(\sigma_2) \hspace{2pt} a_{\sigma_2({n_1}+1){n_1+1}} \cdots a_{\sigma_2(n)n}
  \\={}&
  \abs{A_{11}}\abs{A_{22}}
\end{split}
\end{equation}
}\label{exam:det2}

\exercise{
$n_1 = 2, n_2 = 3$として、例\ref{exam:det2}を確かめよ。

{\bf 解答}

$\{1, 2\}$の置換全体の集合を$S_2$、$\{3, 4, 5\}$の置換全体の集合を$S_3$とする。$\sigma_1 \in S_2, \sigma_2 \in S_3$とする。
$\sigma_1$と$\sigma_2$はそれぞれ$S_2, S_3$の上を独立に動くから、行列式は以下のようになる。

\begin{equation*}
\begin{split}
  \abs{A}
  ={}&
  a_{11}a_{22} - a_{21}a_{12}
  \\&\times 
    a_{33}a_{44}a_{55} + a_{43}a_{54}a_{35} + a_{53}a_{34}a_{45}
  \\&- a_{33}a_{54}a_{45} - a_{43}a_{34}a_{55} - a_{53}a_{44}a_{35}
  \\={}&
  \abs{A_{11}}\abs{A_{22}}
\end{split}
\end{equation*}

したがって、例\ref{exam:det2}と同様の行列式が得られた。
}


\subsubsection{余因子}

$n$次行列$A = (a_{ij}) = (\bm{a}_1, \cdots, \bm{a}_n)$の第$i$行、第$j$列を取り去ってできる$(n - 1)$次行列を$A^{(ij)}$とかくことにする。そのとき、$A$の第$j$列を第$i$基本ベクトル$\bm{e}_i$でおきかえて得られる行列の行列式は次の式で与えられる。($\abs{\bm{a}_1, \cdots, \bm{a}_j, \cdots, \bm{a}_n}$の$\bm{a}_j$が$\bm{e}_i$に入れ替えらている。)

\begin{equation}
\abs{\bm{a}_1, \cdots, \bm{e}_i, \cdots, \bm{a}_n} = (-1)^{i+j}\abs{A^{ij}}
\label{defi:cofactor}
\end{equation}


この式の右辺を$A$の$(i,j)${\bf 余因子}(cofactor)という。

実際、余因子は次のように求められる。

$\bm{a}_1, \cdots, \bm{a}_n$の$i$行、$j$列を消し去り、第$j$列を基本ベクトル$\bm{e}_i$で置き換えた行列は以下のように表示できる。


$$
A^{(ij)} = \begin{pmatrix*}
a_{11} & \cdots & a_{1{j-1}} & 0 & a_{1{j+1}} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
a_{{i-1}1} & \cdots & a_{{i-1}{j-1}} & 0 & a_{{i-1}{j+1}} & \cdots & a_{{i-1}n} \\
0 & \cdots & 0 & 1 & 0 & \cdots & 0 \\
a_{{i+1}1} & \cdots & a_{{i+1}{j-1}} & 0 & a_{{i+1}{j+1}} & \cdots & a_{{i+1}n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
a_{n1} & \cdots & a_{n{j-1}} & 0 & a_{n{j+1}} & \cdots & a_{nn}
\end{pmatrix*}
$$


行列式の性質より、一組の列と列（あるいは行と行）を入れ替えると、符号が反転するところに着目すると、$A^{(ij)}$の第$j-1$列と第$j$列(つまり、$\bm{e}_i$)を入れ替えた行列$A^{(ij)'}$は

$$
A^{(ij)} = -A^{(ij)'}
$$

と表すことができる。この入れ替えを列に関して$j-1$回行うと

$$
\begin{pmatrix*}
0 & a_{11} & \cdots & a_{1{j-1}} & a_{1{j+1}} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & a_{{i-1}1} & \cdots & a_{{i-1}{j-1}} & a_{{i-1}{j+1}} & \cdots & a_{{i-1}n} \\
1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
0 & a_{{i+1}1} & \cdots & a_{{i+1}{j-1}} & a_{{i+1}{j+1}} & \cdots & a_{{i+1}n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & a_{n1} & \cdots & a_{n{j-1}} & a_{n{j+1}} & \cdots & a_{nn}
\end{pmatrix*}
$$

の行列が得られるから、この行列の行列式は

$$
(-1)^{j-1}\abs{\bm{e}_i, \bm{a}_1, \cdots, \bm{a}_{j-1}, \bm{a}_{j+1}, \cdots}
$$

の形になる。さらに、行に関して$i-1$回入れ替えを行うと最終的に

$$
\begin{pmatrix*}
1 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
0 & a_{11} & \cdots & a_{1{j-1}} & a_{1{j+1}} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & a_{{i-1}1} & \cdots & a_{{i-1}{j-1}} & a_{{i-1}{j+1}} & \cdots & a_{{i-1}n} \\
0 & a_{{i+1}1} & \cdots & a_{{i+1}{j-1}} &a_{{i+1}{j+1}} & \cdots & a_{{i+1}n} \\
\vdots & \vdots & \vdots & \vdots &\vdots & \vdots & \vdots \\
0 & a_{n1} & \cdots & a_{n{j-1}} &a_{n{j+1}} & \cdots & a_{nn}
\end{pmatrix*}
$$

の行列が得られる。この行列の行列式は

$$
(-1)^{(i-1)+(j-1)}\begin{vmatrix}
a_{11} & \cdots & a_{1{j-1}} & a_{1{j+1}} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
a_{{i-1}1} & \cdots & a_{{i-1}{j-1}} & a_{{i-1}{j+1}} & \cdots & a_{{i-1}n} \\
a_{{i+1}1} & \cdots & a_{{i+1}{j-1}} &a_{{i+1}{j+1}} & \cdots & a_{{i+1}n} \\
\vdots & \vdots & \vdots & \vdots &\vdots & \vdots & \vdots \\
a_{n1} & \cdots & a_{n{j-1}} &a_{n{j+1}} & \cdots & a_{nn}
\end{vmatrix}
$$

に等しい。よって、$(-1)^{(i-1)+(j-1)} = (-1)^{i+j}$から

$$
\abs{A^{(ij)}} = \abs{\bm{a}_1, \cdots, \bm{e}_i, \cdots, \bm{a}_n} = (-1)^{i+j}\abs{A^{(ij)}}
$$

が成立する。

\subsubsection{余因子展開}

この余因子を使ったn次行列式を求めるテクニックを紹介する。

$A = (a_{ij}) \in M_n(\mathbb{R})$の第$(i,j)$余因子を$\Delta_{ij} = (-1)^{i+j}\abs{A^{(ij)}}$とすれば、

第$j$列に関する展開を

$$
\abs{A} = \sum_{i=1}^{n} a_{ij}\Delta_{ij}
$$

第$i$行に関する展開を

$$
\abs{A} = \sum_{j=1}^{n} a_{ij}\Delta_{ij}
$$

とできる。

{\bf 証明}

列に関する展開式を証明すれば十分である。$A$の列ベクトルを$\bm{a}_1, \cdots, \bm{a}_n$とすれば、

$$
\bm{a}_j = \sum_{i=1}^{n} a_{ij}\bm{e}_i
$$

であるから、行列式の線形性$D(c\bm{a}_1, \bm{a}_2) = cD(\bm{a}_1, \bm{a}_2)$により

$$
\abs{A} = D(\bm{a}_1, \cdots, \bm{a}_j, \cdots, \bm{a}_n) = \sum_{i=1}^{n} a_{ij}D(\bm{a}_1, \cdots, \bm{e}_i, \cdots, \bm{a}_n)
$$

となることより、

$$
\abs{A} = \sum_{i=1}^{n} a_{ij}\Delta_{ij}
$$

を得る。

\exam{
$n = 3$のとき、第1列に関する余因子展開は次となる。

$$
\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{vmatrix} = a_{11}\begin{vmatrix}
a_{22} & a_{23} \\
a_{32} & a_{33}
\end{vmatrix} - a_{21}\begin{vmatrix}
a_{12} & a_{13} \\
a_{32} & a_{33}
\end{vmatrix} + a_{31}\begin{vmatrix}
a_{12} & a_{13} \\
a_{22} & a_{23}
\end{vmatrix}
$$
}

\rem{

$A, B \in M_n(\mathbb{R})$に対して

$$
\abs{AB} = \abs{A}\abs{B}.
$$

{\bf 証明}

Bの列ベクトルを$\bm{b}_1, \cdots, \bm{b}_n$とすれば、
$$
\abs{AB} = D(A\bm{b}_1, \cdots, A\bm{b}_n)
$$

右辺を$F(\bm{b}_1, \cdots, \bm{b}_n)$とおけば、明らかに行列式の性質(D1), (D2)をもっている。


\begin{equation*}
\begin{split}
  F(\bm{b}_1, \cdots, \bm{b}_n)
  ={}&
  F(\bm{e}_1, \cdots, \bm{e}_n)D(\bm{b}_1, \cdots, \bm{b}_n)
  \\={}&
  F(\bm{e}_1, \cdots, \bm{e}_n)\abs{B}
\end{split}
\end{equation*}

$A$の列ベクトルを$\bm{a}_1, \cdots, \bm{a}_n$とすれば、$A\bm{e}_j = \bm{a}_j$であるから

$$
F(\bm{e}_1, \cdots, \bm{e}_n) = D(\bm{a}_1, \cdots, \bm{a}_n) = \abs{A}
$$
}

\rem{
$A \in M_n(\mathbb{R})$が可逆であるためには、$\abs{A} \neq 0$が必要十分である。このとき、$\abs{A}^{-1} = \abs{A^{-1}}$となる。

{\bf 証明}

$A$が可逆ならば

$$
AA^{-1} = E
$$

であるから、両辺の行列式をとり

$$
\abs{A}\abs{A^{-1}} = \abs{E} = 1
$$

よって$\abs{A} \neq 0, \quad \abs{A}^{-1} = \abs{A^{-1}}$

}

% TODO 吉田さん
% 線形写像 4.6行列の階数からレビュー再開


\begin{thebibliography}{n}
\bibitem[1]{key2} 佐武一郎 [線形代数] 共立出版株式会社
\end{thebibliography}

\end{document}
